[{"content":"   핸즈온 데이터 시각화 (저자: 잭 도허티, 일리야 일라얀코우)\n  추천하는 대상:\n 시각화를 업무에 자주 사용하는 분  한줄평: 데이터시각화를 위한 교과서적인 책이며, 중요한 개념인만큼 차근차근 짚어나간다.\n친근하면서도 어려운 데이터 시각화 데이터 시각화는 단순히 데이터를 가지고 그래프를 그리는 행위가 아니다. 데이터 시각화는 데이터에 기반한 분석과 주장을 더욱 효과적으로 전달하기 위한 한 방법이다. 텍스트만으로는 전달하기 힘든 통찰은 차트, 지도 등과 결합하여 더욱 강한 설득력을 가진다.\n핸즈온 데이터 시각화 이렇게 단순히 차트를 그리는 것을 넘어 데이터 시각화에 대한 이론부터 차근차근 다루고 있는 핸즈온 데이터 시각화 는 2020년 경부터 이미 저자들이 인터넷 상에 책 내용을 무료로 공개하여 화제가 된 적이 있는 데이터 시각화 강의 교재이다. (독자의 편의성을 위해 실물로도 판매가 되고 있는데, 2022년 판매 금액은 전액 우크라이나 구호 재단에 기부된다고 하여 더욱 의미가 깊은 책이다.)\n전 독자층을 배려한 구성 이 책은 데이터 시각화에 대한 올바른 접근을 위한 이론과, 주로 구글 스프레드시트, 태블로 등의 도구를 활용한 실습 예제로 이루어져 있다. 또한 자바스크립트 기반의 Chart.js, 나 Highcharts 같은 기술의 경우, 코딩이 익숙하지 않은 독자층을 위해 코드를 작성하지 않고도 실습을 진행해볼 수 있도록 한 점이 눈길을 끈다.\n의미있는 스토리를 전달하기 위한 시각화 이 책에서 가장 도움이 많이 되었던 부분을 뽑자면, 책의 후반부에 기술된 시각화를 통해 정보를 왜곡된 시각화의 사례와 편향 등을 다룬 부분이다.\n우리는 시각화를 통해 효과적으로 정보에 기반한 주장을 할 수도 있지만, 의도적으로 왜곡된 정보를 전달할 수도 있다. 시각화를 통해 거짓말을 할 수 있는 방법에는 여러가지가 있다.\n 변화를 과장하거나, 변화를 축소하여 보여준다. 비율을 의도적으로 조작하여 보여준다. 세로축을 여러개 사용하여 혼돈을 준다.  이러한 부분을 인지한다면, 시각화를 통해 생길 수 있는 오해를 방지하고 보다 객관적인 관점을 유지할 수 있을 것이다.\n마무리하며 업무를 직접 진행하며 한 경험에 빗대어 본다면, 데이터 시각화는 사람마다 보는 관점이 달라 생각할 수록 어려운 영역이었다. 저자의 생각도 마찬가지이다. 하지만 시각화의 여러가지 사례와 안티 패턴을 소개하면서 읽는 이로 하여금 효과적인 데이터 시각화란 무엇인지 생각해볼 수 있는 능력을 키워주려고 하는 것이 이 책의 목적이라고 할 수 있다. 데이터 시각화는 절대적인 정답이 없다. 다만 저자는 이렇게 말한다.\n 학습자로서 여러분이 해야할 일은 문제에 대한 단 하나의 정답만을 고수하지 않고, 계속해서 더 좋은 답을 찾는 것입니다.\n  한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://lucaseo.github.io/posts/2022-07-19-book-reivew-hands-on-data-visualization/","summary":"핸즈온 데이터 시각화 (저자: 잭 도허티, 일리야 일라얀코우)\n  추천하는 대상:\n 시각화를 업무에 자주 사용하는 분  한줄평: 데이터시각화를 위한 교과서적인 책이며, 중요한 개념인만큼 차근차근 짚어나간다.\n친근하면서도 어려운 데이터 시각화 데이터 시각화는 단순히 데이터를 가지고 그래프를 그리는 행위가 아니다. 데이터 시각화는 데이터에 기반한 분석과 주장을 더욱 효과적으로 전달하기 위한 한 방법이다. 텍스트만으로는 전달하기 힘든 통찰은 차트, 지도 등과 결합하여 더욱 강한 설득력을 가진다.\n핸즈온 데이터 시각화 이렇게 단순히 차트를 그리는 것을 넘어 데이터 시각화에 대한 이론부터 차근차근 다루고 있는 핸즈온 데이터 시각화 는 2020년 경부터 이미 저자들이 인터넷 상에 책 내용을 무료로 공개하여 화제가 된 적이 있는 데이터 시각화 강의 교재이다.","title":"[KR] 책 리뷰 : 핸즈온 데이터 시각화"},{"content":"   기업의 성공을 이끄는 Developer Relations (저자: 메리 셍발)\n  추천하는 대상:\n Developer Relations라는 용어가 궁금하신 분 기술을 사랑하고 커뮤니티 활동에도 관심이 많으신 분 오픈소스 활동에 관심 있으신 분  한줄평: 한국에서 DevRel 에 대한 인식과 활동 무대가 더욱 커질 수 있는 시작점이 될 책\n들어가기에 앞서, Developer Relations 이라는 용어를 처음 접하고, Public Relations이 떠올랐다. 보통 줄여서 PR 이라고 하고, 한국에서는 PR한다 와 같이 콩글리쉬로도 쓰인다. 흔히들 PR을 홍보, 마케팅과 결을 같이 하는 용어라고 이해하는 경우가 많지만, 좀 더 정확히 하자면, PR은 대중과 호의적인 관계를 유지하기 위한 모든 활동을 의미한다. (홍보 또는 마케팅은 그 일부분이다.) 그렇기에 PR은 기업의 상업적인 활동 뿐만 아니라, 정당, 정치단체, 기관, 정부, 연예인과 같은 유명인사들도 대중을 대할 때, 필수로 PR 담당자를 대동하거나, PR 전문 업체의 조언을 받는다.\n이렇듯 relations 라는 단어는 한글로 단순히 \u0026ldquo;관계\u0026quot;를 의미하지만, 그 역할이 커버하는 영역은 정말 광범위 하다.\n그래서인지 인터넷에서 Developer Relations 이라는 용어를 접했을 때, 해당 직군이 개발 업무 그 자체가 아닌 개발자를 대하는 업무를 한다는 것은 직감했다. 하지만 개발자와의 관계를 유지하고 개발자를 대하는 직군이라는 건 어떤 일을 하는 것인지, 감을 잡기 어려웠다. 기업의 성공을 이끄는 Developer Relations 가 눈길을 끌었던 이유다.\nDeveloper Relations? 책에서 정의하는 Developer Relations(DevRel, 이하 데브렐)은 다음과 같다.\n 기업과 개발자의 관계를 만들고 유지하며, 장기적으로 회사가 기술적 우위를 점할 수 있도록 기반과 생태계를 만드는 분야\n 정말 와닿았다. 데브렐이란, 기술을 개발하는 기업이 해당 기술을 사용하는 개발자들과 소통하며 소위 팬층을 구축하고, 이들이 기술 사용에 있어 궁금한 점이나 어려움이 있을 때 서로 도움을 주며 더 좋은 아이디어가 있으면 건의하거나 더 나아가 기여할 수 있는 사용자들만의 커뮤니티를 만들어나가는 일을 하는 것이다.\n늘 많은 도움을 받고 있는 한국 Elasticsearch 사용자 페이스북 그룹이 그러했으며, 최근에 컨퍼런스를 열었던 통합 Databricks, Weighs \u0026amp; Biases, 그리고 꾸준하게 MLOps KR 커뮤니티를 이끌어나가고 있는 SuperbAI의 커뮤니티 그로스 매니저님도 비슷한 사례였음을 뒤늦게 깨닫게 되었다.\n조금 더 곱씹어보면 이들에게 개발자들은 자사의 기술을 함께 발전시켜나갈 동반자이기에 고객 그 이상의 가치를 가진다. 그래서 전통적인 영업의 기술이나 고객 관리의 역학이 단순하게 적용되지 않는다.\n생각보다 매우 sophisticated한 분야임은 틀림없다. (우리말로 표현하기에 적당한 말을 찾을 수 없었다)\n세상 친절한 DevRel 입문서 이 책에서는 막연하게 짐작할 수 있는 데브렐의 영역에 대해 작은 부분까지 짚어가며, 친절하게 설명되어 있어, 독자는 다음과 같은 질문에 대한 답을 얻을 수 있다.\n사용자 커뮤니티의 필요성  왜 현 시점에서 개발자(사용자)들만의 커뮤니티가 필요한가? 필요하다면, 커뮤니티를 통해 이루고자 하는 것은 무엇인가 ? 커뮤니티의 역할은 무엇인가?  새로운 사용자층의 유입을 위한 대외 홍보의 공간 기존 프로덕트를 발전시켜나가기 위한 피드백과 토론의 공간    데브렐 조직의 시작  커뮤니티 구축을 위한 데브렐 팀을 만드려면 어디서부터 시작해야 하는가?  데브렐 조직은 기업의 어떠한 니즈를 충족시킬 수 있는가? 데브렐 팀의 탄생을 위해서 어떤 이해관계자들을 설득해야 하는가?   데브렐 팀의 포지셔닝 데브렐팀의 성과는 어떻게 측정하는가?  커뮤니티 운영을 위한 팁  커뮤니티를 새로 만들어야 할 지, 존재하는 커뮤니티를 공략해야 하는가? 커뮤니티에서 이벤트는 어떻게 기획하는가? 커뮤니티에서 생길 수 있는 문제는 어떤 것들이 있고, 어떻게 대응해야 하는가? 커뮤니티 내에서 데브렐 담당자는 어떻게 포지셔닝해야 하고, 어떤 이미지를 가져야 하는가? 어떻게 이미지(퍼스널 브랜딩)를 구축하는가?  데브렐 전문가의 인터뷰 이 책의 한글 번역 출판본에는 라인, SK텔레콤, 우아한 형제들 등 한국 기업의 데브렐 담당자들의 인터뷰를 추가적으로 담고 있다.\n각 조직들의 현업에 대한 이야기가 담겨있는 한편, 기술 자체로 프로덕트를 만드는 기업이 아니더라도 데브렐 조직은 기술 조직의 브랜딩을 담당하여 치열한 구인시자에서 좋은 인재 채용에 기여하는 등 또 다른 역할과 커뮤니티 타겟층에 대한 내용도 다루고 있다.\n책의 본문에 소개된 다양한 해외 사례와 더불어 이와 같이 한국 데브렐 전문가의 인터뷰까지 수록되었기 때문에, 단순한 번역본보다도 더 완성도가 있는 책이라는 생각이 든다.\n책에서 언급된 것처럼 데브렐은 2017년 이후에나 접하게 된, 매우 새로운 영역임이 틀림 없다. 데브렐 포지션에 관심이 없더라도 한번이라도 커뮤니티에 질문을 올렸거나, 구글링을 통해 커뮤니티에서 문제에 대한 해답을 찾았던 개발자라면(아마 거의 대부분의 개발자가 아닐까\u0026hellip;?), 커뮤니티의 수혜를 한번이라도 입은 개발자라면, 한번은 읽어봄직한 책이라 생각된다. 번역도 매끄럽고 잘 읽힌다. 추천!\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://lucaseo.github.io/posts/2022-06-23-book-review-developer-relations/","summary":"기업의 성공을 이끄는 Developer Relations (저자: 메리 셍발)\n  추천하는 대상:\n Developer Relations라는 용어가 궁금하신 분 기술을 사랑하고 커뮤니티 활동에도 관심이 많으신 분 오픈소스 활동에 관심 있으신 분  한줄평: 한국에서 DevRel 에 대한 인식과 활동 무대가 더욱 커질 수 있는 시작점이 될 책\n들어가기에 앞서, Developer Relations 이라는 용어를 처음 접하고, Public Relations이 떠올랐다. 보통 줄여서 PR 이라고 하고, 한국에서는 PR한다 와 같이 콩글리쉬로도 쓰인다.","title":"[KR] 책 리뷰 : 기업의 성공을 이끄는 Developer Relations"},{"content":"어떠한 일련의 작업들을 순서대로 묶어 위험요소나 외부의 개입을 최소화하고 자동으로 실행하고자 할 때 파이프라인을 구축한다.\n파이프라인의 개념이 적용되지 않은 머신러닝 프로젝트는 결과물을 얻기 위한 과정의 자동화와 재사용성이 확보되지 않기 때문에 일회성 PoC에 그칠 가능성이 높다.\n머신러닝 파이프라인이라는 개념은 머신러닝 모델을 학습하고, 재사용하며, 필요한 자원들을 관리하고 배포하는 일련의 과정을 표준화하고 자동으로 동작할 수 있도록 하기 위해 논의 되기 시작했다.\nDAG 파이프라인은 대개 DAG(Directed acyclic graph: 방향성 비순환 그래프)의 형태를 띄고 있다. 이름에서 알 수 있듯이, DAG는 방향을 가지되, 루프는 존재하지 않아, 그래프의 시작과 종료가 이루어지는 시점이 명확하기에 파이프라인을 구성하는데 적절한 구조이다.\n   DAG의 예시\n  DAG의 형태를 띈 머신러닝 파이프라인을 구성하는 요소에는 컴포넌트(component)와 아티팩트(artifact)가 있다.\n머신러닝 파이프라인의 요소 머신러닝 파이프라인에는 각각의 작업을 하는 컴포넌트와, 컴포넌트의 결과물이자 다음 작업에 사용되는 아티팩트가 있다.\n      머신러닝 파이프라인에서의 컴포넌트와 아티팩트\n  컴포넌트 (Component)  컴포넌트는 각각의 독립되고 재사용이 가능한 모듈로서 입력을 받아 어떠한 처리나 연산을 거쳐 결과값을 출력한다. 스크립트, 노트북 또는 다른 실행 가능한 형태의 코드가 될 수 있다. 예) 데이터 검증, 데이터 전처리, 학습, 평가, 추론 등  아티팩트 (Artifact)  아티팩트는 컴포넌트의 산출물로서, 파일이나 경로가 될 수 있다. 아티팩트는 다른 컴포넌트의 입력값의 역할을 하며, 컴포넌트와 컴포넌트 사이를 잇는 역할을 할 수 있다. 아티팩트는 저장된 경로와 그 버전이 꼭 관리가 되어야 한다. 예) 데이터, 모델, 피쳐 등   머신러닝 파이프라인의 단계 머신러닝 파이프라인에도, 구현하고자 하는 기능과 재사용성, 자동화 여부 등에 따라 파이프라인의 성숙도 즉, 수준이 구별된다.\n   Level 0 모델을 위한 갖가지 학습을 반복적으로 진행하고, 특정한 성능 지표의 달성을 최우선으로 한다. 주로 노트북이나 개별로 실행하는 스크립트의 형태를 띄고 있으며, 재사용성은 매우 낮다. 따라서 프로덕션 용도로 사용되지 않는다.\n주로 사이드프로젝트나, 새로운 기술을 익히기 위한 용도로 사용되며, 캐글 대회나 연구, PoC 등을 진행할 때 주로 보인다.\n즉, 파이프라인으로서의 역할과 그 효과가 매우 미미하거나 없는 형태인 수준이라고 할 수 있다.\nLevel 1 데이터 처리와 학습, 성능 평가, 추론 등의 단계가 컴포넌트로 독립되어 있으며 각각의 산출물은 아티팩트로서 저장된다. 따라서 재학습이 용이하며, 각 실험결과 또한 기록된다.\n프로덕션 상태에서도 모니터링이 이루어지며, 추론 속도, 재학습 속도 등의 성능 또한 중요한 지표로 기록된다.\n레벨 1의 파이프라인은 최소한의 작업단위들을 모듈로서 표준화 하고 빠르게 프로덕션 환경을 프로토타이핑 하는데 용이하며, 재학습을 가능케 한다. 또한, 이러한 형태의 파이프라인은 업무의 공유나, 인수인계 상황에서 복잡한 과정을 피할 수 있다.\nLevel 2 레벨 2의 파이프라인은 CI/CD(Continuous Integration and Continuous Deployment)와 더불어 CT(Continuous Training)의 요소까지 접목된 수준을 나타낸다.\nCI/CD/CT가 접목된 파이프라인은 모델의 개선과 프로덕션의 수정을 용이하게 하고, 개선된 파이프라인과 기존 파이프라인의 A/B테스팅이 용이해진다. 또한 공백없는 배포를 자동으로 수행하기 때문에, 결과적으로 협업을 통한 제품의 개선과 반영 속도를 훨씬 빠르고 안전하게 할 수 있다.\n 마무리하며 이번에는 MLOps 에 대해 공부하며, 머신러닝 파이프라인에 대한 개념과 수행 기능, 요건 등을 정리해보았다. 현재 머신러닝 파이프라인을 구축하는데 사용되는 툴이나 라이브러리는 너무나 다양하게 포진되어 있기에, 어떠한 특정 툴을 사용하는 것보다는, 파이프라인을 구축하는데 무엇이 필요하고 어떤 기능이 꼭 구현되어야 하는지를 제대로 파악한 뒤, 필요에 따른 적합한 도구를 사용하는 것이 매우 중요하다고 판단된다.\nReference  Udacity: Machine Learning DevOps Engineer  ","permalink":"https://lucaseo.github.io/posts/2022-06-12-intro-ml-pipeline/","summary":"어떠한 일련의 작업들을 순서대로 묶어 위험요소나 외부의 개입을 최소화하고 자동으로 실행하고자 할 때 파이프라인을 구축한다.\n파이프라인의 개념이 적용되지 않은 머신러닝 프로젝트는 결과물을 얻기 위한 과정의 자동화와 재사용성이 확보되지 않기 때문에 일회성 PoC에 그칠 가능성이 높다.\n머신러닝 파이프라인이라는 개념은 머신러닝 모델을 학습하고, 재사용하며, 필요한 자원들을 관리하고 배포하는 일련의 과정을 표준화하고 자동으로 동작할 수 있도록 하기 위해 논의 되기 시작했다.\nDAG 파이프라인은 대개 DAG(Directed acyclic graph: 방향성 비순환 그래프)의 형태를 띄고 있다. 이름에서 알 수 있듯이, DAG는 방향을 가지되, 루프는 존재하지 않아, 그래프의 시작과 종료가 이루어지는 시점이 명확하기에 파이프라인을 구성하는데 적절한 구조이다.","title":"[KR] 머신러닝 파이프라인 개념 정리"},{"content":"   MLOps 도입 가이드 (저자: 데이터이쿠)\n  추천하는 대상:\n MLOps를 도입하기 위한 고민을 하는 분  한줄평: 머신러닝 모델을 배포하는 것은, 잘 돌아가는 걸 확인하는 하나의 단계일 뿐, 그 외 해야할 일은 많다.\n더 이상 선택요소가 아니게 된 MLOps 머신러닝이라는 기술에 대한 고도화가 이루어져 다양한 분야에서 활용되고 있는 현 시점에서, MLOps 라는 방법론 한번 슬쩍 접하는 것이 아닌 필수 사항이 되어가고 있다. MLOps는 현재 명확하게 정립되지 않아 논문이 아닌 수많은 블로그 포스트들로 다루어지고 있고, 또 수많은 도구들이 나타나 서로가 편리하다고 주장을 하고 있는 상황이다. 요즘 들어 나오고 있는 MLOps 관련 강의들 또한 이러한 도구들의 사용법에 대해 다루는 경우를 많이 볼 수가 있다.\n하지만 누구나 현재 가지고 있는 자원과 또는 지원 받을 수 있는 자원이 다르고, 다룰 수 있는 능력의 정도가 다르기에, 더더욱 개념부터 더 확실하게 숙지하고 현재 나의 일에 어떻게 적용시킬 수 있을지 고민해보는 과정이 필요하다고 느끼고 있다.\nMLOps 도입가이드 의 원제는 “Introducing MLOps”으로, 제목 그대로 MLOps에 대한 전반적인 내용을 다루고 있다. 이 책에서는 MLOps에 대한 전반적인 내용을 폭 넓게 다루며, 실제 구현보다는 각 개념과 필요성에 대한 설명에 초점을 맞추고 있다.\n   MLOps 의 수많은 이해관계자들의 역할\n   MLOps의 개념 및 이해관계자들 MLOps의 핵심 기능과 필요성 MLOps 적용 방법  모델 개발 상용화 준비 상용 배포 모니터링 반복 거버넌스   적용 사례  한가지 눈에 띄는 점이라면, 책의 저자가 미국과 프랑스에 오피스를 두고 있는 AI/ML 전문 기업인 데이터이쿠(Dataiku) 라는 점이다. 데이터이쿠에서 일하고 있는 9명의 임원과 스태프들이 이 책의 각 부분을 맡아 집필했는데, 1~3명의 소수가 책을 쓰는 것과는 확연히 차이점을 보이고 있다. 그만큼 이 책은 한 기업에 속한 여러 전문가가 각자의 전문성을 발휘할 수 있는 주제를 맡았다는 점에서, 인상적이다.\nMLOps는 리스크 관리 이 포스트에서 이 책이 다루는 MLOps에 대한 내용을 전부 다 다룰 수는 없고, 또한 시중의 다른 자료들과 겹치는 부분도 많다. 하지만 가장 인상적인 부분을 하나 꼽아보자면, 리스크를 줄이고자 하는 차원에서의 MLOps의 필요성에 대해서 서술한 부분이다.\n   머신러닝 모델의 리스크를 측정하기 위한 리스크 매트릭스\n   \u0026ldquo;결국, 모델을 상용 환경에 배포하는 작업은 머신러닝 모델 생애주기의 최종 단게는 아니다. 단지 성능과 정상 작동 여부를 확인하는 시작점이 될 뿐이다. (중략) 더 많은 머신러닝 모델을 상용 환경에 배포할수록, MLOps는 비즈니스에 치명적일 수 있는 잠재적 리스크를 줄이는데 더 필수적인 요소가 된다.\u0026quot;\n 머신러닝 모델은 운용하는 과정에서 다양한 리스크를 맞이할 수 있다. 앞서 언급했듯이, 어느 조직이나 활용 가능한 리소스가, 리소스를 갖추기 위한 지원의 정도가 다르다. 따라서 이 책에서 제시한 리스크 매트릭스를 활용하여, 머신러닝 모델의 리스크를 정량적으로 판단하고, 이에 따라 MLOps의 적용 범위와 정도를 정할 수 있을 것이라 기대된다. 만일 리스크가 크지 않다면 MLOps 시스템을 구축하는 과정에서 우선순위가 높지 않을 수 있고, 만일 리스크가 크지만, 이에 대비할 수 있는 리소스가 갖춰지지 않았다면 이에 대비하는 작업을 우선적으로 시행할 수 있을 것이다.\n마무리하며 MLOps 도입 가이드 는 책의 두께 두껍지 않아 빠르게 읽고, 필요한 부분을 참고할 수 있다. 다만, 앞서 말한 바와 같이 MLOps에는 많은 이해관계자가 필요하고, 각자 가지고 있는 환경과 리소스가 다르기에, 한번에 모든 것을 갖추기보다, 핵심적으로 필요하거나, 시급한 부분을 취사 선택하는 것은 독자의 몫(= 나의 몫)이다. 가벼운 길잡이로 활용해보자.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://lucaseo.github.io/posts/2022-05-28-book-review-introducing-mlops/","summary":"MLOps 도입 가이드 (저자: 데이터이쿠)\n  추천하는 대상:\n MLOps를 도입하기 위한 고민을 하는 분  한줄평: 머신러닝 모델을 배포하는 것은, 잘 돌아가는 걸 확인하는 하나의 단계일 뿐, 그 외 해야할 일은 많다.\n더 이상 선택요소가 아니게 된 MLOps 머신러닝이라는 기술에 대한 고도화가 이루어져 다양한 분야에서 활용되고 있는 현 시점에서, MLOps 라는 방법론 한번 슬쩍 접하는 것이 아닌 필수 사항이 되어가고 있다. MLOps는 현재 명확하게 정립되지 않아 논문이 아닌 수많은 블로그 포스트들로 다루어지고 있고, 또 수많은 도구들이 나타나 서로가 편리하다고 주장을 하고 있는 상황이다.","title":"[KR] 책 리뷰 : MLOps 도입가이드"},{"content":"오래간만에 다시 글또 활동을 시작하며 몇 개월 만에 새로운 글또 기수가 시작이 되었다. 이번 기수에도 크게 고민하지 않고 신청을 했는데, 지난 3번의 글또 활동을 시작할 때와 달리, 이번에는 시작을 앞둔 각오가 조금은 다른 것을 느끼고 있다.\n이번에는 너무 잘 하려고 하지 않으려고 한다. 과거에는 회사에서의 일, 퇴근 후 개인 공부, 글또 등등 성장과 관련된 부분들은 전부 다 잡고 싶고 잘 하려고 했던 것 같다. 의욕만 과다했던 탓일까, 때로는 번아웃이 오기도 했고, 스스로 생각해도 퀄리티가 높지 않았지만 데드라인에 맞춰서 그냥 낸 적도 있다. 회고 시점이 되어 돌아보면 열심히 하지 않은 것 같아서 만족스럽지 않았다. 의욕만 과다했던게 독이 되지 않았나 되돌아본 계기가 되었다.\n의욕과 스트레스만 가득했던 2019-2021년 이 지나 2022년에는 어느 정도 좀 내려놓고, 느린 속도여도 꾸준히 한다는 마음 가짐을 유지 하고 있다. 회사일, 대학원, 글또, 한빛미디어서평단 등 \u0026hellip; 나중에 돌아봤을 때, 그래도 아예 내려놓지 않고 꾸준했다 라고만 느껴도 성공한 한 해가 되지 않을까 싶다.\n이번 기수에서 다루고자 하는 주제 1~2달 이후 본격적으로 머신러닝 파이프라인 구축 업무를 하게 될 예정인데, 여러 적용 케이스를 참고하고 우리 팀의 요구 사항을 정리해서, 어떻게 구축할 것인지 고민한 흔적을 정리할 생각이다. 변성윤님의 발표자료에서 MLOps 춘추전국시대라고 묘사되었 듯 너무나 많은 도구와 각기 다른 상황에 따른 적용 방식이 있기에, 우리에게 맞는 방법을 초기에 고민해서 정립하는 것이 중요할 것 같다. 다양한 툴과 프레임워크에 대한 글, 이러한 고민을 토이프로젝트를 하면서 연습하는 과정을 포스트로 정리할 생각이다.\n다른 쪽으로는 팀 문화, 팀원 관리, 제도 등과 관련된 고민을 적어보고 싶다. 현재 회사에서 공식적인 매니저는 아니지만, 팀이 워낙 작은 관계로 각 멤버가 일정 부분에서는 매니저와 같은 역할을 나눠서 하고 있다. 앞으로 회사가 성장하면서 팀원이 더 생길 것으로 예상하는데, 나는 내가 겪었던 \u0026ldquo;체계가 없음\u0026rdquo;, \u0026ldquo;조언해줄 사람이 없음\u0026rdquo;, \u0026ldquo;참고할 게 없음\u0026rdquo;, \u0026ldquo;내 포지션의 고충을 아무도 이해 못 함\u0026rdquo;, \u0026ldquo;같이 고민할 사람 없음\u0026rdquo; 과 같은 상황을 새로 들어올 팀원에게 똑같이 물려주고 싶지 않다(그런 사람은 누구도 없을 것이다) 그렇다면 무엇이 마련되어 있어야 하는지, 어떤 문화가 정착되어야 하는지, 사례를 보고 책을 읽어서 정리해야 할 것 같다. 다행스럽게도, 작년에는 조직문화 전문가 팀원이 채용되었다. 회사의 기존 DNA도 있기에 그 속도가 드라마틱하지는 않지만, 덕분에 일반적인 기업 문화의 경우에는 긍정적으로 바뀌는 중이라고 생각한다. 나는 기술 조직의 성향이나 문화를 조금 고민해보고 싶다. (딱히 내게 주어진 업무는 아니긴 하다. 어쩌면 대학교 때 전공이 인사관리였기 때문에 그런 성향이 남아있는 것일지도 모르지만)\n어김없이 다짐글을 마무리하며 이번에는 너무 구체적이고, 원대한 계획을 세우지 않았다. 지난 시간을 돌이켜봤을 때, 그게 도움보다는 부담과 스트레스를 줬고 이대로 계속하다간 정신건강의원을 갈지도 모르겠다는 생각이 들었다.(정신건강의원을 가는게 안 좋다는 건 아니지만!!) 그래서 이번에는 내가 퇴근하면 운동도 즐기고, 책도 많이 읽으면서 좀 느슨하게 해보려고 한다. 이렇게 보낸 5개월은 어떨까. 나는 2022년을 어떻게 회고하게 될까.\n","permalink":"https://lucaseo.github.io/posts/2022-05-14-init-geultto-7/","summary":"오래간만에 다시 글또 활동을 시작하며 몇 개월 만에 새로운 글또 기수가 시작이 되었다. 이번 기수에도 크게 고민하지 않고 신청을 했는데, 지난 3번의 글또 활동을 시작할 때와 달리, 이번에는 시작을 앞둔 각오가 조금은 다른 것을 느끼고 있다.\n이번에는 너무 잘 하려고 하지 않으려고 한다. 과거에는 회사에서의 일, 퇴근 후 개인 공부, 글또 등등 성장과 관련된 부분들은 전부 다 잡고 싶고 잘 하려고 했던 것 같다. 의욕만 과다했던 탓일까, 때로는 번아웃이 오기도 했고, 스스로 생각해도 퀄리티가 높지 않았지만 데드라인에 맞춰서 그냥 낸 적도 있다.","title":"[KR] 글또 7기를 시작하며"},{"content":"   머신 러닝 파워드 애플리케이션 (저자: 파노스 알렉소풀로스)\n  추천하는 대상:\n 데이터 모델을 개발하거나 체계를 구축하는 업무를 하시는 분  한줄평: 데이터는 금가루가 박혀있는 원석. 시맨틱 데이터 모델링은 원석을 정제해서 금가루를 모아 금괴로 만드는 기술.\n시맨틱 데이터 모델링이란 데이터는 그냥 쌓아둔다고 좋은 것이 아니다. 데이터는 사용이 가능하도록 분류하는 과정을 거쳐 저장되어야 적절히 사용할 수 있다. 지극히 당연한 이야기지만, 데이터를 분류할 때 어떠한 의미를 부여할 것인지에 대해 생각해보자면, 막막할 때가 많다.\n그렇기에 시멘틱 데이터 모형화 방법론을 적용하여 데이터에 의미를 부여하는 것이 하나의 방법이 될 수 있다.\n 시맨틱 데이터 모형화란 인간과 컴퓨터 시스템에서 모두 명료하고 정확하며 일반적으로 이해되는 방식으로 데이터 표현을 개발하는 일이라고 정의할 수 있다.\n 이러한 방식으로 데이터를 표현하게 되었을 때 우리는 전산에서 데이터를 활용하며, 동시에 데이터를 공통된 의미로 정의하여 다른 사람들도 이해할 수 있어 효율적이고 관리가 용이하다.\n따라서 시멘틱 데이터 모형화 기술은 우리가 흔히 접할 수 있는 E-R 모형(entity-relationship model)이나 메타데이터 뿐만 아니라, 어떠한 분야의 지식 체계를 정리하는 온톨로지(ontology), 택소노미(taxonomy) 등에도 모두 적용된다.\n시맨틱 모형과 온톨로지? 텍소노미? 온톨로지와 텍소노미, 분류체계라는 정의는 매우 생소하다. 사전적 정의를 참고하자면 다음과 같다.\n온톨로지?\n 온톨로지(Ontology)란 사람들이 세상에 대하여 보고 듣고 느끼고 생각하는 것에 대하여 서로 간의 토론을 통하여 합의를 이룬 바를, 개념적이고 컴퓨터에서 다룰 수 있는 형태로 표현한 모델로, 개념의 타입이나 사용상의 제약조건들을 명시적으로 정의한 기술이다. 온톨로지는 일종의 지식표현(knowledge representation)으로, 컴퓨터는 온톨로지로 표현된 개념을 이해하고 지식처리를 할 수 있게 된다. 프로그램과 인간이 지식을 공유하는데 도움을 주기 위한 온톨로지는, 정보시스템의 대상이 되는 자원의 개념을 명확하게 정의하고 상세하게 기술하여 보다 정확한 정보를 찾을 수 있도록 하는데 목적이 있다. [위키피디아]\n 택소노미?\n 가나다, … ABC, …와 같이 표준화되고 체계적으로 분류된 전통적인 분류학 기반의 분류 체계. 트리형의 위계적 구조로서 이미 결정된 체계를 가지고 있다는 특징이 있다. 그리스어로 ‘분류하다’라는 ‘tassein’과 ‘법, 과학’이라는 ‘nomos’의 합성어로 사람들에 의해 이해되는 관계를 기준으로 분류되는 폭소노미(folksonomy)에 대비되는 용어이다. [네이버 지식백과]\n 온톨로지는 어떠한 개념이나 타입 등의 지식에 의미를 부여하여 정보시스템에서도 유용하게 활용할 수 있도록 의미를 명확하고 상세하게 부여하는 기법이라면, 택소노미는 표준화된 분류체계이다. 즉, 이러한 기술들의 공통된 점은 데이터의 의미를 명시하는 것이다.\n택소노미의 예시\n2022년 2월 3일 유럽연합은 “그린 택소노미(Green Taxonomy)”의 최종안을 발표했다. 그린 택소노미는 ‘환경적으로 지속가능한 경제 활동’의 범위를 정한 분류체계로, 어떠한 산업, 어떠한 경제활동이 탄소중립에 기여하고 환경적으로 지속가능한 것인지를 명시한다. 산업과 경제활동이란 것이 매우 광범위 하기 때문에, 이를 아주 세세하고 또 어떠한 편법과 잘못된 해석이 발생하지 않도록 치밀하게 정의 및 분류된 것을 알 수 있다.\n시멘틱 데이터 모형화 과정에서 유의해야 할 점 분류체계를 작성하는 일은 우리가 상상하는 것보다 훨씬 더 전문적인 접근 방법이 존재한다. 그저 한 개인이 자신의 배경지식과 경력을 가지고 만들어 나갈 수 있는 것이 아니다. 또한 잘못된 분류체계와 의미는, 시간이 지나면서 오류가 발각되고 지속적인 수정이 필요하게 되기 때문에, 초기 개발과 구축 과정 신중함을 요구한다.\n함정 이 책에서 제시하는 시맨틱 모형 개발 과정에서의 함정에는 어떤 유형이 있을까?\n 나쁜 설명 - 잘못된 이름과 의미 부여, 또는 정의 생략 잘못된 규격과 잘못된 지식 공급원, 획득방법, 지식 나쁜 품질 관리 잘못된 활용 나쁜 전략과 나쁜 조직 \u0026hellip;  딜레마 실무에서 어떠한 지식 체계를 정리할 때 자주 발생할 수 있는 딜레마에 대한 내용도 다루고 있다.\n 표현의 딜레마 표현과 내용 간의 관계에 대한 딜레마 모형 개선 방향, 모형 관리에 대한 딜레마  마무리하며 시멘틱 데이터 모형화 는 데이터를 분류하는 방법론에 있어 생각보다 훨씬 전문적인이고 상세하게 내용을 다루고 있다. 입문서는 아니라고 생각되며, 실제로 데이터의 체계를 잡아가는 일을 하시는 분들이라면, 지침서로 삼기 아주 좋은 책이다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://lucaseo.github.io/posts/2022-04-24-book-review-semantic-data-modeling/","summary":"머신 러닝 파워드 애플리케이션 (저자: 파노스 알렉소풀로스)\n  추천하는 대상:\n 데이터 모델을 개발하거나 체계를 구축하는 업무를 하시는 분  한줄평: 데이터는 금가루가 박혀있는 원석. 시맨틱 데이터 모델링은 원석을 정제해서 금가루를 모아 금괴로 만드는 기술.\n시맨틱 데이터 모델링이란 데이터는 그냥 쌓아둔다고 좋은 것이 아니다. 데이터는 사용이 가능하도록 분류하는 과정을 거쳐 저장되어야 적절히 사용할 수 있다. 지극히 당연한 이야기지만, 데이터를 분류할 때 어떠한 의미를 부여할 것인지에 대해 생각해보자면, 막막할 때가 많다.","title":"[KR] 책 리뷰 : 시멘틱 데이터 모형화"},{"content":"   머신 러닝 파워드 애플리케이션 (저자: 에마뉘엘 아메장)\n  추천 대상:\n 머신러닝이 적용된 제품을 만들고자 하는 분 머신러닝 엔지니어로 취업/이직하고자 하는 분  한줄평: 데이터 수집, 전처리, 학습 \u0026hellip; 이제 이걸 가지고 뭘 하지?\n머신러닝 모델은 서비스가 될 때 진정한 가치가 있다 “머신러닝에 대해 배운다” 라고 하면 대부분의 커리큘럼은 갖가지 알고리즘과 그 작동 방식에 대해 배우는 것부터 시작한다. 가장 간단한 형태의 모델부터 최신의 복잡한 구조의 모델까지 훑고, 토이 데이터셋으로 실습까지 진행한다. 머신러닝이라는 기술을 구현하기 위한 대략적인 지식을 얻게 되었다면, 그 다음에는 이 질문이 떠올라야 한다.\n “그럼 이제 이걸 가지고 뭘 하면 되지?”\n 머신러닝, 딥러닝, 인공지능이라는 신박한 기술이 소개되어 사람들을 매료시키던 시대는 이제 지났다고 할 수 있다. 기술은 실제로 사용이 되어야 그 가치가 있는 것과 마찬가지로 머신러닝 모델 또한 사용자에게 서비스가 될 때 진정한 가치가 있다. 우리는 지금 이 기술을 활용한 제대로 된 제품을 만들어 내야 하는 시기에 있다.\n머신러닝 파워드 애플리케이션 은 책 제목 그대로 머신러닝 기술에 기반한 제품(서비스)를 만들기 위한 내용을 담고 있다. 따라서, 현업에서 머신러닝 기술을 연구하는 단계가 아닌, 실제로 적용한 비즈니스 진행시켜야 하거나, 머신러닝 엔지니어로서 취업/이직을 고려하는 사람에게 적합하다. 단, 이 책은 머신러닝에 대한 기본적인 개념이 잡혀 있다는 것을 전제로 하고 있기 때문에, 초심자에게는 권하지 않는다. (머신러닝 개발자와 커뮤니케이션이 자주 발생하는 기획자는 업무에 참고할 만 하다.)\n머신러닝 제품화 길잡이 머신러닝 파워드 애플리케이션 은 머신러닝 기반 제품을 만들기 위한 아주 좋은 가이드북이다. 책에서 참고할 수 있는 내용들은 다음과 같다.\n1. 문제를 해결하기 위한 머신러닝 모색  문제를 해결하고 편의를 제공하기 위한 어플리케이션 정의 문제를 어떻게 정의하고, 적합한 모델을 선정하여 필요한 데이터를 준비해야할 지 파악하는 과정 머신러닝 기반 제품 개발의 전체적인 프로젝트의 일정의 계획하는 과정     다양한 문제에 따른 접근 방법 예시\n  2. 가장 간단하게 만드는 엔드투엔드 파이프라인    간단하지만 핵심 기능을 하는 제품부터 만들어보자\n   가장 필요로 하는 서비스를 제공하기 위한 최소한의 기능을 하는 엔드투엔드(End-To-End) 파이프라인 설계 가장 간단한 파이프라인의 성능을 평가하는 다양한 시각 (모델 성능, 사용성, 비용, 속도 등)     학습과 추론 파이프라인 예시\n  3. 모델 학습과 디버깅    토이프로젝트와 실제 산업에서의 차이\n   모델 학습 과정  토이 프로젝트가 아닌 실제 제품 개발을 위한 데이터 준비 과정 반복적인 과정을 거친 모델 학습 과정   실제 서비스되는 모델의 정상적인 동작을 위한 테스트 과정  4. 모델 배포와 모니터링    스트리밍 배포 방식\n     배치 배포 방식\n   사용자가 실제로 머신러닝 모델을 사용하기 위한 다양한 배포 방식 배포 과정에서 생길 수 있는 다양한 문제점들 모델의 이상 작동을 포착하고, 데이터 분포를 감지하기 위한 모니터링의 중요성     데이터 변화에 따른 재학습 주기 예시\n     이상탐지를 통한 모델의 비정상적인 동작 포착 예시\n  마무리하며 이 책은 기본적으로 다양한 예시를 들어 머신러닝 기반 제품이 만들어지는 과정을 설명한다. 따라서 읽는 사람에 내용의 흐름이 약간은 혼란스러울 수 있다. 일반적인 내용만 파악하더라도 실제로 만들고자 하는 서비스를 구현하는데에는 충분히 참고할 수 있는 책이기 때문에, 예시와 예제 코드에 너무 집중할 필요는 없다고 생각한다.\n또한 위에서 서술한 바와 같이 머신러닝의 기초적인 내용은 다루지 않기 때문에, 어느 정도 개념이 잡힌 상태에서 읽는 것을 권장한다.\n+) 현업 CTO / 머신러닝 엔지니어 / 데이터 사이언티스트 들의 인터뷰는 피가 되고 살이 되는 조언들이 담겨있으니 꼭 읽어보자 !!\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://lucaseo.github.io/posts/2022-03-27-book-review-ml-powered-application/","summary":"머신 러닝 파워드 애플리케이션 (저자: 에마뉘엘 아메장)\n  추천 대상:\n 머신러닝이 적용된 제품을 만들고자 하는 분 머신러닝 엔지니어로 취업/이직하고자 하는 분  한줄평: 데이터 수집, 전처리, 학습 \u0026hellip; 이제 이걸 가지고 뭘 하지?\n머신러닝 모델은 서비스가 될 때 진정한 가치가 있다 “머신러닝에 대해 배운다” 라고 하면 대부분의 커리큘럼은 갖가지 알고리즘과 그 작동 방식에 대해 배우는 것부터 시작한다. 가장 간단한 형태의 모델부터 최신의 복잡한 구조의 모델까지 훑고, 토이 데이터셋으로 실습까지 진행한다.","title":"[KR] 책 리뷰 : 머신러닝 파워드 애플리케이션"},{"content":"   금융 전략을 위한 머신러닝 (저자: 하리옴 탓샛, 사힐 푸리, 브래드 루카보)\n   추천 대상: 금융업계에 종사하는 PM / 기획자 / 소프트웨어 개발자 한줄평: 무엇을 좋아할 지 몰라서 다 준비해봤어  금융 서비스를 위한 머신러닝? 2022년 현재 머신러닝과 데이터사이언스는 커머스, 소셜, 스포츠 등 수많은 분야에서 각자 필요한 영역을 찾아 적용되어지고 있다. 머신러닝을 직접적으로 개발에 관여하지 않는 부서의 사람들이 봤을 때는 크게 단순히 사람의 노동을 대체할 수 있는 자동화의 측면이거나, 혹은 더 나아가 인간의 역량으로는 쉽게 할 수 없던 작업을 할 수 있는 알고리즘을 만드는, 이렇게 2가지로 인식된다. 모두가 “인공지능\u0026quot;, “데이터 사이언스\u0026quot;, “머신러닝\u0026quot;, “딥러닝\u0026quot; 이라는 기법을 적용해서 혁신을 이루어내겠다는 열의를 불태우는 가운데, 가장 그 반응이 강한 분야 중 하나는 바로 금융 섹터이다.\n금융이라는 단어는 쉽게 다가올 수 있지만, 사실 그 아래에는 은행, 증권시장, 보험, 자산운용, 신용카드, 신용평가 등 수많은 세부 영역들과 해결하고자 하는 문제들이 존재한다. 더불어 그 어느 영역보다 나라의 규제가 많이 적용되는 분야이기도 하기 때문에, 마음대로 할 수 없는 부분이 많다. 따라서 금융 섹터에서는 머신러닝 전문가의 경우 금융 배경의 복잡한 도메인 지식과 규제에 대한 이해가 부족하고, 반대로 금융 도메인 지식이 풍부한 인력은 머신러닝에 대한 지식이 부족한 경우가 많다. (둘 다 잘 알면 서로 모셔가려는 슈퍼스타고 \u0026hellip; ) 그만큼 다른 분야 비해 머신러닝 전문가와 비전문가가 긴밀한 관계를 유지하며 함께 일해야 할 필요가 있는 분야가 바로 금융이다.\n금융 문제에 접근하기 위한 머신러닝 기법의 \u0026ldquo;간단\u0026quot;하고 최대한 \u0026ldquo;다양한\u0026rdquo; 설명 [금융 전략을 위한 머신러닝]에서는 다음과 같은 내용을 다룬다.\n 금융 분야에서 머신러닝이 적용될 수 있는 영역들  알고리즘 트레이딩, 로보어드바이저, 이상 거래 탐지, 대출 심사, 위험 관리, 돈세탁 방지, 감성 분석 등의 개념을 아주 간략하게 소개한다.   통상적으로 머신러닝 프로젝트를 위해 가장 많이 쓰이는 파이썬과 관련 프레임워크  파이썬과 머신러닝을 개발하는 단계를 간략히 설명한다.   금융 문제에 접근하기 위한 머신러닝 기법의 소개  머신러닝의 큰 줄기와 알고리즘에 대해 설명한다.  지도학습: 회귀와 분류, 시계열 모델 비지도학습: 차원축소, 클러스터링 강화학습: 강화학습 자연어처리(NLP)에 대한 소개      또한 이 책에서는 간단한 데이터셋을 통해 실제 프로젝트와 비슷한 환경에서 어떠한 머신러닝 기법을 적용할 수 있는지 예제를 제공한다.\n머신러닝 관련 금융 프로젝트를 이끌어가야 하는 분들을 위한 입문서 저자는 [금융 전략을 위한 머신러닝]이 관련업종에서 종사하는 데이터 사이언티스트, 데이터 엔지니어, 머신러닝 설계자, 퀀트 연구원 등의 직군에 적합하다고 했지만, 단도직입적으로 머신러닝 연구자나 엔지니어를 위한 책은 아니다. 실제로 데이터 관련 직군 종사자들은 머신러닝 알고리즘의 개념과 프레임워크 사용법, 코드는 이미 숙지하고 있을 것이다. 또한 이 책에서 제공하는 예제가 실제 현업의 프로젝트와 난이도는 분명히 차이가 있으므로, 그대로 활용하는 것은 무리가 있을 것이다.\n반대로 내가 오히려 추천하는 대상은 금융 분야에 종사하는 관리자, PM, 기획자 또는 소프트웨어 엔지니어이다. 머신러닝으로 금융 도메인의 문제에 접근하는 측면에서 훌륭한 입문서로 활용하여, 이 책이 제공하는 기본적인 이해를 바탕으로, 데이터 관련 직군의 동료와 함께 프로젝트를 이끌어나갈 수 있는 기반을 쌓을 수 있을 것이라 생각한다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://lucaseo.github.io/posts/2022-02-20-book-review-ml-ds-blueprint-finance/","summary":"금융 전략을 위한 머신러닝 (저자: 하리옴 탓샛, 사힐 푸리, 브래드 루카보)\n   추천 대상: 금융업계에 종사하는 PM / 기획자 / 소프트웨어 개발자 한줄평: 무엇을 좋아할 지 몰라서 다 준비해봤어  금융 서비스를 위한 머신러닝? 2022년 현재 머신러닝과 데이터사이언스는 커머스, 소셜, 스포츠 등 수많은 분야에서 각자 필요한 영역을 찾아 적용되어지고 있다. 머신러닝을 직접적으로 개발에 관여하지 않는 부서의 사람들이 봤을 때는 크게 단순히 사람의 노동을 대체할 수 있는 자동화의 측면이거나, 혹은 더 나아가 인간의 역량으로는 쉽게 할 수 없던 작업을 할 수 있는 알고리즘을 만드는, 이렇게 2가지로 인식된다.","title":"[KR] 책 리뷰 : 금융 전략을 위한 머신러닝"},{"content":"20년 당시 회고글을 돌아보니, 키워드 별로 정리해놓은 포맷이 나쁘지 않은 것 같아 이번에도 같은 포맷으로 회고를 해보고자 합니다.\n\u0026amp;nbsp\n올해의 키워드 1. 올해의 고난 (자세히 기술하지는 못 합니다) 회사에서 기존에 맡고 있던 업무의 비즈니스 모델(BM)이 올해 중반을 기점으로 전혀 예상하지 못 했던 방향으로 계획을 변경되면서 개인적으로는 하반기가 참 힘이 들었습니다. 모든 팀원들이 한번도 경험해보지 못 한 “처음 해보는 일”을 짧은 시간 내 해내야하는 프로젝트가 시작이 되었습니다. 이로 인해 기존에 계획되었던 것보다 더 급하게 자회사가 설립되고, 짧은 시간동안 많은 외부 인사 영입이 이루어졌습니다. 제 역할도 많이 붕 떠버렸고, 제 분야가 아님에도, 현재 할 수 있는 인력이 없기 때문에 떠앉아버린 업무가 많아졌습니다. 해당 업무를 전문성을 가지고 해나갈 수 있는 인력의 채용 계획이 있는지도 확인되지 않았습니다. 그런 와중에 첫번 째 마일스톤은 2022년 2월, 그 다음 마일스톤은 2022년 5월으로 잡혀 있구요. 체력, 감정 소모에 이어 번아웃이 몇 차례 왔었는데요, 그래서 그런지 다른 사람들처럼 활기차게 2022년! 을 맞이할 수 없는 것 같습니다. 어떻게 하면 슬기롭게 헤쳐나갈 수 있을까요? 2022년 1분기부터 고민이 많아집니다.\n\u0026amp;nbsp\n2. 올해의 가장 큰 변화 그럼에도 불구하고 2021년 가장 긍정적으로 받아들이는 변화는, 바로 같은 방향성을 가진 동료가 생겼다는 것입니다. 작년 하반기에 합류한 동료 S와 올해 중순부터 업무적인 대화를 많이 하기 시작했고, 팀과 회사의 프로덕트에 대한 고민을 현실적으로 의논할 수 있었습니다. 문제가 있다면 그 문제가 실제로 있음을 인정하고 구체적인 해결방안을 논의할 수 있는, 그런 제 상식에 부합하는 동료가 생겼다는 것은 여태까지 없던 아주 긍정적인 변화입니다.\n2021년에는 동료 S와 함께 주도적으로 나서서 우리 팀만의 시스템을 갖추기 시작했습니다. 기존에 문제라고 생각했던 “내용이 중구난방인 미팅”과 “목적 없는 모이는 미팅”을 없애기로 했습니다. 미팅에서 다루는 내용을 세분화하여 서로 업무를 공유하기 위한 스탠드업 미팅, 기술적인 내용을 공유하기 위한 미팅, 비즈니스팀과 커뮤니케이션 할 미팅을 정의했습니다. 주기적인 미팅이 각각의 명확한 목적을 띄면서, 업무 공유가 훨씬 투명하게 이루어지고, 자연스럽게 서로 피드백 해줄 수 있게 되었습니다. 이전보다 일정 관리가 수월해지고, 업무에 대한 토론도 원활해진 것 또한 큰 수확이라고 생각됩니다.\n이렇게 작은 부분부터 함께 맞춰보았고, 이제 2022년에는 팀의 비전과 운영 방식, 인력 구성, 충원, 성장, 외부 프로모션 등을 어떻게 해나갈지 함께 고민하고 있습니다. 정답은 없지만 함께 논의하면서 최선의 방식을 추구하는 이 일련의 과정이 꽤 긍정적이었습니다. 역량도 뛰어나고 경험도 있기 때문에 늘 배우는 점이 많아서, 저도 민폐 끼치지 않는 좋은 동료가 되고 싶다는 동기부여도 되고 있습니다. 그래서 참 감사하고 2022년에 어떤 일을 같이 해나갈지 기대가 되는 부분입니다.\n\u0026amp;nbsp\n3. 올해의 큰 수확 올해 가장 큰 수확은 무엇보다 건강을 챙기는 습관을 들인 것과, 운동을 시작한 것입니다. 2021년 2월 말부터 집 바로 앞에 위치한 필라테스 센터에서 1:1 운동을 시작했습니다. 직군 특성 상, 오래 앉아있으면서 자세가 흐트러지기 쉽고, 체력도 점점 안 좋아지는 것을 느껴서 운동을 꼭 해야겠다고 느끼는 와중에 필라테스에 도전해보기로 했습니다. 필라테스는 그 무엇보다 코어에 많은 포커스를 맞추는 운동입니다. 코어 운동을 많이 하면서 자연스럽게 자세가 덜 흐트러지고, 체력이 많이 늘었습니다. 무엇보다 제대로된 좋은 선생님을 만났다는 것이 정말 다행이기도 하구요. 운동 할 때마다 너무 힘들어서 선생님 참 미운데요 \u0026hellip; 그러니까 좋은 선생님이겠죠 \u0026hellip; ?\n그리고 2021년 10월부터는 테니스를 다시 시작하게 되었습니다. 집 근처 안양천에 테니스장이 개장하면서, 주 3회 아침 7시에 레슨을 받고, 주말에도 종종 다른 분들과도 테니스를 치고 있습니다. 테니스는 중3 때 처음 조금 배우고, 그 뒤로는 근처에 코트가 없어서 또는 같이 칠 사람이 없어서, 할 수 없었는데요, 이번에 기회가 생겨서 다시 열심히 치고 있습니다. 테니스는 신체 접촉이 없어서 코로나 시대에 많은 인기를 얻고 있는 운동이기도 하고, 테니스공을 치면서 스트레스도 해소할 수 있어서 최근에 멘탈 관리에도 큰 도움이 되고 있습니다. 주변에도 테니스 치는 사람들이 많이 생겼으면 좋겠어요. 이번에 테니스를 치는 인스타그램 계정도 새로 팠어요. 함께 테니스 칠 분을 찾습니다. 많관부 !! 꼭 연락 주세요 :)\n2021년에 기대하는 것 MLOps MLOps에 대한 공부를 따로 하고 있습니다. 2021년에는 MLOps에 대한 철학과 다양한 use case 에 대해 알아보았다면, 2022년에는 우리 회사에 어떤 부분에 어떻게 MLOps가 필요할 지에 대한 본격적인 고민과 이를 실제로 적용해보게 될 것입니다. 매 순간 순간이 새로운 의사결정이 될 텐데요, MLOps에 대한 배경지식이 없는 팀원들에게 이를 어떻게 이해시키고 커뮤니케이션 할 지에 대해서도 정리를 해야겠습니다.\n글또 글또의 다음 기수 참여도 벌써부터 기대하고 있습니다. 이번에는 MLOps와 도서 리뷰, 그리고 팀운영에 대한 글을 써보게 될 것 같습니다. 그리고 회사에서 우리 팀 자체적으로도 대외 홍보를 위한 방법 중 하나로 컨퍼런스 참여와 기술 블로그 운영을 고민하고 있는데요, 글또를 통해 글을 작성한 경험과, 글또에서 많은 분들이 보여주시는 훌륭한 포스팅을 참고해서 기술 블로그를 런칭할 계획에 있습니다.\n마무리하며 2021년을 되돌아보면 혼돈 속에 시간만 빨리 갔습니다. 그 와중에 하는 일 속에서 의미를 찾으려고 고군분투 했던 해라고 자평해보게 되네요. 그렇지만 2022년에는 뚜렷한 동기부여가 생겼고, 앞으로 잘 풀릴 거라는 기대가 있습니다. 무엇보다 2022년을 회고할 때는 어떤 분위기일까요, 그때 다시 한번 이 글을 꺼내보아야겠습니다.\n","permalink":"https://lucaseo.github.io/posts/2022-01-01-review-2021/","summary":"20년 당시 회고글을 돌아보니, 키워드 별로 정리해놓은 포맷이 나쁘지 않은 것 같아 이번에도 같은 포맷으로 회고를 해보고자 합니다.\n\u0026amp;nbsp\n올해의 키워드 1. 올해의 고난 (자세히 기술하지는 못 합니다) 회사에서 기존에 맡고 있던 업무의 비즈니스 모델(BM)이 올해 중반을 기점으로 전혀 예상하지 못 했던 방향으로 계획을 변경되면서 개인적으로는 하반기가 참 힘이 들었습니다. 모든 팀원들이 한번도 경험해보지 못 한 “처음 해보는 일”을 짧은 시간 내 해내야하는 프로젝트가 시작이 되었습니다. 이로 인해 기존에 계획되었던 것보다 더 급하게 자회사가 설립되고, 짧은 시간동안 많은 외부 인사 영입이 이루어졌습니다.","title":"[KR] 2021년이 지났다."},{"content":"   빅데이터 시대, 성과를 이끌어내는 데이터 문해력 (저자: 카시와기 요시키)\n   추천 대상: \u0026ldquo;데이터를 들여다보면 되지 않을까?\u0026rdquo; 하는 분들 한줄평: 데이터에는 답이 없다고? 데이터에서 답을 찾으려던거 아니었어?  데이터 문해력 Data Literacy 데이터, 2010년대 후반 들어 가장 많이 언급되는 단어가 아닐까 합니다. 이런 트렌드에 힘입어 데이터 문해력(Data literacy) 또한 많은 사람들이 관심을 가지고 있는 키워드라고 할 수 있습니다.\n데이터 문해력은 어떻게 정의 내릴 수 있을까요? \u0026ldquo;데이터가 쏟아져 흘러 넘치는 빅데이터 시대에 데이터를 읽고 무언가를 해석하는 능력\u0026quot;이라고 하면 알맞은 정답이 될 수 있을까요?\n이 책의 저자는 이 책을 통해서 데이터 리터러시란, \u0026ldquo;문제 해결 방법에 대해 스스로 정답을 고민하며, 데이터를 활용해 논리적으로 이를 풀어내는 능력\u0026rdquo; 이라 자신의 생각을 전합니다. 그리고 \u0026ldquo;어딘가에 있을 정답을 찾으러 간다\u0026rdquo; 는 식의 접근 방법은 학교 시험에나 해당되는 말과 함께 말이죠. (아 뼈 아퍼)\n   데이터 보기 전에 생각했나요?\n  저자는 이렇 듯, 온화한 화법으로 쉬지 않고 독자에게 질문을 던집니다. 그 질문 하나하나가 팩폭이라 정신이 번쩍 들게 하는데요, 데이터 직군 뿐만 아니라 데이터를 접하는 마케터, 기획자, 애널리스트 등 많은 분들께 유용한 지침들로 가득하다는 생각이 들었습니다.\n책을 읽으며 개인적으로 와닿았던 몇가지 내용을 아래와 같이 정리해봤습니다. 중간중간 내용이 뜨끔했다면 여러분도 책을 직접 읽어보시는 것을 강력 추천합니다!\n데이터를 다룬 다는 것은 무엇을 의미할까?  \u0026ldquo;점점 편리해지는 기계와 도구들이 많아져 사용법과 조작법만 익히면 전부 알아서 해주고 원하는 것까지 손에 넣을 수 있는가라고 묻는다면 그 대답은 아쉽지만 NO입니다.\u0026quot;\n 최근 들어 데이터 분석, 인공지능이 유망한 분야로 떠오르게 되면서, \u0026ldquo;데이터 분석을 배우고 싶습니다, 파이썬을 해야하나요? R을 해야 하나요? SQL은 어느 수준까지 해야 하나요?\u0026rdquo; 와 비슷한 질문들을 많이 접하게 됩니다. 그리고 많은 사람들이 데이터를 다루는 기술이 곧 데이터를 분석하는 행위라고 생각하는 모습을 보게 돼죠. 물론 기술을 익히는 것도 중요합니다. 하지만 이러한 기술은 \u0026ldquo;컴퓨터가 인간보다 빠르고 실수 없이 잘 하는 작업들을 컴퓨터에게 시키기 위한 것\u0026quot;에 목적을 두고 있으며, 도구에 지나지 않습니다.\n그렇다면 도구를 활용하는 방법만 숙달했을 때 어떤 일들이 벌어지게 될까요? 아마 아래와 같은 단계를 밟게 될 것 입니다.\n1. 데이터를 활용해서 무언가를 해보고 싶다. 데이터로 성과를 내고 싶어 ... 2. 오 눈 앞에 데이터가 있네! 그리고 난 데이터 분석 도구를 사용할 줄 알아. 3. 그럼 일단 통계값부터 내보자. 4. 그리고 시각화를 해보자. 5. 통계값을 낸 결과와 시각화를 한 결과는 이러하다. 6. 난 데이터를 분석했어! 이렇게 당장 눈 앞에 놓인 데이터를 가지고 이리저리 통계값을 내고 그래프를 만들고 ~가 증가했다, ~가 ~보다 크다 와 같은 해석을 하게 되겠죠. 하지만 여기까지가 한계입니다\u0026hellip; \u0026ldquo;그래서 뭐?\u0026quot; 라는 추궁을 받게 되면 남은 건 내거친생각과불안한눈길과그걸지켜보는 흔들리는 동공 뿐입니다.\n   (저렇게 모니터 보면 목이랑 허리 나가요 \u0026hellip; )\n  반면, 저자가 제시하는 방식은 좀 다릅니다.\n1. 내가 알고 싶은 것은 무엇인지 2. 어떤 문제를 해결하고 싶은지, 앞서 알게 된 것을 토대로 무엇을 하고 싶은지 3. 어떤 데이터를 봐야 하는지 4. 어떤 지표를 적용할 지 즉, 저자는 어떠한 문제를 해결하기 위해 어떤 목적을 가지고 어떠한 데이터를 사용할 지 설계해나가고, 결과를 토대로 의사결정을 해나가는 데이터를 기반한 사고력(critical thinking)을 갖추는 것이 핵심이라고 강조합니다.\n데이터를 제대로 활용하지 못 하는 이유? 한편, 데이터를 보유하고 있고 데이터 분석을 진행했음에도 불구하고, 시원한 개선이 없다거나 제대로 활용하지 못 했다는 찜찜함이 남는 경우가 있습니다. 왜 그런 것일까요?\n풀고자 하는 문제가 명확하지 않다 문제를 제대로 정의하지 않은 상태에서 당장 눈 앞에 있는 데이터에 달려들어버리면 결과가 좋을 수 없습니다. 우리는 데이터를 들여다보기에 앞서 문제를 풀고자 하는 목적, 문제, 원인, 해결 방안을 구분해서, 구체적인 언어로 정의 해야 합니다.\n저 또한 문제의 윤곽만 어렴풋이 보이는 상태에서 데이터만 탐색했던 적이 떠올라서 뼈를 맞은 듯한 기분을 지울 수가 없네요. 최근에 들어서야 문제를 명확히 정의한다 라는 것의 의미를 알고 이행하려고 하고 있습니다.\n정의한 문제와 사용하는 데이터가 일치하지 않는다 문제와 목적이 정해지면, 이를 객관적으로 측정하기 위한 올바른 지표가 설정되어야 하고, 우리가 보는 데이터가 이 지표를 통해 설명이 되어야 합니다.\n예를 들어, 우리가 측정하고 하는 것은 시간인데, 우리가 사용하는 데이터는 무게와 거리, 사람의 수 등 엇나간 데이터라면 문제를 활용하기 위한 적절한 도구가 될 수 없겠죠. 다시 말하지만, 데이터가 무게와 거리, 사람의 수가 있기 때문에 지표를 무게, 거리, 사람의 수로 정하는 것이 아닙니다. 우리가 정의한 문제에서 시간을 보고자 한다면, 우리가 사용하는 데이터 또한 시간을 지표로 측정할 수 있어야 합니다. 일의 순서를 혼동하지 맙시다!\n결과가 나왔다. 이제 무엇을 할 것인가?  \u0026ldquo;조직에서 데이터를 잘 활용하고 있다는 것은 그 정보를 통해 문제 방안을 수립하거나 구체적인 행동계획을 세우거나 관계자들이 납득할 만한 합의 또는 판단을 내릴 수 있다는 이야기입니다. 즉, 결론으로 유도하지 못하는 정보는 가치가 별로 없으며, 목적에 이르지 못한 어중간한 상태입니다. 그런데 실제로 이 상태를 끝으로 데이터를 활용했다고 말하는 경우가 압도적으로 많습니다.\u0026quot;\n 혹시 데이터 정리해서 현황만 파악 해놓고 데이터를 활용했다고 하진 않았나요? (네 그게 바로 접니다) 저자가 뼈를 때렸 듯, 데이터를 도구 삼아 문제를 해결하기 위한 계획(action plan)을 수립했을 때 우리는 비로소 데이터를 활용했다 말 할 수 있을 것입니다.\n다만, 현재 상태를 안다고 해서 바로 해결 방안을 찾거나 조치를 취하는 단계로 넘어가는 것을 주의해야 합니다. 저자는 데이터 정리를 통해 파악된 현재 상태에 대한 원인을 집요하게 파고드는 훈련을 잊지 말아야 하며, 만약 해결 방안이 성과를 내지 못 한다면 원인을 제대로 짚어내지 못 한 것은 아닌지를 검토해 볼 것을 강조합니다.\n    \u0026ldquo;원인은 항상 단순한 구조가 아닙니다. 가급적 \u0026lsquo;어째서\u0026rsquo;, \u0026lsquo;왜\u0026rsquo;를 반복해서 더욱 \u0026lsquo;본질적\u0026rsquo;인 원인까지 파고들어야 정확하고 밀도 있는 해결 방안을 도출할 수 있습니다. \u0026hellip; (중략) 원인이 명확히 규명된 후 수립하는 대책과 그런 과정 없이 즉흥적으로 만든 대책은 그 효과와 정확성에 엄청난 차이가 있습니다. 논리적인 흐름과 구조에 대해 생각하는 것에 비하면 구체적인 방법을 고민하는 것이 즐겁고 편하므로 이를 우선하기 쉽습니다. 하지만 \u0026lsquo;해결방안\u0026rsquo;을 고민하는 것은 마지막 단계라는 것을 언제나 염두에 두시기 바랍니다.\u0026quot;\n 마무리하며 막상 책의 내용을 다시 곱씹어보며 정리해보자니, 전부 지극히 당연하면서도 원론적인 이야기가 많았습니다. 그만큼 이전에는 기본에 충실하지 않았다는 뜻이 아닐까 되돌아보게 되는 계기가 되었네요.\n240페이지 밖에 안 되고 심지어 한 페이지에 글자 수도 많지 않은 얇은 도서임에도 불구하고 시종일관 뼈를 때리는 내용으로 가득해서(이미 남아나는 뼈가 없\u0026hellip;), 저는 당분간은 종종 일하면서 이 책을 가이드라인으로 삼으려고 합니다.\n","permalink":"https://lucaseo.github.io/posts/2021-11-21-book-review-data-literacy/","summary":"빅데이터 시대, 성과를 이끌어내는 데이터 문해력 (저자: 카시와기 요시키)\n   추천 대상: \u0026ldquo;데이터를 들여다보면 되지 않을까?\u0026rdquo; 하는 분들 한줄평: 데이터에는 답이 없다고? 데이터에서 답을 찾으려던거 아니었어?  데이터 문해력 Data Literacy 데이터, 2010년대 후반 들어 가장 많이 언급되는 단어가 아닐까 합니다. 이런 트렌드에 힘입어 데이터 문해력(Data literacy) 또한 많은 사람들이 관심을 가지고 있는 키워드라고 할 수 있습니다.\n데이터 문해력은 어떻게 정의 내릴 수 있을까요? \u0026ldquo;데이터가 쏟아져 흘러 넘치는 빅데이터 시대에 데이터를 읽고 무언가를 해석하는 능력\u0026quot;이라고 하면 알맞은 정답이 될 수 있을까요?","title":"[KR] 책 리뷰 : 빅데이터 시대, 성과를 이끌어내는 데이터 문해력"},{"content":"데이터 사이언스, 머신러닝 프로젝트를 수행하기 위해서는 다양한 파라미터를 실험하는 과정이 동반됩니다. 이번 글에서는 파라미터와 설정값을 간결하게 관리하고 사용할 수 있게 도와주는 Hydra에 대해 알아보도록 하겠습니다.\n1. Hydra란 Hydra는 페이스북에서 오픈소스로 공개한 프레임워크로, 어플리케이션에서 사용하는 여러가지 설정값을 관리할 수 있는 기능을 제공합니다.\n직접 사용해본 Hydra는 아주 명확한 특징을 가지고 있습니다.\n 모든 설정 및 파라미터 값은 config.yaml로 관리하고 계층적으로 설정 그러한 와중에 command-line을 통해서 오버라이딩(overriding) 가능 한번의 명령어로 각각 다른 값을 대입하여 다중 실행 가능  이렇게 설명해도 와닿지 않을 수가 있겠죠. 그럴 때 저는 이렇게 설명합니다.\n \u0026ldquo;더 이상 실험 한번 돌릴 때마다 파라미터 값을 스프레드시트에 적어두지 않아도 돼\u0026rdquo;\n 2. Hydra 기본적인 사용 방법 2.1. 설치하기 pip install hydra-core --upgrade 2.2. Hydra로 Config 불러오기 2.2.1. Config 저장 형태 기본적으로 Hydra에서는 Yaml형태로 config값들을 저장합니다.\n예를 들어, 아래와 같은 파일 구조를 가지고 있다고 할 때,\n. ├── config │ └── config.yaml └── app.py config.yaml 파일에 파라미터를 정의해보았습니다.\n# ./config/config.yaml train_params: epoch: 5 batch_size: 10 learning_rate: 1e-4 2.2.2. Config 불러오기 실제로 config를 불러올 어플리케이션에서는 경로와 파일명을 전달하여 config를 불러옵니다. 이렇게 불러온 config의 데이터타입은 DictConfig 입니다.\n# ./app.py import hydra @hydra.main(config_path=\u0026#34;config\u0026#34;, config_name=\u0026#34;config\u0026#34;) def func(config): # access elements of the config print(type(config)) print(config) if __name__ == \u0026#34;__main__\u0026#34;: func() $ python app.py \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001}} Hydra는 ./output 디렉토리 아래 각 실행마다 결과 또는 로그의 출력을 저장합니다.\n. ├── app.py ├── config │ └── config.yaml └── outputs └── 2021-11-07 └── 22-11-07 └── app.log 3. Config값 오버라이드 하기 앞서 예시의 config.yaml 파일에서는 epoch를 5, batch_size는 10으로 설정했습니다. 그렇다면 다음 실험은 epoch를 2, batch_size는 10로 설정하고 싶다면 config 파일을 내용을 수정해야 할 까요?\nHydra는 command line에서 config 값에 대한 파라미터 오버라이팅을 지원하기 때문에, 파일의 내용을 수정할 필요가 없습니다.\n아래의 명령어를 보면 이해가 빠를 것 같은데요, command line에서 epoch값을 2로 오버라이드 한 실행 결과입니다.\n$ python app.py train_params.epoch=2 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 2, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001}} 애초에 설정하지 않았던 값을 +를 통해 추가할 수도 있습니다.\n$ python app.py train_params.epoch=2 +train_params.random_state=42 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 2, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;random_state\u0026#39;: 42}} +) Config Group 을 적용하면 더욱 다양하고 복잡한 형태의 config 오버라이딩이 가능합니다.\n4. Hydra를 통해 다중실행하기 Hydra에서는 --multiruns 인자를 통한 다중 수행을 할 수 있다는 것도 아주 유용합니다.\n$ python app.py --multirun train_params.learning_rate=0.0001,0.001,0.01,0.1 [2021-11-07 23:09:31,088][HYDRA] Launching 4 jobs locally [2021-11-07 23:09:31,088][HYDRA] #0 : train_params.learning_rate=0.0001 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001}} [2021-11-07 23:09:31,156][HYDRA] #1 : train_params.learning_rate=0.001 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.001}} [2021-11-07 23:09:31,230][HYDRA] #2 : train_params.learning_rate=0.01 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.01}} [2021-11-07 23:09:31,297][HYDRA] #3 : train_params.learning_rate=0.1 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.1}} +) Hydra는 기본적으로 다중실행을 순차적으로 수행하지만, 병렬 실행을 하고자 한다면 Joblib Launcher가 동반되어야 합니다.\n마무리하며 처음에는 데이터 모델을 실험할 때 config를 쉽게 관리할 수 있는 방법에 대해서 찾아보는 중에 Hydra를 발견하게 되었습니다. 하지만 사용법에 대해 알아보면서, 단순히 데이터사이언스와 머신러닝 뿐만 아니라 그 외 어플리케이션에도 다양하게 접목할 수 있는 유용한 프레임워크라는 생각이 드네요. 더욱 고도화해서 실제 프로젝트에 접목할 방법을 구상해봐야겠습니다.\nReference  https://hydra.cc/docs/intro https://pjt3591oo.github.io/hydra_translate/build/html/index.html  ","permalink":"https://lucaseo.github.io/posts/2021-11-07-hydra-for-machine-learning/","summary":"데이터 사이언스, 머신러닝 프로젝트를 수행하기 위해서는 다양한 파라미터를 실험하는 과정이 동반됩니다. 이번 글에서는 파라미터와 설정값을 간결하게 관리하고 사용할 수 있게 도와주는 Hydra에 대해 알아보도록 하겠습니다.\n1. Hydra란 Hydra는 페이스북에서 오픈소스로 공개한 프레임워크로, 어플리케이션에서 사용하는 여러가지 설정값을 관리할 수 있는 기능을 제공합니다.\n직접 사용해본 Hydra는 아주 명확한 특징을 가지고 있습니다.\n 모든 설정 및 파라미터 값은 config.yaml로 관리하고 계층적으로 설정 그러한 와중에 command-line을 통해서 오버라이딩(overriding) 가능 한번의 명령어로 각각 다른 값을 대입하여 다중 실행 가능  이렇게 설명해도 와닿지 않을 수가 있겠죠.","title":"[KR] Hydra를 활용해서 Config 관리 효과적으로 해보자"},{"content":"Streamlit은 파이썬 기반으로 웹어플리케이션을 개발할 수 있는 프레임워크입니다. 이번 포스팅에서는 Streamlit에서 제공하는 서비스를 통해 아주 쉽고 간단하게 내가 개발한 웹을 배포하는 방법에 대해 소개하고자 합니다.\n웹 어플리케이션 개발 \u0026hellip; 그 다음에는? 웹개발과 다소 거리가 먼 데이터분석가, 데이터 사이언티스트 분들에게 Streamlit은 분석 결과를 공유하거나, 학습한 모델을 제품으로 프로토타이핑 할 수 있는 아주 강력한 도구로 자리 잡았습니다.\n하지만 나 혼자만 보고 사용하면 무슨 소용일까요\u0026hellip;? (도발)\n웹어플리케이션 개발과 별개로 배포 또한 하나의 장벽으로 느껴질 수 있는데요, Streamlit Sharing 은 Streamlit 어플리케이션을 배포하기 위해 알아야 할 모든 지식과 과정을 극도로 간소화 했습니다. (이 정도로 간소화가 아닌 삭제라고 봐도 무방합니다.)\nStreamlit Sharing Streamlit Sharing은 Streamlit에서 자체적으로 제공하는 Streamlit 어플리케이션 배포, 관리 및 공유 플랫폼입니다.\n기본적으로 무료 계정의 경우 3개의 어플리케이션까지 배포할 수 있고, public 레포지토리(Github)만 배포가 가능합니다. 또한 하나의 어플리케이션 당 프로비저닝 되는 리소스는 RAM 1GB로 제한됩니다. (그외 소규모 팀단위 플랜과 엔터프라이즈 플랜이 옵션으로 있습니다. 더 자세한 사항은 링크 를 참고해주세요)\nStreamlit Sharing에서 웹어플리케이션을 배포하는 과정은 정말정말 단순하고 간단합니다.\n Streamlit Sharing에 가입하고 Github 계정을 연동 배포하고자 하는 Streamlit 어플리케이션을 설정  Github의 repo를 선택 브랜치를 선택 실행하고자 하는 어플리케이션 파일을 선택(app.py)   어플리케이션을 수정할 경우에는 선택한 브랜치에 git push 만 하면 변경된 사항이 반영  (참쉽죠1)\n하나씩\n1. 가입 및 Github 계정 연동  https://share.streamlit.io/에서 Sign in 합니다.     오른쪽 상단 Settings를 클릭 후 \u0026ldquo;Sign in with Github\u0026quot;을 통해 Github 계정을 연동합니다.     2. Github 레포지토리 연결    New app에서 배포하고자 하는 Streamlit 어플리케이션의 Github 레포지토리를 연결합니다.\n2.1. 레포지토리, 브랜치, 앱파일 선택 이번 예제에서 사용할 레포지토리 streamlit_app_stockprice_downloader의 파일 구조는 아래와 같습니다.(링크)\n. ├── README.md ├── app.py ├── prepare_data.py ├── requirements.txt ├── resource │ ├── stock_list.json │ └── stock_market.json └── utils ├── __init__.py ├── utils.py └── utils_fdr.py 아래와 같이 레포지토리와 브랜치, 그리고 실행하고자 하는 파일을 선택합니다.\n   2.2. (Optional) 파이썬 버전, 환경변수 설정하기 Advanced Settings를 통해 파이썬의 버전을 선택할 수 있고, 인증이나 DB 관련 민감한 정보를 TOML 포맷으로 하여 환경변수로 설정할 수 있습니다.\n   2.3. 배포하기 ! Deploy 버튼을 클릭하면, Streamlit Sharing에서 자동으로 어플리케이션을 위한 리소스를 할당하고, 연동된 Github 레포지토리 내 requirments.txt에서 정의된 라이브러리를 설치한 후, Streamlit 어플리케이션을 실행시킵니다. (참쉽죠2)\n      예제에서 실행된 웹어플리케이션의 링크에서 보실 수 있습니다.\n3. 배포 후 어플리케이션을 수정할 경우에는 별다른 과정 없이, 그저 처음에 설정한 브랜치에 git push 만 하면 Streamlit Sharing에서는 continous deployment를 할 수 있습니다. (참쉽죠3)\n마무리 하며 Streamlit Sharing이라는 플랫폼을 통해 간단히 Streamlit 기반 웹어플리케이션을 배포하는 과정에 대해 알아보았습니다.\n한 가지 아쉬운 점은 어플리케이션의 규모나 ML모델의 사이즈가 다소 큰 상황일 경우에는 실행하고자 하는 앱에 따라 무료 플랜의 1GB RAM의 리소스가 제한적일 수 있다는 것 입니다.(마찬가지로 Heroku도 기본 무료 플랜은 500MB RAM의 리소스로 제한되어 있습니다)\n따라서 다음 포스팅에서는 Streamlit Sharing이나 Heroku 같은 PaaS가 아닌, 메이저 클라우드 플랫폼에서 웹어플리케이션의 배포와 CI/CD를 하는 과정을 다루어보도록 하겠습니다.\nP.S. Streamlit에 대한 다른 포스팅들은 아래를 참고해주세요 !\n  Streamlit 소개 글 Heroku에 Streamlit 웹어플리케이션 배포하기 Streamlit을 사용한 툴 개발기   코드  Stock price info downloader  Reference  Introducing Streamlit Sharing  ","permalink":"https://lucaseo.github.io/posts/2021-10-09-intro-streamlit-sharing/","summary":"Streamlit은 파이썬 기반으로 웹어플리케이션을 개발할 수 있는 프레임워크입니다. 이번 포스팅에서는 Streamlit에서 제공하는 서비스를 통해 아주 쉽고 간단하게 내가 개발한 웹을 배포하는 방법에 대해 소개하고자 합니다.\n웹 어플리케이션 개발 \u0026hellip; 그 다음에는? 웹개발과 다소 거리가 먼 데이터분석가, 데이터 사이언티스트 분들에게 Streamlit은 분석 결과를 공유하거나, 학습한 모델을 제품으로 프로토타이핑 할 수 있는 아주 강력한 도구로 자리 잡았습니다.\n하지만 나 혼자만 보고 사용하면 무슨 소용일까요\u0026hellip;? (도발)\n웹어플리케이션 개발과 별개로 배포 또한 하나의 장벽으로 느껴질 수 있는데요, Streamlit Sharing 은 Streamlit 어플리케이션을 배포하기 위해 알아야 할 모든 지식과 과정을 극도로 간소화 했습니다.","title":"[KR] Streamlit 앱 정말 쉽게 배포하기 (ft. Streamlit Sharing)"},{"content":"Machine Learning Operations(MLOps)에 대한 주제로 공부하며 기초를 정리해보았습니다.\n1. ML VS Real-world ML 1.1. 학술, 연구, 대회, 개인 프로젝트에서의 머신러닝 프로젝트 일반적으로 머신러닝 프로젝트를 시작하거나 학술적인 연구를 하는 경우, 머신러닝 프로젝트는 아래와 같은 성향을 띄고 있음.\n   source: Udacity\n   해결하고자 하는 문제의 정의와 데이터셋이 주어져 있음. 프로젝트의 목적은 주어진 데이터셋을 기반으로 평가 메트릭에서 모델의 성능이 가장 높게 나오는 모델링을 실행하는 것 프로젝트 진행 동안 만족할만한 모델 성능이 나올 때까지 일련의 과정을 사이클로 반복함  데이터셋에 대한 탐구 여러가지 피쳐 엔지니어링 테크닉 적용 여러가지 하이퍼파라미터 조절 여러가지 머신러닝 알고리즘의 적용    1.2. 프로덕션까지 이어진 머신러닝 프로젝트    source: Udacity\n   실제 머신러닝 프로젝트에서는 머신러닝 모델이 프로덕션에서 사용되어지는 경우를 염두에 둬야 함. 해결하고자 하는 문제를 정의하고, 머신러닝 기법이 필요한지의 여부를 결정함.  머신러닝 기법이 필요하지 않은 경우도 있으며, 이 때는 굳이 머신러닝을 적용하지 않아도 됨. (~하지만 제품을 마케팅하기 위해, 과제를 따내기 위해, 투자를 받기 위해 등등 갖가지 이유로 기어코 머신러닝을 적용하는 방향으로 가는 경우도 있음~)   여러 경로를 통해 데이터를 직접 확보해야 함. 데이터 정제, 전처리, 모델링을 통해 모델을 구축하고, 실제 프로덕션 시스템에서 동작할 수있도록 배포할 수 있어야 함. 데이터 드리프트(data drift), 퍼포먼스 드리프트(performance drift) 등이 발생하는 경우에 대비해서 지속적인 모니터링과 모델 재학습의 사이클이 자리잡혀 있어야 함. 여러 팀원과 부서에서 협업할 수 있도록, 과정이 명확하고, 재현 가능해야 함.  2. MLOps의 등장 2.1. MLOps의 정의  미국의 정보 기술 연구 \u0026amp; 컨설팅 기업인 Gartner는 2020년 12월을 기준으로 47%의 인공지능 프로젝트가 제품화에 실패한다는 조사 결과를 밝힘.(링크) 이러한 결과는 대부분의 머신러닝 프로젝트가 상단에서 기술한 1.1. 에 기반을 두고 있어서 임을 유추해볼 수 있음. 따라서 실제 프로덕트에서도 머신러닝을 적용하고 유지할 수 있는 파이프라인에 대한 고민을 하게 되었고, MLOps라는 개념이 등장하게 됨. MLOps의 정의: 실제 프로덕션 환경에서 가장 효율적으로 높은 성능, 안정적, 재현가능, 확장가능, 자동화, 재사용성을 갖춘 머신러닝 파이프라인을 end-to-end로 구축하는 방법론  2.2. 성공적인 머신러닝 프로젝트가 되기 위한 조건  (당연한 말이지만 1) 머신러닝으로 풀어야 하는, 풀 수 있는 문제 정의 (당연한 말이지만 2) 목표와 기간 설정 (당연한 말이지만 3) 충분한 데이터셋 프로덕션 환경에서 모델을 유지하기 위한 전략  한번 배포한 모델이 영원히 잘 동착할 거라고 장담할 수 없음.   프로덕션 환경을 늘 고려할 것.  머신러닝 프로젝트를 진행하는 과정에서 내리는 모든 의사결정은 프로덕션 환경에서 동작되는 시나리오를 고려하여 내려야 함.    2.3. MLOps를 적용해야 하는 환경  프로덕션 환경  PoC(Proof of concept)이나 MVP(minimum viable product)를 위한 머신러닝 프로젝트인 경우에는 적용되지 않아도 됨.   모델의 재현, 재사용이 빈번하게 일어나는 경우 프로젝트의 문서화, 협업이 빈번하게 일어나는 경우 프로젝트의 구조가 복잡한 경우 Model Drift가 발생하는 경우  Data drift, Performance drift라고도 함.  Data drift: 시간의 흐름에 따라 데이터의 특성이 달라질 경우 Performance drift: 시간의 흐름에 따라 모델의 성능이 감소할 경우   대부분의 모델은 일정한 시점의 데이터셋을 기반으로 학습되었음. 따라서 새로운 데이터에 대해서는 안정적인 성능을 보이지 않을 수 있음. 이러한 경우를 대비한 시스템이 구축되어 있어야 함.  모델의 성능, 데이터의 특성 등에 대한 모니터링 새로운 데이터에 대한 EDA, 피쳐 엔지니어링, 하이퍼파라미터 튜닝, 모델 재학습과 같은 사이클      Reference  Udacity: Machine Learning DevOps Engineer  ","permalink":"https://lucaseo.github.io/posts/2021-09-26-what-is-mlops/","summary":"Machine Learning Operations(MLOps)에 대한 주제로 공부하며 기초를 정리해보았습니다.\n1. ML VS Real-world ML 1.1. 학술, 연구, 대회, 개인 프로젝트에서의 머신러닝 프로젝트 일반적으로 머신러닝 프로젝트를 시작하거나 학술적인 연구를 하는 경우, 머신러닝 프로젝트는 아래와 같은 성향을 띄고 있음.\n   source: Udacity\n   해결하고자 하는 문제의 정의와 데이터셋이 주어져 있음. 프로젝트의 목적은 주어진 데이터셋을 기반으로 평가 메트릭에서 모델의 성능이 가장 높게 나오는 모델링을 실행하는 것 프로젝트 진행 동안 만족할만한 모델 성능이 나올 때까지 일련의 과정을 사이클로 반복함  데이터셋에 대한 탐구 여러가지 피쳐 엔지니어링 테크닉 적용 여러가지 하이퍼파라미터 조절 여러가지 머신러닝 알고리즘의 적용    1.","title":"[KR] MLOps: 무엇인가?"},{"content":"주피터노트북을 벗어나보자 데이터 분석 공부를 시작하는 분들 중 90% 이상은 주피터 노트북을 활용하는 데에서 출발하셨을 것입니다. 주피터 노트북은 데이터 분석 결과를 빠르게 확인할 수 있고 있다는 점에서 아주 간편하고 입문자들에게는 진입장벽이 낮은 도구입니다. 하지만 주피터노트북 형태의 코드는 정리가 되어있지 않으면 코드가 뒤죽박죽 섞일 수 있고, 재사용이 힘들어 유지보수를 하거나 협업을 하는 관점에서는 활용도가 매우 떨어집니다.\n이런 상황에서 기술적인 방법론이 아닌, 클린코드에 대한 감을 잡을 수 있도록 가이드 형식의 내용을 정리해보았습니다.\n 대상 독자\n 주피터 노트북으로 코딩에 입문하고, 현재까지도 사용 중이신 분 연구와 PoC만 해오시다, 실제 프로덕션 코드를 작성할 시점에 맞닥뜨리신 분 여태 홀로 일해야 하는 상황이었다가, 이제 협업을 하셔야 하는 분   1. 프로덕션 코드란\u0026hellip;? 프로덕트에 쓰이는 코드는 간단히 말해, 실제 제품에 쓰이는 코드를 말합니다. 따라서 프로덕션 코드는 아래와 같은 조건들을 갖춰야 합니다.\n  클린 코드\n 읽기 쉽고, 간결하며, 명확한 코드는 유지보수와 협업에 필수    모듈의 형태를 띄는 코드\n 일련의 코드가 논리적인 구조에 따라 함수와 클래스 등으로 구분되어진 형태    모듈\n 파일 형태(ex. .py 확장자의 파일)로 구분되어, 효율적으로 재사용할 수 있도록 잘 정리된 코드 import 하여 쓸 수 있음    2. 클린 코드 작성하기 2.1. 의미 있는 이름의 사용  변수명, 함수명 등은 작업을 묘사하고 타입을 명시합니다.  get, convert, 등 동사를 통해 작업을 묘사 하지만 모든 디테일을 다 넣은 긴 이름은 결코 좋은 이름이 될 수는 없음   비슷한 성향을 가졌다면 일관성을 가진 이름을 부여  ex)  ages, age (x) age_list, age (o)     축약어나 한글자 변수명은 지양하는 것이 좋습니다.  2.2. 적절한 공백의 사용  공백을 사용하여 코드의 영역을 구분시키면, 관리하기도 좋고, 가독성도 향상됩니다. 한 줄에 79자 정도로 코드를 제한 하는 것을 권장합니다.  2.3. 모듈러 코드 작성 모듈러 코드를 작성하는 이유는 같은 코드를 반복해서 작성하지 않고도 재사용하는 것이, 장기적으로 봤을 때 유지보수 관점에서 더 낫기 때문입니다.\n모듈러 코드를 작성할 때 지키면 좋을 가이드를 몇가지 알아보도록 하겠습니다.\n 반복하지 말 것 (Don\u0026rsquo;t repeat yourself - DRY 객체(함수, 클래스, 모듈 등)의 갯수를 최소화. 불필요하게 많은 객체는 오히려 역효과가 있음. 하나의 함수는 한가지 작업만을 수행해야 함.  함수가 하나 이상의 작업을 수행할 경우, 재사용이나 일반화를 하는데 제한적임. 만약 함수 내에서 AND의 요소가 발견된다면, 쪼개서 작성할 것을 추천함.   재사용을 위해 변수명, arguemnt명은 되도록이면 가장 기본적인 의미를 담는 것이 좋습니다.  간단한 변수명일 수록 가독성이 향상 다양한 상황에서의 재사용이 쉬워짐\u001c   하나의 함수당 argument 사용을 3개까지 제한하는 것을 권장합니다. 만약 4개 이상 사용하게 된다면, 함수를 쪼개어 구성하는 방안을 생각해보세요.  3. 리팩토링(Refactor) 리팩토링이란 코드의 기능에 영향을 끼치지 않은 상태를 유지하며 코드의 구조와 호율성을 향상시키는 작업입니다. 주로 초기 의도한 코드를 작성한 이후 이루어집니다.\n리팩토링은 좋은 퀄리티의 코드를 위해서 빠질 수 없는 작업입니다. 초반 기능을 구현하기 바쁠 대에는 불필요해 보일 수 있어도, 장기적으로 봤을 때 반드시 이점을 볼 수 있는 작업이므로 되도록이면 개발 기간을 산정할 때, 리팩토링할 시간을 포함하는 것이 좋습니다.\n4. 문서화(Documentations) 문서화는 개발에 대한 전반적인 내용을 문서로 기록하여 다른 사람들로 하여금 코드에 대한 이해를 돕는 작업인데요, 문서화가 문서를 작성하는 것만 한정하지는 않습니다.\n4.1. 인라인 코멘트 (Inline Comments) 인라인 코드는 주석이라고도 하는데요, 코드 중간 중간 설명을 다는 행위를 뜻합니다. (파이썬에서는 #를 사용합니다) 특정 코드에 대한 설명을 달아 빠른 이해를 도울 수 있습니다.\n주석은 복잡한 로직을 가지고 있는 코드 블럭의 경우, 주석을 통해 자연어로 설명하여 이해를 도울 수 있습니다. 또한 코드 자체가 어떠한 로직의 배경이나 이유, 영향을 받는 외부적인 요인 등을 설명하지 못할 때는 주석으로 설명하는 것이 효과적입니다.\n4.2. 독스트링(Docstrings) 독스트링은 세개의 따옴표로 감싼 주석으로 함수나 모듈에 대한 설명을 작성할 때 사용됩니다.\n  One line docstring\n 함수가 매우 간단할 경우, 아래와 같이 독스트링을 한 줄로 작성할 수 있습니다. 예시) def female_ratio(female_count, total_count): \u0026#34;\u0026#34;\u0026#34; Calculate the ratio of female \u0026#34;\u0026#34;\u0026#34; return female_count / total_count     Multiline docstring\n 다소 설명이 많이 필요한 경우에는 아래의 조건을 만족시키며 docstring의 내용을 확장할 수 있습니다.  argument에 대한 설명 argument의 타입 명시 함수의 목적 함수의 출력에 대한 설명   예시) def female_ratio(female_count, total_count): \u0026#34;\u0026#34;\u0026#34;Calculate the ratio of female. Args: female_count: int. count of female. total_count: int. total count of people. Returns: female_ratio: female_count/total_count. \u0026#34;\u0026#34;\u0026#34; return female_count / total_count     Docstring에는 더욱 다양한 스타일에 대한 내용은 링크 를 참조하세요.\n4.3. 프로젝트 문서 코드에 대한 문서 뿐만 아니라 전체 프로젝트에 대한 문서도 빠질 수 없습니다. 우리가 가장 쉽게 접할 수 있는 프로젝트 문서는 Github에서 종종 볼 수 있는 README.md 파일이죠. 프로젝트 문서는 프로젝트에 대한 개요, 의존성, 사용법, 디버깅 방법 등을 기술합니다.\n효과적인 프로젝트 문서 작성 방식에 대한 내용은 Udacity에서 제공하는 무료 강의를 참조하실 수 있습니다.\n마무리 하며 팀원이 많아지고 프로젝트의 단위가 커질 수록, 클린 코드는 권장사항이 아니라 필수인 것을 체감하고 있습니다. 그리고 클린코드는 습관인 것 같다는 생각이 듭니다. 기존의 습관을 떨쳐내기가 쉽지 않네요. 입사 초창기부터 가이드해줄 수 있는 시니어의 존재가 조직에 없었던 것이 참 아쉽게 다가오기도 합니다. 괜히 세상에 존재하지도 않는 그 분을 탓해보네요. (좋은 시니어나 멘토가 있다면 당신은 정말 축복받은 사람입니다!)\n아무튼 주니어 분석가를 위한 클린코드에 대한 가이드를 나름대로 적어보았는데요, 처음 기획했던 것보다 다뤄야 할 내용이 다소 더 많아진 것 같습니다. 조금 더 공부하고 정리해서 다음에는 2편으로 돌아오겠습니다 :)\nReference  파이썬 클린 코드 Udacity: Machine Learning DevOps Engineer  ","permalink":"https://lucaseo.github.io/posts/2021-09-11-intro-clean-code/","summary":"주피터노트북을 벗어나보자 데이터 분석 공부를 시작하는 분들 중 90% 이상은 주피터 노트북을 활용하는 데에서 출발하셨을 것입니다. 주피터 노트북은 데이터 분석 결과를 빠르게 확인할 수 있고 있다는 점에서 아주 간편하고 입문자들에게는 진입장벽이 낮은 도구입니다. 하지만 주피터노트북 형태의 코드는 정리가 되어있지 않으면 코드가 뒤죽박죽 섞일 수 있고, 재사용이 힘들어 유지보수를 하거나 협업을 하는 관점에서는 활용도가 매우 떨어집니다.\n이런 상황에서 기술적인 방법론이 아닌, 클린코드에 대한 감을 잡을 수 있도록 가이드 형식의 내용을 정리해보았습니다.","title":"[KR] 주피터노트북만 써왔던 당신을 위한 클린코드 소개서"},{"content":"Airflow를 처음 알게 된 후, 이것 저것 찾아보는 중, 아래와 이미지를 접한 기억이 납니다.\nSource Airflow: a workflow management platform\n수많은 태스크들이 모여 복잡한 DAG를 이루는 파이프라인이 구축되어 있는데요, 처음에 언뜻 봤을 때는, 정말 그럴 듯하고 멋있어 간지나 보였습니다. 하지만, Airflow를 적극 도입하려고 하는 현재 시점에서 다시 생각해보니, 막상 저런 파이프라인을 전부 다 파악하고 관리할 상상을 하니까 결코 쉽지 않을 것 같다는 인상도 피할 수 없었습니다. 하지만 이런 고민은 Airflow2에서부터 도입된 Task Group의 도움을 받아 개선할 수 있을 것으로 보입니다.\n이번 글에서는 Task Group의 개념과 사용 예제를 정리합니다.\nTask Group 개념 특히 사용자는 Task Group을 통해 복합하게 연결된 태스크를 복수 개의 그룹으로 묶어 Webserver의 Graph View에서 깔끔하게 시각화 할 수 있으며, 특정 프로세스의 그룹은 재사용할 수도 있습니다.\nSubdag와 Task Group의 비교 Airflow 1.x 에서는 유저가 SubDag를 따로 정의하여 이러한 역할을 수행할 수 있게 했습니다. 하지만 Subdag는 하나의 DAG 내부에 또 다른 DAG를 선언하는 형식으로 구성되기 때문에 파라미터와 스케줄이 추가적으로 정의되어야 하고, 만에 하나 엉킨다면 상위 DAG에서 문제를 일으킬 소지가 있는 만큼, 안정적인 방법은 아니었습니다.\n반면, Task Group의 경우, 개별 파라미터와 스케줄 등을 정의할 필요 없이 단순히 태스크를 묶어주는 역할만 하기 때문에, 많은 자원을 차지하지 않습니다.\nTask Group 정의 방법 Task Group 객체는 다음과 같이 사용할 수 있습니다.\n Task Group 정의 방법  # Task Group 객체 불러오기 from airflow.utils.task_group import TaskGroup # 시작 task task_0 = DummyOperator(task_id=\u0026#39;init\u0026#39;) # Task Group 1 with TaskGroup(group_id=\u0026#39;group_1\u0026#39;) as tg_1: task_1 = DummyOperator(task_id=\u0026#39;task_1\u0026#39;) task_2 = DummyOperator(task_id=\u0026#39;task_2\u0026#39;) # Task Group 1 내부에서의 의존성 task_1 \u0026gt;\u0026gt; task_2 # Task Group 2 with TaskGroup(group_id=\u0026#39;group_2\u0026#39;) as tg_2: task_3= DummyOperator(task_id=\u0026#39;task_3\u0026#39;) task_4 = DummyOperator(task_id=\u0026#39;task_4\u0026#39;) [task_3, task_4] \u0026gt;\u0026gt; task_5 # 종료 task task6 = DummyOperator(task_id=\u0026#39;end\u0026#39;)  DAG 내부에서 의존성을 정의하는 방식은 기존 task 간의 의존성 정의 방식과 마찬가지로 \u0026gt;\u0026gt; 연산자를 사용하면 됩니다.  # DAG 내 task 의존성 정의 task_0 \u0026gt;\u0026gt; tg_1 \u0026gt;\u0026gt; tg_2 \u0026gt;\u0026gt; task6 Task 테스트 시 주의 사항 보통 task를 테스트하는 명령어는 다음과 같습니다.\nairflow tasks test [dag_id] [task_id] [date] 하지만 Task Group에 속하는 task의 경우에는 다음과 같이 group_id를 앞에 붙여줘야 합니다.\nairflow tasks test [dag_id] [group_id.task_id] [date] Task Group을 활용한 예제 Task Group을 활용하여 어떠한 가상의 프로세스 파이프라인에서 특성이나 단계에 따라 task들을 묶어 DAG를 형성해보았습니다. Webserver 상의 Graph View도 그룹에 따라 구분되었으며, 그룹을 펼쳐 볼 수도 있게 되었습니다. 자세한 내용은 아래 덧붙인 예제 코드와 Graph View 상의 DAG 시각화를 비교해보시길 바랍니다.\n      import airflow from airflow import DAG from airflow.utils.task_group import TaskGroup from airflow.operators.dummy import DummyOperator default_args = { \u0026#34;owner\u0026#34;: \u0026#34;temp\u0026#34;, \u0026#34;depends_on_past\u0026#34; : False, \u0026#34;start_date\u0026#34; : None } with DAG( dag_id = \u0026#39;dummy_dag\u0026#39;, default_args = default_args, description = \u0026#34;Task Group demo\u0026#34;, schedule_interval = None, start_date = airflow.utils.dates.days_ago(10), ) as dag: task_0 = DummyOperator(task_id=\u0026#34;task_0\u0026#34;) with TaskGroup(group_id=\u0026#39;group_1\u0026#39;) as tg_1: task_1_1 = DummyOperator(task_id=\u0026#34;task_1_1\u0026#34;) task_1_2 = DummyOperator(task_id=\u0026#34;task_1_2\u0026#34;) with TaskGroup(group_id=\u0026#39;group_1_3\u0026#39;) as tg_1_3: task_1_3_1 = DummyOperator(task_id=\u0026#34;task_1_3_1\u0026#34;) task_1_3_2 = DummyOperator(task_id=\u0026#34;task_1_3_2\u0026#34;) task_1_3_3 = DummyOperator(task_id=\u0026#34;task_1_3_3\u0026#34;) task_1_3_4 = DummyOperator(task_id=\u0026#34;task_1_3_4\u0026#34;) [task_1_3_1, task_1_3_2, task_1_3_3] \u0026gt;\u0026gt; task_1_3_4 task_1_1 \u0026gt;\u0026gt; task_1_2 \u0026gt;\u0026gt; tg_1_3 with TaskGroup(group_id=\u0026#39;group_2\u0026#39;) as tg_2: task_2_1 = DummyOperator(task_id=\u0026#34;task_2_1\u0026#34;) with TaskGroup(group_id=\u0026#39;group_2_2\u0026#39;) as group_2_2: task_2_2_1 = DummyOperator(task_id=\u0026#34;task_2_2_1\u0026#34;) task_2_2_2 = DummyOperator(task_id=\u0026#34;task_2_2_2\u0026#34;) task_2_2_1 \u0026gt;\u0026gt; task_2_2_2 with TaskGroup(group_id=\u0026#39;group_2_3\u0026#39;) as group_2_3: task_2_3_1 = DummyOperator(task_id=\u0026#34;task_2_3_1\u0026#34;) task_2_3_2 = DummyOperator(task_id=\u0026#34;task_2_3_2\u0026#34;) task_2_3_3 = DummyOperator(task_id=\u0026#34;task_2_3_3\u0026#34;) [task_2_3_1, task_2_3_2, task_2_3_3] with TaskGroup(group_id=\u0026#39;group_2_4\u0026#39;) as group_2_4: task_2_4_1 = DummyOperator(task_id=\u0026#34;task_2_4_1\u0026#34;) task_2_4_2 = DummyOperator(task_id=\u0026#34;task_2_4_2\u0026#34;) task_2_4_3 = DummyOperator(task_id=\u0026#34;task_2_4_3\u0026#34;) [task_2_4_1, task_2_4_2] \u0026gt;\u0026gt; task_2_4_3 with TaskGroup(group_id=\u0026#39;group_2_5\u0026#39;) as group_2_5: task_2_5_1 = DummyOperator(task_id=\u0026#34;task_2_5_1\u0026#34;) task_2_5_2 = DummyOperator(task_id=\u0026#34;task_2_5_2\u0026#34;) [task_2_5_1, task_2_5_2] task_2_1 \u0026gt;\u0026gt; group_2_2 \u0026gt;\u0026gt; group_2_3 \u0026gt;\u0026gt; group_2_4 \u0026gt;\u0026gt; group_2_5 with TaskGroup(group_id=\u0026#39;group_3\u0026#39;) as tg_3: task_3_1 = DummyOperator(task_id=\u0026#39;task_3_1\u0026#39;) task_3_2 = DummyOperator(task_id=\u0026#39;task_3_2\u0026#39;) task_3_3 = DummyOperator(task_id=\u0026#39;task_3_3\u0026#39;) task_3_4 = DummyOperator(task_id=\u0026#39;task_3_4\u0026#39;) [task_3_1, task_3_2, task_3_3, task_3_4] with TaskGroup(group_id=\u0026#39;group_4\u0026#39;) as tg_4: task_4_1 = DummyOperator(task_id=\u0026#39;task_4_1\u0026#39;) task_4_2 = DummyOperator(task_id=\u0026#39;task_4_2\u0026#39;) task_4_1 \u0026gt;\u0026gt; task_4_2 task_0 \u0026gt;\u0026gt; [tg_1, tg_2] \u0026gt;\u0026gt; tg_3 \u0026gt;\u0026gt; tg_4 Reference  Airflow Documentation - DAGs  ","permalink":"https://lucaseo.github.io/posts/2021-08-29-airflow2-task-group/","summary":"Airflow를 처음 알게 된 후, 이것 저것 찾아보는 중, 아래와 이미지를 접한 기억이 납니다.\nSource Airflow: a workflow management platform\n수많은 태스크들이 모여 복잡한 DAG를 이루는 파이프라인이 구축되어 있는데요, 처음에 언뜻 봤을 때는, 정말 그럴 듯하고 멋있어 간지나 보였습니다. 하지만, Airflow를 적극 도입하려고 하는 현재 시점에서 다시 생각해보니, 막상 저런 파이프라인을 전부 다 파악하고 관리할 상상을 하니까 결코 쉽지 않을 것 같다는 인상도 피할 수 없었습니다. 하지만 이런 고민은 Airflow2에서부터 도입된 Task Group의 도움을 받아 개선할 수 있을 것으로 보입니다.","title":"[KR] Airflow 2.x 에서는 Task Group 씁시다"},{"content":"작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1.x에 한정되어 있기 때문에, 이번 글에서는 Airflow의 공식 Docker 이미지를 활용해보도록 하겠습니다.\n0. Docker 설치 Docker가 설치되어 있지 않은 경우 아래의 링크를 통해 Docker를 설치하도록 합니다.\n Docker 설치 (맥, 윈도우, 리눅스)  맥, 윈도우의 경우 Docker 데스크탑을 설치할 시, Docker Compose가 함께 설치 됩니다. 단, 리눅스의 경우에는 별도의 설치가 필요합니다.\n Docker Compose 설치 (리눅스)  1. 아무것도 모르지만 일단 실행해보기 Airflow 공식 문서(Running Airflow in Docker)에서는 기본적인 Docker 및 Docker Compose를 활용한 Airflow 예제를 제공하고 있습니다. 이 파일을 토대로 일단 한번 실행 과정을 따라가보겠습니다.\n1.1. Airflow 2 Docker Compose 파일 공식문서에서의 Docker Compose 파일 기본적으로 CeleryExecutor를 사용하는 환경설정이 정의되어 있지만, 이번 예제에서는 단일 머신에서 구동을 할 것이기 때문에 LocalExecutor를 사용하도록 docker-compose.yml 파일을 아래와 같이 약간만 변경하도록 하겠습니다. (기본적으로 Celery와 Redis와 관련된 설정은 제외하였습니다.)\n./docker-compose.yml\nversion: \u0026#39;3\u0026#39; x-airflow-common: \u0026amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}  # Docker 이미지는 Airflow 2.1.2 를 사용 environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: LocalExecutor  # LocalExecutor를 사용 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # PostgreSQL을 데이터베이스로 사용 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#39;true\u0026#39; # 실행시 DAG가 자동으로 동작되지 않음 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#39;false\u0026#39; # 기본 예제 DAG는 불러오지 않음 AIRFLOW__API__AUTH_BACKEND: \u0026#39;airflow.api.auth.backend.basic_auth\u0026#39; _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} volumes:\t # DAG, 로그, 플러그인 파일이 저장될 경로의 폴더를 볼륨으로 마운트함 - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins - ./data_files:/opt/airflow/data_files user: \u0026#34;${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\u0026#34; depends_on: postgres: condition: service_healthy services: postgres: image: postgres:13 environment: POSTGRES_USER: airflow POSTGRES_PASSWORD: airflow POSTGRES_DB: airflow volumes: - postgres-db-volume:/var/lib/postgresql/data healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] interval: 10s retries: 5 restart: always airflow-webserver: \u0026lt;\u0026lt;: *airflow-common command: webserver ports: - 8080:8080\t# 실행되는 컨테이너와 localhost의 포트를 8080으로 맞춰, 실행 중인 Webserver에 접근할 수 있도록 함 restart: always airflow-scheduler: \u0026lt;\u0026lt;: *airflow-common command: scheduler restart: always airflow-init: \u0026lt;\u0026lt;: *airflow-common command: version environment: \u0026lt;\u0026lt;: *airflow-common-env _AIRFLOW_DB_UPGRADE: \u0026#39;true\u0026#39; _AIRFLOW_WWW_USER_CREATE: \u0026#39;true\u0026#39; # 기본 유저 계정을 생성함 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}  # 기본 유저 Username _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}  # 기본 유저 Password volumes: postgres-db-volume: 1.2. Docker Compose 파일 실행하여 Airflow 컨테이너 띄우기   우선 DAG, 로그, 플러그인, 데이터 파일을 저장할 경로를 생성합니다.\n mkdir ./dags ./logs ./plugins ./data_files    Docker 컨테이너에서 구동되는 파일과 호스트의 파일이 동일한 user / group permission을 가질 수 있도록 .env파일을 생성합니다.\n echo -e \u0026quot;AIRFLOW_UID=$(id -u)\\nAIRFLOW_GID=0\u0026quot; \u0026gt; .env    Airflow를 실행하기에 앞서, 데이터베이스를 구축하고 유저를 생성하는 명령어를 실행합니다.\n docker-compose up airflow-init 다음과 같은 메세지와 함께 완료되었다면 데이터베이스와 유저 생성이 성공적으로 실행된 것입니다.       3번이 정상적으로 실행되었다면, 다음 명령어를 통해 Airflow를 실행합니다. (실행하는 시간이 걸릴 수 있습니다.)\n docker-compose up    localhost:8080 , 즉 Airflow Webserver UI로 접속해보겠습니다.\n docker-compose.yml 에서 정의한 Username과 Password를 입력해서 로그인합니다.            Security -\u0026gt; List Users 경로에서 추가적으로 유저 계정을 생성할 수 있습니다.           1.3. DAG 작성하기 예제로 사용할 수 있는 간단한 DAG를 작성하고 로컬 경로(./dags/)에 저장해서, 현재 구동 중인 Airflow 컨테이너에서 실행해보겠습니다. 해당 DAG 파일은 Launch Library API에서 우주 발사 관련 데이터를 다운 받고, 이 파일을 변환하여 text파일로 저장하는 프로세스를 수행합니다.\n( Launch Library API는 SpaceDev 사에서 제공하는 API로, 전세계의 우주 로켓 등 모든 우주 발사체의 발사 일정을 데이터로 제공하는 무료 API입니다. )\n./dags/dag_rocket_launch_schedule.py\nimport datetime import requests import airflow from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator def save_launch_schedule_txt(): with open(\u0026#34;/tmp/launches.json\u0026#34;) as f: launches = json.load(f)[\u0026#39;results\u0026#39;] launches_df = pd.DataFrame.from_dict(launches, orient=\u0026#39;records\u0026#39;) cur_datetime = str(datetime.datetime.now()) launches_df.to_csv(\u0026#39;/opt/airflow/data_files/{}_launch_schedule.txt\u0026#39;.format(cur_datetime), encoding=\u0026#39;utf-8\u0026#39;, sep=\u0026#39;\\t\u0026#39;) default_args = { \u0026#34;start_date\u0026#34; : airflow.utils.dates.days_ago(1), \u0026#34;owner\u0026#34; : \u0026#34;airflow_admin\u0026#34; } with DAG( dag_id=\u0026#34;get_upcoming_rocker_launch_schedule\u0026#34;, schedule_interval=@daily, default_args = default_args ) as dag: download_launches = BashOperator( task_id = \u0026#34;download_launches\u0026#34;, bash_command = \u0026#34;curl -o /tmp/launches.json -L \u0026#39;https://ll.thespacedevs.com/2.0.0/launch/upcoming\u0026#39;\u0026#34; ) save_launch_schedule = PythonOperator( task_id = \u0026#34;update_launch_schedule\u0026#34;, python_callable = save_launch_schedule_txt ) download_launches \u0026gt;\u0026gt; save_launch_schedule 작성한 DAG 가 실행되었고, 결과 파일이 성공적으로 저장된 것을 마운트한 로컬 머신의 디렉토리에서 확인할 수 있습니다.\n                 2. 커스텀 이미지 만들기 가장 기본적인 예제를 순서대로 따라가며 Airflow 2.x를 Docker를 활용해서 실행해보았습니다. 그렇다면 이대로 예제를 수정해가며 DAG를 작성해서 실전에 적용시켜도 될까요? 실제 프로젝트에 적용하기에는 한계가 존재합니다. Airflow 공식 Docker 이미지는 말 그대로 reference image로, Airflow가 Docker 컨테이너에서 구동하기 위한 최소한의 요소만 갖추어져 있습니다. 즉, 대부분의 경우에는 실제 프로젝트에서 사용하는 커스텀 패키지나 의존적인 부분이 설치되어 있지 않습니다. 따라서 사용자는 Airflow 이미지를 활용하여 커스텀 이미지를 빌드하는 것이 적절한 사용 예라고 Airflow에서 안내하고 있습니다. (Airflow 공식 문서 - Building Image)\n2.1. 커스텀 Airflow Docker 이미지 만들기 커스텀 Docker 이미지를 만드는 것은 그리 어렵지 않습니다.\n  사용하고자 하는 패키지가 정의된 requirements.txt 파일 작성합니다.\n./requirements.txt(예시)\nCython==0.29.23 numpy==1.19.2 pymssql==2.1.5 pandas==1.1.2 pytz==2020.1   Docker 파일 작성을 작성합니다.\n./Dockerfile\nFROMapache/airflow:2.1.2 # 앞서 다룬 docker-compose.yml 파일에서 사용한 동일한 Airflow 이미지를 사용합니다. USERroot RUN apt-get update \\  \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\  build-essential libopenmpi-dev \\  \u0026amp;\u0026amp; apt-get autoremove -yqq --purge \\  \u0026amp;\u0026amp; apt-get clean \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* USERairflowCOPY requirements.txt requirements.txt # 현재 경로에 작성해둔 requirements.txt 파일으로 복사합니다.RUN pip3 install -r requirements.txt --no-cache-dir # 패키지를 설치합니다. 캐쉬는 저장할 필요가 없음을 파라미터로 넘깁니다.  커스텀 Docker 이미지를 빌드합니다.\n  --tag 로 이미지의 이름과 버전을 전달합니다.\n docker build . --tag \u0026quot;my_custom_image:0.0.1\u0026quot;        Docker 이미지가 빌드된 것을 확인할 수 있습니다.\n          2.2. 커스텀 Airflow Docker 이미지를 통해 컨테이너 띄우기  커스텀 Docker 이미지를 활용하여 Airflow를 실행할 수 있도록 docker-compose.yml를 수정합니다.\n./docker-compose.yml\nversion: '3' x-airflow-common: \u0026amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-my_custom_image:0.0.1} # Airflow 이미지를 커스텀 Docker 이미지로 수정합니다. environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: LocalExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' ...   앞서 docker-compose.yml 파일을 실행하여 컨테이너를 띄웠던 과정을 다시 실행합니다.\ndocker-compose up airflow-init docker-compose up   3. 마무리하며 Airflow 공식 문서를 참고하여, Docker를 활용하여 2.x 버전을 실행시키는 과정을 정리해봤습니다. 확실히 Docker를 쓰면 앞으로의 관리나 지식 전달 등의 과정도 무척 간소화될 것 같아 적극적으로 적용해야겠다는 생각이 드네요.\nReference  Airflow 공식 문서 - Running Airflow in Docker Airflow 공식 문서 - Building Image Datapipelines with Apache Airflow (Bas Harenslak, Julian de Ruiter)  ","permalink":"https://lucaseo.github.io/posts/2021-08-15-airflow2-with-docker/","summary":"작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1.","title":"[KR] Docker를 활용하여 Airflow 2.x 실행하기"},{"content":" 한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 2장 - 전자 회로의 조합 논리 2.0. 전자 회로의 조합 논리  컴퓨터는 2진법을 적용한 비트를 내부 언어로 사용함. 컴퓨터는 비트를 기반으로 새로운 비트를 표현하고 연산하는 불리언대수 또는 조합논리(combinatorial logic)을 통해 구현한 기능을 통해 동작함.  2.1. 논리 게이트  1960년대 개발된 논리 연산을 수행하는 회로. 간단하게 게이트라고도 함. 앞서 1장에서 다루었던 기본적인 부울대수의 연산을 수행하며, 이들의 결합으로 더욱 복합적인 기능을 구현할 수 있음.  2.2. 게이트를 조합한 회로  가산기 adder  덧셈연산을 하는 논리회로 종류  반가산기 전가산기 리플 자리올림 가산기 올림 예측 가산기     디코더 decoder  인코딩된 수를 비트의 집합으로 변환하는 회로   디멀티플렉서 demultiplexer (dmux)  하나의 입력을 받아 몇가지의 출력 중 한 곳으로 전달하는 회로   설렉터 selector (multiplexer)  여러 입력 중 하나의 입력을 선택하는 회로    3장. 순차 논리 3.0. 순차 Sequence  앞서 다룬 조합논리는 입력의 현재 시점의 상태만을 다룸. 반면 순차 논리는 시간의 흐름에 따른 값을 다룸. 입력값의 현재 상태 뿐만 아니라 과거의 입력값 또는 입력값의 과거 상태를 함께 고려해야하는 것을 의미함. 순차 논리를 통해 데이터를 저장하는 메모리 등을 구현함.  3.1. 시간의 표현  발진자  발진자 (또는 진동자)가 진동하는 특정 주기를 기준으로 두어 시간을 측정할 수 있음.   클록  발진자에서 제공되는 시간을 셀 수 있게 해주는 신호. 신호를 통해서 발진자에서 표현되는 전기적 신호의 진동 속도를 측정할 수 있음. 컴퓨터 부품이 고장나지 않는 선에서 클록의 속도를 더욱 빨리 하여 단위 시간당 진동횟수를 늘리는 것을 오버클럭이라고 함.   래치: 1비트의 정보를 기억, 보관할 수 있는 회로 플립플롭: 데이터의 전이(에지)를 통해서 시점을 반영하는 래치 레지스터: 클록을 공유하는 복수개의 플립플롭을 묶은 기억장치 또는 메모리  3.2. 메모리 3.2.1. 임의 접근 메모리 RAM  임의 접근, 휘발성의 특성을 가진 메모리  3.2.2. 읽기 전용 메모리 ROM  휘발성인 RAM과 달리 한번 쓰기가 적용된 이후에는 여러번 읽을 수 있는 메모리  3.2.3. 플래시 메모리  EEPROM의 한종류로, 블록으로 나누는 것이 가능함. 읽을 때 임의접근, 쓸 때 블록 접근의 구조를 가진 장치.  3.3. 블록 장치  우리가 흔히 아는 디스크 드라이브 원판 또는 플래터에 비트를 저장함. 플로피 디스크, 광학 디스크(CD, DVD), 자기 테이프, 하드 드라이브 등이 모두 블록 장치에 해당함 SSD로 세대 교체 중  ","permalink":"https://lucaseo.github.io/posts/2021-08-08-the-secrete-life-of-programs-revie-chp-0203/","summary":"한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 2장 - 전자 회로의 조합 논리 2.0. 전자 회로의 조합 논리  컴퓨터는 2진법을 적용한 비트를 내부 언어로 사용함. 컴퓨터는 비트를 기반으로 새로운 비트를 표현하고 연산하는 불리언대수 또는 조합논리(combinatorial logic)을 통해 구현한 기능을 통해 동작함.  2.1. 논리 게이트  1960년대 개발된 논리 연산을 수행하는 회로. 간단하게 게이트라고도 함. 앞서 1장에서 다루었던 기본적인 부울대수의 연산을 수행하며, 이들의 결합으로 더욱 복합적인 기능을 구현할 수 있음.","title":"[KR] 한 권으로 읽는 컴퓨터 구조와 프로그래밍 2-3장 : 조합논리,  순차논리"},{"content":"다시 돌아온 글또 6기 지난 5월 글또 5기를 마친 시점에서 여러가지로 굉장히 지쳤었던 생각이 납니다. 잠시 그때로 기억을 거슬로 올라가보면, 코로나 시국으로 인해 심적 피로가 쌓여가는 것을 느끼고 있었고, 회사 일과 병행하고 있는 방송통신대 3번째 학기 과제와 기말고사가 겹쳐있었던 시점이었던 것 같습니다. 글또 활동을 통해서 쓴 포스팅의 퀄리티도 곤두박칠 치지 않았나 생각이 들었습니다. 그래서 다음 기수에는 참여할 수 있을지 스스로 의문이 들기도 했습니다. 그런데 다시 신청했네요. 또 속냐\n글또를 참여하는 이유 글또를 참여하는 분들마다 참여 동기와, 각자가 느끼는 글또의 매력은 다 다를 것입니다. 제 경우에는 다양한 분들이 쓰시는 글들을 통해 제 시야가 넓어지는 것에 가장 큰 매력을 느꼈습니다. 글또에서 공유되는 글들은 분야가 겹치기도 하고, 또 직접 찾아보지 않고는 알 수가 없는 생전 처음 들어보는 분야이기도 합니다. 게다가 일반 정보 요약, 메모 용도의 포스팅과 다르게 정성을 들여서 쓴 글이기에 퀄리티도 보장이 되죠. 이해가 안 되는 부분이나 글에 데한 의견을 슬랙을 통해 저자와 나눌 수 있는 부분도 너무 좋았습니다. 이런 교류를 통해 제가 신선한 자극을 받을 수 있다는 점이 정말 중독적으로 다가왔습니다. 이번에도 글또를 참여하는 가장 큰 이유라고 할 수 있습니다. 이건 제가 글또라는 커뮤니티를 통해서 외적으로 얻는 부분입니다.\n그렇다면 글또를 통해서 내적으로 얻을 수 있는 것은 무엇일까요. 글또를 참여하게 되면, 글감에 대한 고민과 주제를 어떻게 글로 풀어나갈지 계속해서 생각하고 정리하게 됩니다. 처음에는 상당히 피로했는데, 4, 5기를 참여하면서 상당히 자연스러운 과정이 되었습니다. 일을 할 때나 개인적인 공부를 할 때도, \u0026ldquo;이걸 글로 정리를 한다면 어떻게 구성해야 할 지\u0026quot;에 대한 생각을 자연스럽게 하게 됩니다. 글을 쓰는 빈도가 다른 분들처럼 높은 것은 아니지만, 그럼에도 불구하고 무엇이든 \u0026ldquo;그냥 하는 일\u0026quot;이 없어졌습니다. 비시즌에는 그냥 생각만 하고, 글또 시즌에는 이를 실제 글로 작성하면서 실천을 하게 됩니다. 글또는 제가 이런 과정을 지속적으로 이행할 수 있게 도와주는 \u0026ldquo;훈련 시즌\u0026rdquo; 같은 거라고 생각하고 있습니다.\n이번 기수에서 다루고자 하는 주제 지난 글또5기에서는 글의 주제가 굉장히 들쑥날쑥했습니다. 아마도 급하게 글을 땜빵으로 작성해서인 것 같습니다. 그래서 이번 기수에서는 주제를 정해놓지 않되, 작성되는 글의 주제가 들쑥날쑥하게 하는 것이 가장 큰 목표입니다.\n생각해놓은 글의 주제는 다음과 같습니다.\n 파이썬 멀티 프로세싱에 관한 정리 및 예제 글 에어플로우 기초 개념 및 초기 구축에 관한 글 MLOps에 대한 이론과 토이 프로젝트 연재 현재 일하고 있는 회사의 도메인에 관한 글 기술 관련 서적에 관한 리뷰 글 방송통신대 과정에 관한 글  글을 마무리 하며 항상 큰 포부를 가지고 글또 시즌을 시작하곤 했는데요, 이번에는 이상하게 그런 기분이 들지 않는 것 같습니다. 반면에 많은 분들이 참여를 해서 그런지, 기대감은 굉장히 많이 부풀어 있는 것 같기도 하네요. 많은 분들을 통해서 많은 자극을 받고, 시야를 넓히며, 또 반대로 제가 다른 분들께 같은 느낌을 드릴 수 있도록 열심히 글을 작성해보겠습니다.\n","permalink":"https://lucaseo.github.io/posts/2021-07-30-init-geultto-6/","summary":"다시 돌아온 글또 6기 지난 5월 글또 5기를 마친 시점에서 여러가지로 굉장히 지쳤었던 생각이 납니다. 잠시 그때로 기억을 거슬로 올라가보면, 코로나 시국으로 인해 심적 피로가 쌓여가는 것을 느끼고 있었고, 회사 일과 병행하고 있는 방송통신대 3번째 학기 과제와 기말고사가 겹쳐있었던 시점이었던 것 같습니다. 글또 활동을 통해서 쓴 포스팅의 퀄리티도 곤두박칠 치지 않았나 생각이 들었습니다. 그래서 다음 기수에는 참여할 수 있을지 스스로 의문이 들기도 했습니다. 그런데 다시 신청했네요. 또 속냐\n글또를 참여하는 이유 글또를 참여하는 분들마다 참여 동기와, 각자가 느끼는 글또의 매력은 다 다를 것입니다.","title":"[KR] 글또 6기의 시작"},{"content":" 한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 1장 - 컴퓨터 언어 체계 1.0. 컴퓨터의 언어  언어는 정보의 소통을 하기 위한 매개체 언어를 사용하기 위해서는 2가지가 필요함  인코딩: 어떠한 기호의 집합으로 변환, 기록되어야 함. 문맥: 의사소통의 당사자들이    1.1. 2진법 체계 1.1.1. 비트  비트(bit) 2진법을 사용하는 체계 참/거짓 (또는 다른 형식의 표현)을 기반으로 하는 비트를 사용하여 논리연산을 할 수 있음  1.1.2. 불리언 대수 (Boolean algebra)  비트에 대한 연산 규칙들의 집합 불리언 연산자 (Boolean operators)  NOT : 논리적 반대 AND : 논리곱. 둘 다 참인 경우에만 참 (또는 둘 다 거짓인 경우에만 거짓) OR : 논리합. 둘 중 하나 이상이 참인 경우에만 참 (둘 중 하나라도 거짓인 경우 거짓) XOR : 배타적논리합. 두개의 bit값이 서로 다를 경우에만 참. 서로 같을 경우에는 거짓   드모르간의 법칙  a AND b = NOT( NOT a OR NOT b) 긍정적 논리(정논리)를 부정적 논리(부논리)로 변환하여 연산할 수 있게 됨.    1.2. 정수의 표현 1.2.1. 10진수의 표현  0~9 의 10가지 숫자를 사용하여 표현 1603의 10진수 표현  \\(1 \\times 10^3 + 6 \\times 10^2 + 0 \\times 10^1 + 3 \\times 10^0\\)    1.2.2. 2진수의 표현  비트를 사용하여 표현. 단, 1과 0 두가지 기호 밖에 사용할 수 없음. 1603의 2진수 표현  \\(1 \\times 2^{10} + 1 \\times 2^9 + 0 \\times 2^8 + 0 \\times 2^7 + 1 \\times 2^6 + 0 \\times 2^5 + 0 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0\\)  \\(1024 + 512 + 64 + 2 + 1\\) 즉, 11001000011 로 표현될 수 있음.      1.2.3. 2진수의 덧셈  2진수의 덧셈에서 사용할 비트의 개수를 벗어날 때를 오버플로(overflow)라고 함.  상태 코드 레지스터(condition code register)에 오버플로 비트를 정보를 담아둠.    1.2.4. 음수의 표현  음수를 표현하기 위한 방법으로 부호를 사용하는 것이 있음.  가장 왼쪽 비트 (MSB)를 부호를 표현하기 위한 비트로 사용하는 것. 하지만, XOR과 AND를 사용한 덧셈 연산을 수행할 수 없음.   음수를 표현하기 위한 방법으로 보수(one\u0026rsquo;s coomplement)가 있음.  양수의 모든 비트를 반대로 뒤집는 방법. 단, 순환올림의 상황에서 하드웨어의 추가가 필요하기에 비용이 발생함.   부호, 보수의 단점을 해결하는 음수 표현의 방식으로 2의 보수 표현법이 있음.  비트를 모두 뒤집고, 1을 추가하는 방법.    1.3. 실수의 표현  실수에는 소수점이 포함됨 고정소수점 표현법  소수점의 위치를 정해놓고 표현하는 방법 특수 목적을 위해서 사용되고 있으나, 범용 컴퓨팅에서는 사용되고 있지 않음.   부동소수점 표현법 (IEEE)  IEEE 표준에서 제안하는 부동소수점 방식  1bit - 부호 8bit - 지수 23bit - 가수      1.4. 텍스트의 표현 1.4.1. 아스키 코드 (ASCII)  키보드에 있는 모든 값에 대해서 7비트의 수를 할당한 체계 ASCII 코드 테이블 알파벳 이외에 장치를 제어하기 위해 사용되는 제어문자가 포함됨. 알파벳 뿐만 아니라 다른 언어를 지원하기 위한 표준들도 생김.  ISO-646, ISO-8859, KSC5601 등등    1.4.2. Unicode  문자에 16비트를 부여한 체계  1.4.3. UTF-8 (유니코드 변환 형식 8비트)  호환성과 효율성을 고려해 8비트만으로 문자를 표현할 수 있도록 Unicode를 인코딩하는 방식 ASCII 문자의 변환 때에도 추가의 공간이 필요로 하지 않음 ASCII기호에 해당하지 않는 기호나 문자의 경우에는 UTF-8 덩어리를 복수개를 사용하여 표현 함.  1.6. 색의 표현  대표적으로 적, 녹, 청을 섞은 RGB가 있으며, 이 조합으로 가지고 컬러큐브를 표현할 수 있음.  색을 표현할 때 #rrggbb의 형식으로 표현하며, 각 색상은 16진법으로 표현함.   투명도를 가미한 색표현을 RGBa가 있음.  ","permalink":"https://lucaseo.github.io/posts/2021-07-25-the-secrete-life-of-programs-revie-chp-01/","summary":"한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 1장 - 컴퓨터 언어 체계 1.0. 컴퓨터의 언어  언어는 정보의 소통을 하기 위한 매개체 언어를 사용하기 위해서는 2가지가 필요함  인코딩: 어떠한 기호의 집합으로 변환, 기록되어야 함. 문맥: 의사소통의 당사자들이    1.1. 2진법 체계 1.1.1. 비트  비트(bit) 2진법을 사용하는 체계 참/거짓 (또는 다른 형식의 표현)을 기반으로 하는 비트를 사용하여 논리연산을 할 수 있음  1.","title":"[KR] 한 권으로 읽는 컴퓨터 구조와 프로그래밍 1장 : 컴퓨터 내부의 언어 체계"},{"content":"글또 5기를 마무리하며 글또 5기의 활동이 곧 마지막이라는 알림을 보고 약간 당황했습니다. 5기를 언제부터 시작했는지도 약간 가물가물하던 차, 다시 한번 찾아보니 작년 11월이더라구요. 언제 부터 시작했는지조차 기억이 나지 않는 것을 보니 이번 기수 동안에는 참 개인적으로 정신이 없었다는 걸 새삼 느끼게 되었습니다. 그래도 희미한 기억을 되짚어가며 글또 5기의 되돌아보고자 합니다.\n돌아보기 5기의 시작 어느 직장이나 바쁜 시즌이 있듯이, 현재 일하고 있는 직장에서는 10~12월에 항상 무언가가 터지는 시즌입니다. 연말을 앞두고 쎄한 느낌이 들어서였을까요, 11월 초 글또 5기를 시작하면서 다짐글을 작성하지 않았습니다. 지키지 못 할 약속은 하지 않는다 대신 글또라는 시스템을 통해 무엇인가라도 꾸준히 쓰자 라는 생각을 했던 것 같습니다.\n아쉬웠던 점 그래서 5기 활동 기간동안 썼던 포스팅을 보니 중구난방도 이런 중구난방이 없습니다. 사실 저만 보려고 쓴 포스팅도 있고요, 좀 부끄럽기도 하고요 \u0026hellip; 그래서 글또 슬랙 채널에 공유하지 않은 포스팅도 보입니다.\n초반에 오디오 데이터를 다루는 글, 그 이후에는 자료구조를 정리했고, 마지막으로는 회사에서의 업무내용을 공유하는 포스팅을 작성했습니다. 가장 아쉬운 점은, 시리즈의 느낌을 띄는 주제를 다뤘음에도 확실하게 끝맺음을 짓지 못 했다는 것이라는 생각이 듭니다. 반대로 생각하면 그만큼 방황하며 정체성을 찾는 기간이었다고도 볼 수 있을 것 같네요. 는 합리화\n그럼에도 좋았던 점 글을 작성하게 하는 시스템과 더불어 글또의 진짜 매력은 아무래도 사람에 있습니다. 비슷한 일을 하는 분들과 함께 고민을 나누고, 좋은 점은 눈팅하고, 배우고, 피드백하고, 서로 응원하며 이름모를 동료애를 키울 수 있는 커뮤니티는 정말 많지 않습니다. 이번 기수에서도 분석가 팀 뿐만 아니라 AI/ML엔지니어, 데이터 엔지니어 채널을 통해서 직간접적으로 다양한 분들과 교류할 수 있었습니다.\n갑분 다짐 여전히 기술적인 내용을 습득해서 내용을 정리하고 글로 풀어내는 것은 어렵게 느껴집니다. 그렇지만 글또는 계속해서 참여할 생각입니다. 항상 깊이 있는 내용의 포스팅만 작성할 수는 없겠죠. 작성하는 글이 간단한 개념 정리일 수도 있고, 기술을 소개하는 글이 될 수도 있다고 생각하며 글 쓰는 부담을 좀 내려놓으려고 합니다. 다만, 다음 글또 기수를 참여하게 된다면, 방향성을 가지고 일관적인 글 주제를 유지하며, 데이터 직군으로 일하는 제 정체성도 한번 챙겨보려고 합니다.\n","permalink":"https://lucaseo.github.io/posts/2021-05-02-reivew-geultto-5/","summary":"글또 5기를 마무리하며 글또 5기의 활동이 곧 마지막이라는 알림을 보고 약간 당황했습니다. 5기를 언제부터 시작했는지도 약간 가물가물하던 차, 다시 한번 찾아보니 작년 11월이더라구요. 언제 부터 시작했는지조차 기억이 나지 않는 것을 보니 이번 기수 동안에는 참 개인적으로 정신이 없었다는 걸 새삼 느끼게 되었습니다. 그래도 희미한 기억을 되짚어가며 글또 5기의 되돌아보고자 합니다.\n돌아보기 5기의 시작 어느 직장이나 바쁜 시즌이 있듯이, 현재 일하고 있는 직장에서는 10~12월에 항상 무언가가 터지는 시즌입니다. 연말을 앞두고 쎄한 느낌이 들어서였을까요, 11월 초 글또 5기를 시작하면서 다짐글을 작성하지 않았습니다.","title":"[KR] 글또 5기를 마무리하며"},{"content":"1. 정규표현식이란 Regular Expression (또는 Regex)\n 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어. (Wikipedia)\n 2. 정규표현식의 구조 /PATTERN/FLAG\n / :  정규표현식임을 알리는 기호. 정규표현식의 시작과 끝에 위치함.   PATTERN  정규표현식으로 찾고자하는 문자열의 패턴   FLAG  옵션    3. 정규표현식, 표현의 종류 3.1. 그룹과 범위 group and ranges  |  OR, 또는   ()  그룹 지정 복수의 패턴을 하나의 그룹으로 묶어 찾는 식으로 사용할 수 있음 예시)  gray 또는 grey 를 찾고자 할 때  /gr(a|e)y/gm   URL 패턴을 찾고자 할 때  /(http|https):// ... 이하 생략   한번에 패턴을 검색하되, 다른 별개의 그룹으로 찾고자 할 때  (Hi|Hello)|(AND)   그룹을 지정하고 싶지 않을 때     (?:)  그룹을 지정하지 않음 예시)  grey 또는 gray를 찾되, 그룹으로 지정하지않음  (?:grey|gray)       []  대괄호의 모든 문자열에 대해서, 하나라도 만족하는 문자열 예시)  gr로 시작하는 모든 알파벳 3개로 구성된 문자열  gr[a-zA-Z0-9]   gr과 y사이에 a, e, d 중 하나를 충족하는 문자열  gr[aed]y       [^]  부정 문자열. 괄호 안의 어떤 문자열을 제외한 문자열 예시)  대소문자 알파벳, 숫자, 공백이 아닌 모든 문자열  [^a-zA-Z0-9 ]        3.2. 수량 quantifier  ?  없거나, 있거나 (zero or one)   *  없거나, 있거나, 많거나 (zero or more)   +  하나 또는 많이 (one or more)   {n}  n번 반복   {n,}  최소 n번 반복   {,m}  최대 m번 반복   {n, m}  최소 n번, 최대 m번 반복    3.3. 경계 boundary-type  \\b  단어의 경계 문자열의 앞 또는 뒤에서 쓰이는 문자열만 찾음   \\B  단어의 경계가 아님   ^  문장의 시작   $  문장의 끝    3.4. 문자열 character classes  \\  특수 문자가 아닌 문자 예시)  정규표현의 .이 아닌 실제 문장에서 .을 찾고자 할 때  \\.   정규표현식에서 그룹지정의 ()가 아닌, 실제 텍스트에서 ()를 찾고자 할 때,  \\(\\)       .  줄바꿈을 제외한 모든 문자   \\d  숫자   \\D  숫자 아님   \\w  문자열   \\W  문자열 아님   \\s  공백   \\S  공백 아님    4. 간단한 예제 4.1. 전화번호 찾기  \\d{2,3}[- .]\\d{3,4}[- .]\\d{4}  4.2. E-mail 찾기  [\\S]+@[a-zA-Z0-9-]+\\.[a-z.]+  4.3. URL 찾기 (Youtube 주소에서 default url을 제외한 영상 고유의 id찾기)\n (https?:\\/\\/)?(www\\.)?youtu\\.be\\/([a-zA-Z0-9-]+)  5. 파이썬과 정규표현식 5.1. 정규표현식 패키지 - re  파이썬에 내장된 re 패키지를 사용하면 됨. (문서)  import re 5.2. 주요 기능 5.2.1. 정규식 패턴 정의  re.compile  정규식 패턴을 직접 변수로 전달할 수도 있겠지만, re.compile을 통해 미리 컴파일하여 객체로 사용하는 것이 편함    pat = re.compile(\u0026#39;[a-z]+\u0026#39;) print(pat) re.compile('[a-z]+') 5.2.2. 검색  .match  문자열의 처음부터 정규식 패턴과 일치하는지 검색 묹자열의 처음부터 일치하지 않을 경우 검색되지 않음    m = re.match(pat, \u0026#34;python @789\u0026#34;) print(m) \u0026lt;re.Match object; span=(0, 6), match='python'\u0026gt; m = re.match(pat, \u0026#34;#123 python @789\u0026#34;) print(m) None  .search  문자열 전체에 대해서 정규식 패턴과 일치하는 문자열 검색    m = re.search(pat, \u0026#34;#123 python @789\u0026#34;) print(m) \u0026lt;re.Match object; span=(5, 11), match='python'\u0026gt;  re.findall  정규식 패턴과 매치되는 모든 부분 문자열(substring)을 리스트로 리턴    m = re.findall(pat, \u0026#34;#123 python @789\u0026#34;) print(m) ['python']  re.finditer  정규식 패턴과 매치되는 모든 부분 문자열을 iter 객체로 리턴    m = re.finditer(pat, \u0026#34;#123 python pythonista, python @789\u0026#34;) for match in m: print(match) \u0026lt;re.Match object; span=(5, 11), match='python'\u0026gt; \u0026lt;re.Match object; span=(12, 22), match='pythonista'\u0026gt; \u0026lt;re.Match object; span=(24, 30), match='python'\u0026gt;  검색 객체의 메서드  .group() : 매치된 문자열 리턴 .start() : 매치된 문자열의 시작 위치 리턴 .end() : 매치된 문자열의 끝 위치 리턴 .span() : 매치된 문자열의 (시작, 끝) 튜플 리턴    m = re.finditer(pat, \u0026#34;#123 python pythonista, python @789\u0026#34;) for match in m: print(\u0026#34;매치된 문자열 match : {}\u0026#34;.format(match.group())) print(\u0026#34;매치된 문자열 start : {}\u0026#34;.format(match.start())) print(\u0026#34;매치된 문자열 end : {}\u0026#34;.format(match.end())) print(\u0026#34;매치된 문자열 span : {}\u0026#34;.format(match.span())) print(\u0026#34;----------------------------\u0026#34;) 매치된 문자열 match : python 매치된 문자열 start : 5 매치된 문자열 end : 11 매치된 문자열 span : (5, 11) ---------------------------- 매치된 문자열 match : pythonista 매치된 문자열 start : 12 매치된 문자열 end : 22 매치된 문자열 span : (12, 22) ---------------------------- 매치된 문자열 match : python 매치된 문자열 start : 24 매치된 문자열 end : 30 매치된 문자열 span : (24, 30) ---------------------------- 5.3. 문자열 변경  re.sub(정규식패턴, 변경할문자열, 타겟문자열)  문자열 바꾸기 파이썬에서 .replace()와 같은 역할을 함    m = re.sub(pat, \u0026#34;hello\u0026#34;, \u0026#34;#123 python @789\u0026#34;) print(m) #123 hello @789 Reference  정규표현식 , 더이상 미루지 말자 (Youtube 영상) Programiz - Python Programming 정규 표현식에 대한 쉬운 설명 점프 투 파이썬(위키독스)  ","permalink":"https://lucaseo.github.io/posts/2021-04-22-python-regex-basic/","summary":"1. 정규표현식이란 Regular Expression (또는 Regex)\n 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어. (Wikipedia)\n 2. 정규표현식의 구조 /PATTERN/FLAG\n / :  정규표현식임을 알리는 기호. 정규표현식의 시작과 끝에 위치함.   PATTERN  정규표현식으로 찾고자하는 문자열의 패턴   FLAG  옵션    3. 정규표현식, 표현의 종류 3.1. 그룹과 범위 group and ranges  |  OR, 또는   ()  그룹 지정 복수의 패턴을 하나의 그룹으로 묶어 찾는 식으로 사용할 수 있음 예시)  gray 또는 grey 를 찾고자 할 때  /gr(a|e)y/gm   URL 패턴을 찾고자 할 때  /(http|https):// .","title":"[KR] 정규표현식 기본 개념과 파이썬 re 패키지"},{"content":" 본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Image - 도커는 레이어드 파일 시스템 기반\n 도커 이미지는 프로세스가 실행되는 파일들의 집합 또는 환경이라고 할 수 있음 프로세스가 실행되면 발생하는 파일들에 대한 변경을 이미지에 쌓는 것  Image  이미지는 두가지로 나뉠 수 있음  읽기전용 Only Read 쓰기가능 Writable   Base Image  읽기 전용 수정할 수 없음 대신 해당 이미지 위에 또 다른 층을 추가할 수 있음    예시 1) docker commit을 통해 이미지 만들기 우분투 베이스 이미지에 Git을 설치하여 새로운 이미지로 저장하기\n 우분투 베이스 이미지에 Git 설치  docker pull ubuntu:latest docker run -it --name git ubuntu:latest bash apt-get update apt-get install -y git git --version # git version 2.25.1 변경이 생긴 베이스 이미지를 커밋하기   docker commit으로 이미지 만들기  :git이라는 태그를 붙여 새로운 이미지를 만듬    docker commit git ubuntu:git docker run -it --name git2 ubuntu:git bash Dockerfile   Dockerfile 핵심 명령어\n FROM : 베이스 이미지 지정 RUN : 쉘 명령어 실행 (도커 이미지를 만들기 위해 사용하는 명령여) CMD : 컨테이너 기본 실행 명령어 (도커 컨테이너를 만들때, 실행할 때 사용하는 명령어) EXPOSE : 컨테이너에서 사용하는 포트 정보 ENV : 환경변수 설정 ADD : 파일 도는 디렉토리 추가. (URL / ZIP 사용 가능) COPY : 로컬에 있는 파일 또는 디렉토리를 복사하여 이미지 내에 추가 ENTRYPOINT : 컨테이너 기본 실행 명령어 VOLUMN : 외부 마운트포인트 생성 USER : RUN, CMD, ENTRYPOINT를 실행하는 사용자 WORKDIR : 작업 디렉토리 변경/설정 ARGS : 빌드타임 환경변수 설정 LABEL : key - value 데이터 ONBUILD : 다른 빌드의 베이스로 사용될 때 사용하는 명령어    docker build로 이미지 만들기\n 전체 명령어 규칙  docker build -t {유저네임스페이스/이미지이름:태그} {빌드컨텍스트} -t : 이미지의 이름 지정 -f : Dockerfile의 위치가 다른 디렉토리라면 해당 옵션을 활용할 수 있음   빌드컨텍스트  현재 디렉토리일 경우 점(.)을 사용 필용한 경우 다른 디렉토리를 지정할 수 있음      .dockerignore 지정\n .gitignore와 비슷한 역할을 함 도커 빌드 컨텍스트에서 지정된 패턴의 파일을 무시함 민감한 정보를 제외하는 용도로 사용됨 빌드속도를 개선하기 위해 불필요한 파일을 제외하는 역할로도 사용됨 빌드할 때 사용되는 파일을 .dockerignore에 포함시켜서는 안 됨    예시 2) Dockerfile과 docker build를 통해 이미지 만들기  Dockerfile  FROMubuntu:latestRUN apt-get updateRUN apt-get install -y git Dockerfile을 빌드  docker build -t ubuntu:git-dockerfile . 도커 이미지 만들기 예시 3) Node 기반 Web app 이미지 만들기  Dockerfile  # 1. base image 정의 FROM ubuntu:20.04 RUN apt-get update # 2. node 설치 ## DEBIAN_FRONTEND=noninteractive ## RUN DEBIAN_FRONTEND=noninteractive apt-get install -y nodejs npm # 3. 현재 로컬 디렉토리에 있는 소스를 이미지 내 디렉토리에 복사 COPY . /usr/src/app # 4. Nodejs 패키지 설치 WORKDIR /usr/src/app # 이동 RUN npm install\t# npm install 실행 # 5. WEB 서버 실행 (Listen 포트 정의) EXPOSE 3000\t# 포트 정보 CMD node app.js\t# 컨테이너에서 실행  .dockerignore  node_modules/*  빌드 시작  docker build -t webapp .  빌드 된 도커 이미지 실행  docker run -p 3000:3000 webapp Dockerfile 최적화 하기 예시 4) Dockerfile 최적화 하기  Dockerfile  # 최적화 포인트1: 이미 node가 설치되어 있는 base image 정의 FROM node:12 # 최적화 포인트2: 패키지를 우선적으로 복사 COPY .package* /usr/src/app/ WORKDIR /usr/src/app RUN npm install COPY . /usr/src/app WORKDIR /usr/src/app EXPOSE 3000 CMD node app.js Reference  인프런 초보를 위한 도커 안내서  ","permalink":"https://lucaseo.github.io/posts/2021-04-21-docker-image-basic/","summary":"본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Image - 도커는 레이어드 파일 시스템 기반\n 도커 이미지는 프로세스가 실행되는 파일들의 집합 또는 환경이라고 할 수 있음 프로세스가 실행되면 발생하는 파일들에 대한 변경을 이미지에 쌓는 것  Image  이미지는 두가지로 나뉠 수 있음  읽기전용 Only Read 쓰기가능 Writable   Base Image  읽기 전용 수정할 수 없음 대신 해당 이미지 위에 또 다른 층을 추가할 수 있음    예시 1) docker commit을 통해 이미지 만들기 우분투 베이스 이미지에 Git을 설치하여 새로운 이미지로 저장하기","title":"[KR] Docker Image 개념"},{"content":" 본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Compose Docker Compose란  지금까지는 도커에서 개별의 명령어를 쳐서 이미지를 다운로드하고 컨테이너를 띄우는 과정을 거쳤음 도커 명령어를 통한 작업은 실수가 발생할 수 있는데, 도커 컴포즈는 이 문제를 해결할 수 있는 간결한 방법을 제시함 도커 컴포즈는 띄우려고 하는 복수의 컨테이너에 대한 사항을 Yaml(야믈)파일에 정리하여, 한번에 실행시키는 방식으로 동작함.  Docker Compose 설치   도커 컴포즈는 기본적으로 docker for mac을 설치할 때 함께 설치됨 (리눅스의 경우 그러하지 않기 때문에 따로 설치가 필요함)\n  리눅스의 설치\n  $ sudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.29.0/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose  설치 버전 확인하기  $ docker-compose version docker-compose version 1.28.5, build c4eb3a1f docker-py version: 4.4.4 CPython version: 3.9.0 OpenSSL version: OpenSSL 1.1.1h 22 Sep 2020 Yaml(.yml) 개념 XML, JSON과 같이 시스템 사이에 데이터를 주고 받을 때 사용하는 포맷\n XML 의 구조:  key-value : \u0026lt;key\u0026gt;value\u0026lt;/key\u0026gt; array :    \u0026lt;parent\u0026gt; \u0026lt;child1\u0026gt;\u0026lt;/child1\u0026gt; \u0026lt;child2\u0026gt;\u0026lt;/child2\u0026gt; \u0026lt;child3\u0026gt;\u0026lt;/child3\u0026gt; \u0026lt;/parent\u0026gt;  JSON 의 구조  key-value : {key: value} array    {key: [array1], [array2], [array3], }  Yaml의 구조  key-value : key: value array : 하이픈을 통해 array임을 구별    key : - array1 - array2 docker-compose.yml 예제  워드프레스 블로그를 도커로 띄우기  요구사항  MySQL 컨테이너 띄우기  블로그 데이터베이스가 저장될 볼륨 마운트 하기 워드프레스를 위한 데이터베이스 생성하기   Wordpress 컨테이너 띄우기  볼륨 마운트 하기 MySQL 컨테이너에 연결하여 데이터베이스 사용하기        # docker-compose.yml version: \u0026#39;2\u0026#39; services: db: image: mysql/mysql-server:8.0.15 volumes: - ./mysql:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: wordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: image: wordpress:latest volumes: - ./wp:/var/www/html ports: - \u0026#34;8000:80\u0026#34; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_USER: wordpress 실행 $ docker-compose up Reference  인프런 초보를 위한 도커 안내서  ","permalink":"https://lucaseo.github.io/posts/2021-04-15-docker-compose-basic/","summary":"본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Compose Docker Compose란  지금까지는 도커에서 개별의 명령어를 쳐서 이미지를 다운로드하고 컨테이너를 띄우는 과정을 거쳤음 도커 명령어를 통한 작업은 실수가 발생할 수 있는데, 도커 컴포즈는 이 문제를 해결할 수 있는 간결한 방법을 제시함 도커 컴포즈는 띄우려고 하는 복수의 컨테이너에 대한 사항을 Yaml(야믈)파일에 정리하여, 한번에 실행시키는 방식으로 동작함.  Docker Compose 설치   도커 컴포즈는 기본적으로 docker for mac을 설치할 때 함께 설치됨 (리눅스의 경우 그러하지 않기 때문에 따로 설치가 필요함)","title":"[KR] Docker Compose 개념"},{"content":" 본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n 1. 도커의 기본적인 명령어 ls (또는 ps)  docker container ls (= docker ps)  실행 중인 컨테이넝   docker container ls -a  실행이 중지된 컨테이너까지 출력    stop docker stop [OPTIONS] CONTAINER [CONTAINER ... ]  실행 중인 컨테이너를 중지하는 명령어 실행 중인 컨테이너를 복수로 중지시킬 수도 있음  rm docker rm [OPTIONS] CONTAINER [CONTAINER ... ]  종료된 컨테이너를 삭제하는 명령어  logs docker logs [OPTIONS] CONTAINER  기본 옵션  -f : fetch. 새로고침 할 때마다 로그 발생 시 실시간으로 업데이트 됨 --detail: 가장 마지막 부분을 보여줌   컨테이너가 정상적으로 동작하는지 확인하는 좋은 방법 중 하나  pull ``` docker pull [OPTIONS] NAME[:TAG|@DIGEST] ``` - 도커 이미지 다운로드  rmi ``` docker rmi [OPTIONS] IMAGE [IMAGE...] ``` - 도커 이미지 삭제 - 단, 컨테이너가 실행중인 이미지는 삭제되지 않음.  2. run docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]  기능  컨테이너 실행   명령어  -d : detached mode (백그라운드 모드) -p : 호스트와 컨테이너의 포트를 연결 -v : 호스트와 컨테이너의 디렉토리 연결 -e : 컨테이너 내에서 사용할 환경변수 설정 --name : 컨테이너 이름 설정 --rm : 프로세스 종료시 컨테이너 자동 제거  이 옵션이 없다면, 컨테이너가 종료되더라도 삭제되지 않고 남아있어 수동으로 삭제해야 함.   -it : 사용자가 사용할 수 있는 터미널을 위한 옵션  -i : 입력을 가능하게 하기 -t : 터미널 띄우기   --network : 네트워크 연결   설명  run 명령어를 사용하면, 사용할 이미지가 이미 저장되어 있는지 확인 후, 없다면 pull한 뒤, 컨테이너를 create하고 start함  각각의 동작들은 각각의 명령어로도 따로 실행 가능함.   컨테이너가 실행되지만, 별 다른 명령어를 전달하지 않으면, 생성되자마자 종료됨.  컨테이너는 기본적으로 프로세스이기 때문에, 실행 중인 프로세스가 없으면 컨테이너는 종료됨. 예시: docker run ubuntu:20.04      3. exec  이미 실행중인 도커 컨테이너에 접속할 때 사용 컨테이너 안에 ssh server 등을 설치하지 않고 접속함.  4. network docker network create [OPTIONS] NETWORK  도커 컨테이너끼리 이름으로 통신할 수 있는 가상 네트워크를 만듬  docker network connect [OPTIONS] NETWORK CONTAINER  기존에 생성된 컨테이너에 네트워크를 추가  5. volumn (-v)  호스트의 드라이브를 마운트 시키는 명령어 마운트된 드라이브 없이 컨테이너를 중지/삭제시키면 데이터가 날라가게 됨 따라서 데이터를 저장하고 싶다면, 드라이브를 볼륨으로 마운트시켜야 함.  -v [my own dir]:[container dir] 6. 예제  우분투 이미지 컨테이너로 띄우기  docker run --rm -it ubuntu:20.04 /bin/sh  /bin/sh : sh를 실행함 -it : 키보드 입력 --rm : 프로세스 종료 후 컨테이너 자동 삭제  웹 어플리케이션 실행  docker run --rm -p 5678:5678 hashicorp/http-echo -text=\u0026quot;helllo world\u0026quot;  -p : 포트 연결 [호스트 포트]:[도커 내 포트] -text : 웹 어플리케이션에 들어가는 변수 전달  Redis 실행하기 (메모리기반 데이터베이스)  docker run --rm -p 1234:6379 redis  레디스 테스트 해보기  set hello world get hello quit MySQL 실행하기  docker run -d -p 3306:3306 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true \\ --name mysql \\ mysql/mysql-server:8.0.15   mysql 패스워드를 입력하지 않겠다는 환경변수를 -e로 패스\n  --name : 컨테이너에 이름 설정\n  -d : 백그라운드로 실행\n  실제로 실행을 해서 워드프레스용 DB를 생성함.\n docker exec -it mysql mysql mysql 이라는 컨테이너에서 mysql이라는 명령어를 실행    create database wp CHARACTER SET utf8; CREATE USER wp@'%' IDENTIFIED BY 'wp'; GRANT ALL PRIVILEGES ON wp.* TO wp@'%' WITH GRANT OPTION; flush privileges; quit 워드프레스 블로그 실행하기 (앞서 MySQL이 실행 중인 상태에서)  docker run -d -p 8080:80 \\ -e WORDPRESS_DB_HOST=host.docker.internal \\ -e WORDPRESS_DB_NAME=wp \\ -e WORDPRESS_DB_USER=wp \\ -e WORDPRESS_DB_PASSWORD=wp \\ wordpress  기존에 띄워져 있던 mysql 컨테이너의 데이터베이스를 이용해서 워드프레스 블로그를 띄움.  네트워크  docker network create app-network  app-network라는 이름으로 네트워크를 만듬 (wordpress - mysql을 연결)  docker network connect app-network mysql  mysql 컨테이너에 네트워크를 추가  docker run -d -p 8080:80 \\ --network=app-network \\ -e WORDPRESS_DB_HOST=mysql \\ -e WORDPRESS_DB_NAME=wp \\ -e WORDPRESS_DB_USER=wp \\ -e WORDPRESS_DB_PASSWORD=wp \\ wordpress  워드프레스는 이미지를 실행할 때 네트워크를 지정해볼 수 있음 network는 위에서 생성한 app-network를 지정하고, host는 mysql의 이름으로 접근함.  docker stop mysql docker rm mysql docker run -d -p 3306:3306 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true \\ --network=app-network \\ --name mysql \\ -v /Users/wg/dvlp/Lucas/learn_container/volumn:/var/lib/mysql \\ mysql/mysql-server:8.0.15 docker exec -it mysql mysql create database wp CHARACTER SET utf8; CREATE USER wp@'%' IDENTIFIED BY 'wp'; GRANT ALL PRIVILEGES ON wp.* TO wp@'%' WITH GRANT OPTION; flush privileges; quit\t 기존의 mysql 컨테이너를 삭제하고 volumn을 마운트 시켜 다시 띄움 이 경우 mysql의 데이터베이스는 마운트된 디렉토리에 데이터가 남게 되므로, 나중에 다시 동일한 연결하더라도 데이터를 유지할 수 있음.  Reference  인프런 초보를 위한 도커 안내서  ","permalink":"https://lucaseo.github.io/posts/2021-04-11-docker-basic-commands/","summary":"본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n 1. 도커의 기본적인 명령어 ls (또는 ps)  docker container ls (= docker ps)  실행 중인 컨테이넝   docker container ls -a  실행이 중지된 컨테이너까지 출력    stop docker stop [OPTIONS] CONTAINER [CONTAINER ... ]  실행 중인 컨테이너를 중지하는 명령어 실행 중인 컨테이너를 복수로 중지시킬 수도 있음  rm docker rm [OPTIONS] CONTAINER [CONTAINER .","title":"[KR] Docker 기본 명령어"},{"content":" 이번 포스트에서는 현재 일하고 있는 Who\u0026rsquo;s Good에서 웹어플리케이션 기반의 간단한 툴을 개발한 과정을 기술합니다. 자세한 사내 업무 내용은 생략하며, 유사한 상황 및 시나리오로 대체했음을 밝힙니다.\n \u0026amp;nbsp\n1. Quality Check (QC) 현재 일하고 있는 Who\u0026rsquo;s Good에서는 뉴스 기사를 기반으로 기업의 *ESG 리스크를 평가합니다. 모델을 통해 산출된 결과에 대해서는 꼭 검토 및 검증 프로세스를 거치는데요, 산출된 결과값이 정답인지 아닌지 여부를 확인하는 과정이고, Quality Check 또는 줄여서 QC라고 합니다. 주로 ESG 도메인의 전문가인 ESG 리서쳐 또는 QC 스태프가 이 과업을 수행합니다.\n(*ESG : Environment, Social, Governance 기업의 환경, 사회, 지배구조)\n1.1. 기존의 QC과정 기존의 QC과정에서는 (1)FTP서버를 통해 모형의 결과를 다운 받아 (2)구글 스프레드시트에서 QC를 진행하고 (3)이를 개발자 또는 DB관리자에 전달하여 DB에 업로드 되게끔 하는 과정을 거쳤습니다. 이 과정을 간단히 아래의 이미지와 같이 간단히 도식화 해보았습니다.\n   1.2. QC과정의 불편한 부분 현재 QC 작업은 불가피하게도 ESG 도메인을 가진 전문가가 수행해야 하고, 데이터의 퀄리티를 유지하기 위해 절대 생략할 수 없는 과정입니다. 하지만 옆에서 제가 관찰한 바로는 기존의 QC 과정에서 몇 가지 불편한 점과 개선할 수 있는 부분들이 분명히 보였습니다.\n 파일질라(Filezilla) 같은 프로그램으로 FTP서버에 접속하여 QC 템플릿을 다운 받는 것이 불편함. 앞서 기술한 바와 같이 뉴스기사를 통해 기업을 분석하기 때문에 기업 관련 기사 발생량에 따라 QC 작업에 걸리는 시간이 매우 다를 수 있음. 따라서, 완료된 QC 결과를 전달하는 시간이 일정하지 못했고, 이에 개발자 또는 DB관리자가 불규칙하게 일일이 대응해야함. QC하는 과정에서 데이터를 수정하거나, 결과를 기입하는 과정에 실수가 발생할 수 있음. 따라서 DB입력 과정에서 에러가 발생하거나, 입력 후 추후 데이터 퀄리티 면에서 문제가 이어지는 등, 불확실한 상황이 이어짐.  물론 이러한 불편한 점들에 대한 피드백과, QC스태프들이 편리하게 사용할 수 있는 QC툴에 대한 니즈가 많았습니다. 하지만 프로젝트의 우선순위(항상 외부를 대응 하는 일들이 더 급하고 중요해서 내부가 밀려나가는 상황)와 개발팀의 일정이 있어, 이러한 부분의 개선이 빠른 시일 내에 이루어질 것 같아 보이진 않았죠 \u0026hellip;\n\u0026amp;nbsp\n2. 그래도 이 상태로 계속 갈 수는 없으니까 2.1. 최소한의 요구사항 그래도 제가 도울 수 있는 점이 있지 않을까 싶어, QC툴에 대해 팀원들로부터 의견을 취합해보았고, 수많은 희망사항들을 거르고 걸러서 아래와 같이 의견을 좁혀봤습니다.\n QC템플릿을 다운로드하는 과정이 더 간편했졌으면 좋겠다. QC는 아직 엑셀로 하는게 편하다. 엑셀을 이길 수 있는 걸 가져올 생각이 아니라면 쉽게 생각하지 않는게 좋을 거야 찡끗 QC 결과에 대한 검증이 자동으로 이루어졌으면 좋겠다. QC 결과에 오류가 있다면 업로드가 되지 말아야 한다. 직접 업로드하는 과정이 간편하면 좋다.  2.2. 그러면 Streamlit 을 써보자 저는 개발자는 아닙니다\u0026hellip;만, 제가 애용하는 Streamlit을 활용하면 QC툴의 니즈를 어느 정도 충족할 수 있는 툴을 만들 수 있을 것 같았습니다.\n   \u0026amp;nbsp\nStreamlit은 간단하게 ML/DL 어플리케이션을 구현하고 프로토타입을 만들 수 있는 파이썬 기반의 웹어플리케이션 프레임워크로, 2019년 하반기 등장 이후 아주 빠른 속도로 성장하고 있습니다.\nStreamlit은 아주 다양한 기능을 제공하고 있어, ML/DL 관련의 프로토타이핑이라는 기존의 목적 이외에도 더 다양한 용도로 활용 가능합니다. 저 같이 웹 개발에 대한 지식이 전무하고 없고 파이썬 하나만 알고 있더라도, (Django나 Flask를 알지 못 해도) 웹 어플리케이션을 만들 수 있습니다.\nStreamlit이 제공하는 다양한 기능은 Streamlit 소개 글 (클릭)를 참고해주세요.\n2.3. 이렇게 개선해볼 수 있지 않을까 Streamlit을 통해 (1) QC파일 다운로드 (2) QC 결과 검증 (3) QC 파일 업로드 기능을 가진 웹어플리케이션을 개발하고, 슬랙 알림과 DB저장 등의 과정을 스케줄링하는 부분은 Airflow를 통해 구축하기로 했습니다. (이 포스트에서 Airflow에 대한 자세한 내용은 생략하도록 하겠습니다.)\n   \u0026amp;nbsp\n3. 완성된 QC 툴의 예시 완성된 QC툴은 실제 업무 내용이 많이 포함되어 그대로 기술하기에는 어려운 점이 많기 때문에 이 포스트에서는 매우 유사한 형태와 기능을 하는 예제로 대체 했습니다.\n아래 QC툴은 영화 댓글 감성분석 모델의 결과를 검토 및 검증하는 가상의 QC 과정을 수행합니다. 이 툴에서 활용한 데이터는 네이버 영화 댓글 데이터를 가공하여 만든 가상의 데이터(fake data)입니다.\n\u0026amp;nbsp\nQC스태프, 사용자는 QC툴을 활용하여 아래의 과정을 수행합니다.\n  영화 댓글의 긍정/부정 감성분석 결과를 QC 템플릿으로 다운로드 각 산출 결과에 대해 ACCEPT/REJECT 을 입력하여 QC 작업을 수행 결과를 업로드   \u0026amp;nbsp\n그럼 툴을 한번 살펴보도록 하겠습니다. (전체 소스코드는 github에서 확인할 수 있습니다.)\n3.1. 메인 \u0026amp; QC 가이드라인 페이지 사용자는 간단한 인증 기능을 통해 툴을 활성화 시키고, QC작업을 수행하기에 앞서 QC 가이드라인과 주의사항 등을 확인할 수 있습니다.\n   3.2. QC 템플릿 다운로드 페이지 날짜를 설정하고, 해당 날짜의 데이터에 대한 QC 템플릿을 생성합니다. 사용자는 링크를 클릭해서 QC 템플릿 파일을 다운로드 할 수 있습니다.\n   \u0026amp;nbsp\n각 데이터에 대해 감성분석 모델이 1(긍정) , 0(부정)으로 분류한 결과를 검토하는 QC 템플릿 파일입니다. 사용자는 QC 템플릿 파일 내 qc_result 칼럼에 ACCEPT/REJECT를 입력하며 QC 작업을 수행합니다.\n   3.3. QC 템플릿 업로드 페이지 업로드 페이지에서 사용자는 QC 완료 파일을 업로드 합니다. 이 과정에서는 .xlsx 확장자인 파일만 올릴 수 있도록 하고, 업로드 파일명이 다른 형태를 띄고 있다면 업로드를 제한하는 기능을 포함하고 있습니다.\n그리고 qc_result 컬럼에 기입하지 않은 blank cell이 있는지, ACCEPT/REJECT 이 아닌 다른 값이 입력되진 않았는지 검증 작업을 거쳐 QC Result Validity Test를 통해 사용자에게 패스 여부를 알려줍니다. 검증 작업을 패스하지 못 한다면, 파일을 업로드할 수 없습니다.\n   \u0026amp;nbsp\n마지막으로 사용자는 QC 완료 결과를 다시 한번 검토할 수 있으며, 몇 가지 셀프 체크와 파일을 업로드 하는 본인이 누구인지 선택한 후, QC 완료 파일을 업로드할 수 있습니다.\n   \u0026amp;nbsp\n4. 후기 이런 식으로 개발한 QC툴은 현재도 사내에서 사용 중에 있습니다. 이 대신 잇몸이라는 말처럼, 정식으로 개발한 툴은 아니지만, 불편했던 점을 어느 정도 해소하고, 리소스와 시간을 절약할 수 있게 된 부분이 가장 큰 추후 제대로된 QC툴이나 시스템을 구축할 때 개발팀에서 참조할 수 있는 프로토타입의 역할도 하고 있습니다.\n언뜻 보면 간단한 것들도 실제로 개발하고 구현하려면, 밑바닥에서부터 새로 배워야 하는 것들이 많아서 예전에는 엄두가 안 났었습니다만, Streamlit이 아주 간편해서 앞으로도 급하게 프로토타이핑을 할 때 자주 사용할 것 같습니다. 빛과소금 또트림릿\n5.Source code  Github  6.Reference  Streamlit Documentation Streamlit Discussion NSMC Dataset  ","permalink":"https://lucaseo.github.io/posts/2021-03-06-streamlit-simple-tool-development/","summary":"이번 포스트에서는 현재 일하고 있는 Who\u0026rsquo;s Good에서 웹어플리케이션 기반의 간단한 툴을 개발한 과정을 기술합니다. 자세한 사내 업무 내용은 생략하며, 유사한 상황 및 시나리오로 대체했음을 밝힙니다.\n \u0026amp;nbsp\n1. Quality Check (QC) 현재 일하고 있는 Who\u0026rsquo;s Good에서는 뉴스 기사를 기반으로 기업의 *ESG 리스크를 평가합니다. 모델을 통해 산출된 결과에 대해서는 꼭 검토 및 검증 프로세스를 거치는데요, 산출된 결과값이 정답인지 아닌지 여부를 확인하는 과정이고, Quality Check 또는 줄여서 QC라고 합니다. 주로 ESG 도메인의 전문가인 ESG 리서쳐 또는 QC 스태프가 이 과업을 수행합니다.","title":"[KR] 파이썬 기반 데이터 QC 툴 개발기(근데 이제 또 Streamlit을 곁들인)"},{"content":"1. 트리의 개념 1.1. 트리의 정의  노드와 브랜치를 활용하여 구성한 데이터 구조  1.2. 트리와 관련된 용어 트리 관련 용어  노드(Node)  데이터를 저장하는 기본 요소 다른 노드와 연결되는 브랜치에 대한 정보도 포함   브랜치(Branch)  상위 노드와 하위 노드를 연결하는 가지   루트노드(Root Node)  트리 최상단에 위치한 최상위 노드   레벨(Level)  최상위 노드를 Level 0이라고 할 때, 특정 레벨에 위치한 노드의 집합   부모 노드(Parent Node)  상위 노드   자식 노드(Child Node)  하위 노드   단말 노드(Leaf Node)  하위 노드가 없는 노드   형제 노드(Sibling Node)  동일한 부모 노드를 가진 노드   깊이(Depth)  루트에서 어떤 노드에 도달하기 위해 거쳐야 하는 간선의 수   크기(Size)  자신을 포함한 모든 자식노드의 개수   높이(Height)  하위 트리 개수 / 간선 수 (degree) = 각 노드가 지닌 가지의 수   노드의 차수(Degree of Node)  각 노드가 지닌 가지의 수   트리의 차수(Degree of Tree)  트리의 최대 차수    트리의 종류  이진 트리 vs 이진 탐색 트리  이진 트리(Binary Tree)  노드의 최대 브랜치가 2개인 트리   이진 탐색 트리(Binary Search Tree: BST)  왼쪽 노드는 해당 노드보다 작은 값, 오른쪽 노드는 해당 노드보다 큰 값을 가지는 조건이 적용된 이진트리      1.3. 트리의 적용 예시  주요 용도: 데이터 검색(탐색)  1.4. 이진 탐색 트리의 시간복잡도  트리의 높이(Depth)를 \\(h\\)라고 표기한다면, 시간복잡도는 \\(O(h)\\) 노드의 개수가 \\(n\\)일 때,  트리의 높이 \\(h = \\log_2n\\) 에 가까움. 시간 복잡도는 \\(O(\\log n)\\) -\u0026gt; (빅오 표기법에서 \\(\\log\\)의 밑은 2) 한번 판단할 때마다, 50%씩 탐색할 후보를 제외할 수 있음. 시간이 단축 됨.    1.5. 이진 탐색 트리(BST)의 장단점  장점: 탐색 속도를 개선할 수 있음 단점:  평균 시간 복잡도는 \\(O(\\log n)\\)이지만, 이는 트리의 양쪽이 모두 균등할 때의 평균 시간복잡도라고 할 수 있음. 따라서, 트리가 한쪽으로만 치우쳐져 있는 최악의 경우에는 링크드리스트와 동일한 \\(O(n)\\)의 성능을 보여줌.    2. 파이썬을 통한 기본적인 이진탐색트리 구현  노드와 그 다음 노드를 연결된 형태를 띄기 때문에, 링크드 리스트로 구현하면 용이함.  2.1. 노드 클래스 class Node: def __init__(self, value): self.value = value self.left = None self.right = None 2.2. 최상단에 위치한 head 설정 def __init__(self, head): self.head = head 2.3. 이진탐색트리에 데이터 저장 (insert) def insert(self, value): self.current_node = self.head # 각 노드를 순회 while True: if value \u0026lt; self.current_node.value: #현재노드보다 작은 경우 : 왼쪽 가지로 이동 if self.current_node.left != None: # 왼쪽 가지로 이동했을 때, 이미 데이터 노드가 있다면 self.current_node = self.current_node.left # 비교대상 노드를 왼쪽가지 노드로 교체 else: # 데이터 노드가 없다면 self.current_node.left = Node(value) # value를 가지는 새로운 노드를 만들어서 왼쪽 가지에 삽입 break else: # 오른쪽도 동일함. if self.current_node.right != None: self.current_node = self.current_node.right else: self.current_node.right = Node(value) break 2.4. 이진 탐색 트리의 탐색 (search) def search(self, value): self.current_node = self.head # HEAD 노드에서부터 찾고자 하는 노드를 순회하며 찾음 while self.current_node: if self.current_node.value == value: return True elif value \u0026lt; self.current_node.value: self.current_node = self.current_node.left else: self.current_node = self.current_node.right assert False, \u0026#34;Value does not exist\u0026#34; 2.3. 이진 탐색 트리의 삭제 (delete)  노드 삭제의 경우  삭제할 노드에 브랜치가 없을 때 : Leat Node 삭제 삭제할 노드에 브랜치가 한 개 있을 때 : Childe Node가 하나인 노드 삭제 삭제할 노드에 브랜치가 두 개 있을 때 : Childe Node가 둘인 노드 삭제    2.3.1. Case1: Leaf Node 삭제  삭제할 Node의 Parent Node가 삭제할 Node를 가리키지 않게 함.  2.3.2. Case2: Childe Node가 하나인 노드 삭제  삭제할 Node의 Parent Node가 삭제할 Node의 Child Node를 가리키게 함.  2.3.3. Case3: Childe Node가 둘인 노드 삭제  구현 방식  삭제할 Node의 오른쪽 자식들 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 함.  삭제할 노드의 오른쪽 자식 선택  오른쪽 자식의 가장 왼쪽에 있는 노드를 선택  해당 노드를 삭제할 노드의 부모 노드의 왼쪽 브랜치가 가리키게 연결 해당 노드의 왼쪽 브랜치가 삭제할 노드의 왼쪽 자식 노드를 가리키게 함 해당 노드의 오른쪽 브랜치가 삭제할 노드의 오른쪽 자식 노드를 가리키게 함  만약 해당 노드가 오른쪽 자식 노드를 가지고 있었을 경우, 해당 노드의 본래 부모 노드의 왼쪽 브랜치가 해당 노드의 오른쪽 자식 노드를 가리키게 함            Case3-1: 삭제할 노드가 부모 노드의 왼쪽에 있을 때\n 삭제할 Node의 오른쪽 자식들 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 함.  Case3-1-1: 삭제할 노드가 부모 노드의 왼쪽에 있고, 삭제할 노드의 오른쪽 자식 중 가장 작은 값을 가진 노드의 자식노드가 없을 때. Case3-1-2: 삭제할 노드가 부모 노드의 왼쪽에 있고, 삭제할 노드의 오른쪽 자식 중 가장 작은 값을 가진 노드의 오른쪽에 자식 노드가 있을 때  노드의 왼쪽으로는 더 작은 값을 가진 노드가 존재하기 때문에, 가장 작은 값을 가진 노드의 자식노드가 왼쪽에 있을 경우는 없음.      Case3-2: 삭제할 노드가 부모 노드의 오른쪽에 있을 때\n 삭제할 Node의 오른쪽 자식들 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 함.  Case3-2-1: 삭제할 노드가 부모 노드의 오른쪽에 있고, 삭제할 노드의 오른쪽 자식 중, 가장 작은 값을 가진 노드의 자식 노드가 없을 때 Case3-2-2: 삭제할 노드가 부모 노드의 오른쪽에 있고, 삭제할 노드의 오른쪽 자식 중, 가장 작은 값을 가진 노드의 오른 쪽에 자식 노드가 있을 때.  노드의 왼쪽으로는 더 작은 값을 가진 노드가 존재하기 때문에, 가장 작은 값을 가진 노드의 자식노드가 왼쪽에 있을 경우는 없음.      2.3.4. 삭제 코드 구현 def delete(self, value): searched = False # 삭제할 노드가 있는지 판단하는 boolean self.current_node = self.head # 현재 노드 선언 self.parent = self.head # 부모 노드 선언 while self.current_node: if self.current_node.value == value: # 삭제하고자 하는 노드를 찾았다면 searched = True # 삭제할 노드가 있다고 판단함 break elif value \u0026lt; self.current_node.value: self.parent = self.current_node self.current_node = self.current_node.left else: self.parent = self.current_node self.current_node = self.current_node.right if searched == False: assert searched, \u0026#34;Node does not exist\u0026#34; # Case 1 if (self.current_node.left == None) and self.current_node.right == None: # Leaf Node if value \u0026lt; self.parent.value: # 부모노드의 왼쪽일 경우 self.parent.left = None else: # 부모노드의 오른쪽일 경우 self.parent.right = None del self.current_node # Case 2-1 # 삭제할 노드가 왼쪽에 자식노드 한 개를 가지고 있을 경우 elif (self.current_node.left != None) and self.current_node.right == None: if value \u0026lt; self.parent.value: self.parent.left = self.current_node.left else: self.parent.right = self.current_node.left # Case 2-2 # 삭제할 노드가 오른쪽에 자식노드 한 개를 가지고 있을 경우 elif (self.current_node.left == None) and self.current_node.right != None: if value \u0026gt; self.parent: self.parent.left = self.current_node.right else: self.parent.right = self.current_node.left # Case 3: 삭제할 노드에 브랜치가 좌우로 존재할 때.  elif (self.current_node.left != None) and self.current_node.right != None: # Case 3-1: if value \u0026lt; self.parent_value: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.chage_node.left if self.chage_node.right != None: # Case 3-1-2 self.change_node_parent.left = self.change_node.right else: # Case 3-1-1 self.change_node_parent.left = None # 삭제 대상 노드의 부모/자식 노드 간 연결을 끊고, change_node로 대체함.  self.parent.left = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.elft # Case 3-2: else: #value \u0026gt; self.parent_value self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: # Case 3-2-2 self.change_node_parent.left = self.change_node_parent.right else: # Case 3-2-1 self.change_node_parent.left = None # 삭제 대상 노드의 부모/자식 노드 간 연결을 끊고, change_node로 대체함.  self.parent.right = self.change_node self.change_node.left = self.current_node.left self.change_node.right = self.current_node.right 2.4. 전체 코드 class Node: def __init__(self, value): self.value = value self.left = None self.right = None class NodeManagement: def __init__(self, head): self.head = head def insert(self, value): self.current_node = self.head while True: if value \u0026lt; self.current_node.value: if self.current_node.left != None: self.current_node = self.current_node.left else: self.current_node.left = Node(value) break else: if self.current_node.right != None: self.current_node = self.current_node.right else: self.current_node.right = Node(value) break def search(self, value): self.current_node = self.head while self.current_node: if self.current_node.value == value: return True elif value \u0026lt; self.current_node.value: self.current_node = self.current_node.left else: self.current_node = self.current_node.right assert False, \u0026#34;Number does not exist.\u0026#34; def delete(self, value): searched = False self.current_node = self.head self.parent = self.head while self.current_node: if self.current_node.value == value: searched = True break elif value \u0026lt; self.current_node.value: self.parent = self.current_node self.current_node = self.current_node.left else: self.parent = self.current_node self.current_node = self.current_node.right if searched == False: assert False, \u0026#34;Number does not exist.\u0026#34; if (self.current_node.left == None) and self.current_node.right == None: if value \u0026lt; self.parent.value: self.parent.left = None else: self.parent.right = None elif (self.current_node.left != None) and self.current_node.right == None: if value \u0026lt; self.parent.value: self.parent.left = self.current_node.left else: self.parent.right = self.current_node.left elif (self.current_node.left == None) and self.current_node.right != None: if value \u0026gt; self.parent.value: self.parent.left = self.current_node.right else: self.parent.right = self.current_node.left elif (self.current_node.left != None) and self.current_node.right != None: if value \u0026lt; self.parent.value: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.chage_node.left if self.chage_node.right != None: self.change_node_parent.left = self.change_node.right else: self.change_node_parent.left = None self.parent.left = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.elft else: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: self.change_node_parent.left = self.change_node_parent.right else: self.change_node_parent.left = None self.parent.right = self.change_node self.change_node.left = self.current_node.left self.change_node.right = self.current_node.right return True 데이터 입력 import random nums = set() while len(nums) != 100: nums.add(random.randint(0, 999)) print(nums) {512, 518, 525, 526, 536, 25, 539, 551, 53, 57, 570, 61, 62, 576, 577, 592, 597, 87, 600, 90, 602, 619, 110, 112, 113, 124, 130, 643, 133, 654, 143, 663, 154, 670, 160, 690, 188, 204, 209, 723, 212, 226, 232, 747, 748, 237, 754, 759, 248, 251, 259, 262, 775, 774, 272, 273, 274, 275, 277, 278, 791, 796, 798, 809, 815, 307, 829, 325, 332, 333, 847, 851, 852, 853, 355, 358, 888, 376, 894, 909, 400, 913, 912, 409, 929, 930, 422, 936, 425, 426, 446, 458, 466, 986, 477, 996, 488, 495, 499, 500}  head = Node(500) bst = NodeManagement(head) for num in nums: bst.insert(num) 데이터 탐색 # 탐색 대상 노드가 존재하는 경우 bst.search(643) True # 탐색 대상 노드가 존재하지 않는 경우 bst.search(879) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) \u0026lt;ipython-input-10-03b987c0a6aa\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 bst.search(879) \u0026lt;ipython-input-6-bbcc66356cab\u0026gt; in search(self, value) 39 self.current_node = self.current_node.right 40 ---\u0026gt; 41 assert False, \u0026quot;Number does not exist.\u0026quot; 42 43 AssertionError: Number does not exist. 데이터 삭제 # 숫자 10개 랜덤 선택 target_nums = set() while len(target_nums) != 10: target_nums.add(list(nums)[random.randint(0, 99)]) print(target_nums)  {160, 325, 518, 332, 525, 912, 62, 57, 602, 25}  for delete_num in target_nums: bst.delete(delete_num) 4. Reference  잔재미코딩  ","permalink":"https://lucaseo.github.io/posts/2021-03-01-python-datastructure-tree-bst/","summary":"1. 트리의 개념 1.1. 트리의 정의  노드와 브랜치를 활용하여 구성한 데이터 구조  1.2. 트리와 관련된 용어 트리 관련 용어  노드(Node)  데이터를 저장하는 기본 요소 다른 노드와 연결되는 브랜치에 대한 정보도 포함   브랜치(Branch)  상위 노드와 하위 노드를 연결하는 가지   루트노드(Root Node)  트리 최상단에 위치한 최상위 노드   레벨(Level)  최상위 노드를 Level 0이라고 할 때, 특정 레벨에 위치한 노드의 집합   부모 노드(Parent Node)  상위 노드   자식 노드(Child Node)  하위 노드   단말 노드(Leaf Node)  하위 노드가 없는 노드   형제 노드(Sibling Node)  동일한 부모 노드를 가진 노드   깊이(Depth)  루트에서 어떤 노드에 도달하기 위해 거쳐야 하는 간선의 수   크기(Size)  자신을 포함한 모든 자식노드의 개수   높이(Height)  하위 트리 개수 / 간선 수 (degree) = 각 노드가 지닌 가지의 수   노드의 차수(Degree of Node)  각 노드가 지닌 가지의 수   트리의 차수(Degree of Tree)  트리의 최대 차수    트리의 종류  이진 트리 vs 이진 탐색 트리  이진 트리(Binary Tree)  노드의 최대 브랜치가 2개인 트리   이진 탐색 트리(Binary Search Tree: BST)  왼쪽 노드는 해당 노드보다 작은 값, 오른쪽 노드는 해당 노드보다 큰 값을 가지는 조건이 적용된 이진트리      1.","title":"[KR] 자료구조 \u0026 알고리즘 : 트리(Tree)"},{"content":"SQL Practices - SELECT Practices set resources are referenced from w3resource.com\n SQL Exercises, Practice, Solution - Retrieve data from tables SQL Exercises, Practice, Solution - Using Boolean and Relational operators All practice set and answers are written in PostgreSQL   Sample Tables salesman    salesman_id name city commission     5001 James Hoog New York 0.15   5002 Nail Knite Paris 0.13   5005 Pit Alex London 0.11   5006 Mc Lyon Paris 0.14   5007 Paul Adam Rome 0.13   5003 Lauson Hen San Jose 0.12    \u0026amp;nbsp\norders    ord_no purch_amt ord_date customer_id salesman_id     70001 150.5 2012-10-05 3005 5002   70009 270.65 2012-09-10 3001 5005   70002 65.26 2012-10-05 3002 5001   70004 110.5 2012-08-17 3009 5003   70007 948.5 2012-09-10 3005 5002   70005 2400.6 2012-07-27 3007 5001   70008 5760 2012-09-10 3002 5001   70010 1983.43 2012-10-10 3004 5006   70003 2480.4 2012-10-10 3009 5003   70012 250.45 2012-06-27 3008 5002   70011 75.29 2012-08-17 3003 5007   70013 3045.6 2012-04-25 3002 5001    \u0026amp;nbsp\ncustomer    customer_id cust_name city grade salesman_id     3002 Nick Rimando New York 100 5001   3007 Brad Davis New York 200 5001   3005 Graham Zusi California 200 5002   3008 Julian Green London 300 5002   3004 Fabian Johnson Paris 300 5006   3009 Geoff Cameron Berlin 100 5003   3003 Jozy Altidor Moscow 200 5007   3001 Brad Guzan London  5005    \u0026amp;nbsp\nnobel_win    year subject winner country category     1970 Physics Hannes Alfven Sweden Scientist   1970 Physics Louis Neel France Scientist   1970 Chemistry Luis Federico Leloir France Scientist   1970 Physiology Julius Axelrod USA Scientist   1970 Physiology Ulf von Euler Sweden Scientist   1970 Physiology Bernard Katz Germany Scientist   1970 Literature Aleksandr Solzhenitsyn Russia Linguist   1970 Economics Paul Samuelson USA Economist   1971 Physics Dennis Gabor Hungary Scientist   1971 Chemistry Gerhard Herzberg Germany Scientist   1971 Peace Willy Brandt Germany Chancellor   1971 Literature Pablo Neruda Chile Linguist   1971 Economics Simon Kuznets Russia Economist   1978 Peace Anwar al-Sadat Egypt President   1978 Peace Menachem Begin Israel Prime Minister   1994 Peace Yitzhak Rabin Israel Prime Minister   1987 Physics Johannes Georg Bednorz Germany Scientist   1987 Chemistry Donald J. Cram USA Scientist   1987 Chemistry Jean-Marie Lehn France Scientist   1987 Physiology Susumu Tonegawa Japan Scientist   1987 Literature Joseph Brodsky Russia Linguist   1987 Economics Robert Solow USA Economist   1994 Literature Kenzaburo Oe Japan Linguist   1994 Economics Reinhard Selten Germany Economist    \u0026amp;nbsp\nitem_mast    pro_id pro_name pro_price pro_com     101 Mother Board 3200.00 15   102 Key Board 450.00 16   103 ZIP drive 250.00 14   104 Speaker 550.00 16   105 Monitor 5000.00 11   106 DVD drive 900.00 12   107 CD drive 800.00 12   108 Printer 2600.00 13   109 Refill cartridge 350.00 13   110 Mouse 250.00 12    \u0026amp;nbsp\nemp_details    emp_idno emp_fname emp_lname emp_dept     631548 Alan Snappy 27   839139 Maria Foster 57   127323 Michale Robbin 57   526689 Carlos Snares 63   843795 Enric Dosio 57   328717 Jhon Snares 63   444527 Joseph Dosni 47   659831 Zanifer Emily 47   847674 Kuleswar Sitaraman 57   748681 Henrey Gabriel 47   555935 Alex Manuel 57   539569 George Mardy 27   733843 Mario Saule 63     Practice Questions 1. Simple Select 1.1. Write a SQL statement to display all the information of all salesmen. SELECT * FROM salesman; 1.2. Write a SQL statement to display specific columns like name and commission for all the salesmen. SELECT name, commission FROM salesman; 1.3. Write a query to display the columns in a specific order like order date, salesman id, order number and purchase amount from for all the orders. SELECT ord_date, salesman_id, ord_no, purch_amt FROM orders; 1.4. Write a query which will retrieve the value of salesman id of all salesmen, getting orders from the customers in orders table without any repeats. SELECT DISTINCT salesman_id FROM orders; 1.5. Write a SQL statement to display names and city of salesman, who belongs to the city of Paris. SELECT name, city FROM salesman WHERE city = \u0026#39;Paris\u0026#39;; 1.6. Write a SQL statement to display all the information for those customers with a grade of 200. SELECT * FROM Customer WHERE grade=200; 1.7. Write a SQL query to display the order number followed by order date and the purchase amount for each order which will be delivered by the salesman who is holding the ID 5001. SELECT ord_date, ord_no, purch_amt FROM orders WHERE salesman_id=5001; 1.8. Write a SQL query to display the Nobel prizes for 1970. SELECT * FROM nobel_win WHERE YEAR=1970; -- different interpretation of instruction SELECT year,subject,winner FROM nobel_win WHERE year=1970; 1.9. Write a SQL query to know the winner of the 1971 prize for Literature. SELECT winner, country FROM nobel_win WHERE year=1971 AND subject=\u0026#39;Literature\u0026#39;; 1.10. Write a SQL query to display the year and subject that won \u0026lsquo;Dennis Gabor\u0026rsquo; his prize. SELECT year, subject FROM nobel_win WHERE winner=\u0026#39;Dennis Gabor\u0026#39;; 1.11. Write a SQL query to give the name of the \u0026lsquo;Physics\u0026rsquo; winners since the year 1950. SELECT winner FROM nobel_win WHERE subject=\u0026#39;Physics\u0026#39; AND year\u0026gt;=1950; 1.12. Write a SQL query to Show all the details (year, subject, winner, country ) of the Chemistry prize winners between the year 1965 to 1975 inclusive. SELECT year, subject, winner, counter FROM nobel_win WHERE subect=\u0026#39;Chemistry\u0026#39; AND year\u0026gt;=1965 AND year\u0026lt;=1975; 1.13. Write a SQL query to show all details of the Prime Ministerial winners after 1972 of Menachem Begin and Yitzhak Rabin. SELECT * FROM nobel_win WHERE category=\u0026#39;Prime Minister\u0026#39; AND year \u0026gt; 1972; -- different interpretation of instruction SELECT * FROM nobel_win WHERE year \u0026gt;1972 AND winner IN (\u0026#39;Menachem Begin\u0026#39;, \u0026#39;Yitzhak Rabin\u0026#39;); 1.14. Write a SQL query to show all the details of the winners with first name Louis. SELECT * FROM nobel_win WHERE winner LIKE \u0026#39;Louse %\u0026#39;; 1.15. Write a SQL query to show all the winners in Physics for 1970 together with the winner of Economics for 1971. SELECT * FROM nobel_win WHERE subject=\u0026#39;Physics\u0026#39; AND year=1970 UNION SELECT * FROM nobel_win WHERE subject=\u0026#39;Economics\u0026#39; AND year=1971; 1.16. Write a SQL query to show all the winners of nobel prize in the year 1970 except the subject Physiology and Economics. SELECT * FROM nobel_win WHERE year=1970 AND subject NOT IN (\u0026#39;Physiology\u0026#39;, \u0026#39;Economics\u0026#39;); 1.17. Write a SQL query to show the winners of a \u0026lsquo;Physiology\u0026rsquo; prize in an early year before 1971 together with winners of a \u0026lsquo;Peace\u0026rsquo; prize in a later year on and after the 1974. SELECT * FROM nobel_win WHERE subject=\u0026#39;Pysiology\u0026#39; AND year\u0026lt;1971 UNION SELECT * FROM nobel_win WHERE subject=\u0026#39;Peace\u0026#39; AND year\u0026gt;1974; 1.18. Write a SQL query to find all details of the prize won by Johannes Georg Bednorz. SELECT * FROM nobel_win WHERE winner=\u0026#39;Johannes Georg Bednorz; 1.19. Write a SQL query to find all the details of the nobel winners for the subject not started with the letter \u0026lsquo;P\u0026rsquo; and arranged the list as the most recent comes first, then by name in order. SELECT * FROM nobel_win WHERE subject NOT LIKE \u0026#39;P%\u0026#39; ORDER BY year DESC, winner; 1.20. Write a SQL query to find all the details of 1970 winners by the ordered to subject and winner name; but the list contain the subject Economics and Chemistry at last. SELECT * FROM nobel_win WHERE year=1970 ORDER BY CASE WHEN subject IN (\u0026#39;Economics\u0026#39;, \u0026#39;Chemistry\u0026#39;) THEN 1 ELSE 0 END ASC, subject, winnerl; 1.21. Write a SQL query to find all the products with a price between Rs.200 and Rs.600. SELECT * FROM item_mast WHERE pro_price\u0026gt;=200 AND pro_price\u0026lt;=600; -- better approach SELECT * FROM item_mast WHERE pro_price BETWEEN 200 AND 600; 1.22. Write a SQL query to calculate the average price of all products of the manufacturer which code is 16. SELECT AVG(pro_price) FROM item_mast WHERE pro_com=16; 1.23. Write a SQL query to find the item name and price in Rs. SELECT pro_name AS \u0026#39;item name\u0026#39;, pro_price AS \u0026#39;price in Rs\u0026#39; FROM item_mast; 1.24. Write a SQL query to display the name and price of all the items with a price is equal or more than Rs.250, and the list contain the larger price first and then by name in ascending order. SELECT pro_name, pro_price FOM item_mast WHERE pro_price\u0026gt;=250 ORDER BY pro_price desc, pro_name; 1.25. Write a SQL query to display the average price of the items for each company, showing only the company code. SELECT AVG(pro_price), pro_com FROM item_mast GROUP BY pro_com; 1.26. Write a SQL query to find the name and price of the cheapest item(s). SELECT pro_name, pro_price FROM item_mast WHERE pro_price = (SELECT MIN(pro_price) from item_mast); 1.27. Write a query in SQL to find the last name of all employees, without duplicates. SELECT DISTINCT emp_lname FROM emp_details; 1.28. Write a query in SQL to find the data of employees whose last name is \u0026lsquo;Snares\u0026rsquo;. SELECT * FROM emp_details WHERE emp_lname=\u0026#39;Snares\u0026#39;; 1.29. Write a query in SQL to display all the data of employees that work in the department 57. SELECT * FROM emp_details WHERE emp_dept=57; 2. Boolean \u0026amp; Relational Operators 2.1. Write a query to display all customers with a grade above 100. SELECT * FROM customer WHERE grade \u0026gt; 100; 2.2. Write a query statement to display all customers in New York who have a grade value above 100. SELECT * FROM customer WHERE city = \u0026#39;New York\u0026#39; AND grade \u0026gt; 100; 2.3. Write a SQL statement to display all customers, who are either belongs to the city New York or had a grade above 100. SELECT * FROM customers WHERE city = \u0026#39;New York\u0026#39; OR grade \u0026gt; 100; 2.4. Write a SQL statement to display all the customers, who are either belongs to the city New York or not had a grade above 100. SELECT * FROM customer WHERE city = \u0026#39;New York\u0026#39; OR grade \u0026lt;= 100; -- other approach SELECT * FROM customer WHERE city = \u0026#39;New York\u0026#39; OR NOT grade \u0026gt; 100; 2.5. Write a SQL query to display those customers who are neither belongs to the city New York nor grade value is more than 100. SELECT * FROM customer WHERE city NOT \u0026#39;New York\u0026#39; AND NOT grade \u0026gt; 100; -- other approach SELECT * FROM customer WHERE NOT (city = \u0026#39;New York\u0026#39; AND grade \u0026gt; 100); 2.6. Write a SQL statement to display either those orders which are not issued on date 2012-09-10 and issued by the salesman whose ID is 5005 and below or those orders which purchase amount is 1000.00 and below. SELECT * FROM orders WHERE NOT ( (ord_date = \u0026#39;2012-09-10\u0026#39; AND salesman_id \u0026gt; 5005) OR purch_amt \u0026gt; 1000.00 ); 2.7. Write a SQL statement to display salesman_id, name, city and commission who gets the commission within the range more than 0.10% and less than 0.12%. SELECT salesman_id, name, city, commission FROM salesman WHERE commission \u0026gt; 0.10 AND commission \u0026lt; 0.12; 2.8. Write a SQL query to display all orders where purchase amount less than 200 or exclude those orders which order date is on or greater than 10th Feb,2012 and customer id is below 3009. SELECT * FROM orders WHERE ( purch_amt \u0026lt; 200 OR NOT(ord_date \u0026gt;= \u0026#39;2012-02-10\u0026#39; AND customer_id \u0026lt; 3009) ); 2.9. Write a SQL statement to exclude the rows which satisfy 1) order dates are 2012-08-17 and purchase amount is below 1000 2) customer id is greater than 3005 and purchase amount is below 1000. SELECT * FROM orders WHERE NOT ( (ord_date = \u0026#39;2012-08-17\u0026#39; OR customer_id \u0026gt; 3005 ) AND purch_amt \u0026lt; 1000 ); 2.10. Write a SQL query to display order number, purchase amount, achieved, the unachieved percentage for those order which exceeds the 50% of the target value of 6000. SELECT ord_no, purch_amt, (purch_amt / 6000 * 100) AS \u0026#34;Achieved%\u0026#34;, ((6000-purch_amt) / 6000 * 100) AS \u0026#34;Unachieved%\u0026#34; FROM orders WHERE (purch_amt / 6000 * 100) \u0026gt; 50; 2.11. Write a query in SQL to find the data of employees whose last name is Dosni or Mardy. SELECT * FROM emp_details WHERE emp_lname = \u0026#39;Dosni\u0026#39; OR emp_lname = \u0026#39;Mardy\u0026#39;; 2.12. Write a query in SQL to display all the data of employees that work in department 47 or department 63. SELECT * FROM emp_details WHERE emp_dept = 47 OR emp_dept = 63; ","permalink":"https://lucaseo.github.io/posts/2021-02-28-sql-practice-select/","summary":"SQL Practices - SELECT Practices set resources are referenced from w3resource.com\n SQL Exercises, Practice, Solution - Retrieve data from tables SQL Exercises, Practice, Solution - Using Boolean and Relational operators All practice set and answers are written in PostgreSQL   Sample Tables salesman    salesman_id name city commission     5001 James Hoog New York 0.15   5002 Nail Knite Paris 0.13   5005 Pit Alex London 0.","title":"[EN] SQL Practice : select"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 해쉬 테이블 (Hash Table) 1.1. 해쉬 테이블의 구조   키(Key)에 데이터(Value)가 매핑되어 저장되어 있는 구조\n Key를 통해 데이터를 바로 받아올 수 있으므로, 속도가 빠름 파이썬에서는 딕셔너리(Dictionary)가 해쉬 테이블의 예시.  dict = {\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}   1.2. 해쉬 테이블의 용어  해쉬(Hash)  임의의 값을 고정된 길이로 변환하는 것   해쉬 테이블(Hash Table)  Key값의 연산에 의해 직접 접근이 가능한 데이터 구조   해싱 함수(Hashing Function)  Key에 대해 특정 산술 연산을 이용하여 데이터의 위치(해쉬 주소)가 리턴되는 함수   해쉬 값(Hash Value) 또는 해쉬 주소(Hash Address)  Key를 해싱 함수로 연산하여 얻는 값 Key를 해싱 함수로 연산하여 해쉬 값이 데이터의 위치.   슬롯(Slot)  한 개의 데이터를 저장할 수 있는 공간    1.3. 해시테이블의 장단점  장점  데이터 저장/읽기 속도가 빠름 특히 검색 속도가 빠름 Key에 대한 데이터가 있는지 확인이 쉬움. 중복 처리 및 확인이 쉬움   단점  일반적으로 저장공간이 많이 요구됨. 공간효윬어이 떨어짐.  해시함수에 따른 값이 저장될 공간이 확보되어야 하기 때문   해싱 함수로 인해 연산된 해시값/해시주소가 동일한 경우 충돌을 해결해야 함.  해시함수에 대한 의존도가 높음. 따라서 별도의 자료구조가 요구됨.      1.4. 해시테이블의 용도  검색이 많이 필요한 경우 저장, 삭제, 읽기가 빈번한 경우 캐쉬 구현  이미 데이터가 캐시에 있는지 없는지 중복확인을 할 때 해시테이블이 용이하게 적용됨.    \u0026amp;nbsp\n2. 파이썬을 통한 해쉬함수 이해 2.1. Hash Table 2.1.1. Slot hash_table = list([i for i in range(10)]) hash_table [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 2.1.2. Hash Function  해쉬 함수에는 다양한 기법들이 있음. Division 방식은 가장 기본적인 형태로 알려져 있음.  특정 값으로 나눈 후 나머지 값을 이용하는 기법.    def hash_func(key): return key % 5 2.1.3. 데이터 준비 data1 = \u0026#39;Andy\u0026#39; data2 = \u0026#39;Dave\u0026#39; data3 = \u0026#39;Trump\u0026#39; data4 = \u0026#39;Anthor\u0026#39; ## ord(): 문자의 ASCII(아스키)코드 리턴 print (ord(data1[0]), ord(data2[0]), ord(data3[0])) print (ord(data1[0]), hash_func(ord(data1[0]))) print (ord(data1[0]), ord(data4[0])) 65 68 84 65 0 65 65 2.1.4. 데이터 저장 def storage_data(data, value): index_key = ord(data[0]) hash_address = hash_func(index_key) hash_table[hash_address] = value storage_data(\u0026#39;Andy\u0026#39;, \u0026#39;01055553333\u0026#39;) storage_data(\u0026#39;Dave\u0026#39;, \u0026#39;01044443333\u0026#39;) storage_data(\u0026#39;Trump\u0026#39;, \u0026#39;01022223333\u0026#39;) print(hash_table) ['01055553333', 1, 2, '01044443333', '01022223333', 5, 6, 7, 8, 9] 2.1.4. 데이터 읽기 def get_data(data): key = ord(data[0]) hash_address = hash_func(key) return hash_table[hash_address] get_data(\u0026#39;Andy\u0026#39;) '01055553333' \u0026amp;nbsp\n3. 파이썬 예시  해시함수를 다르게 설정하고 일괄적으로 저장, 추출까지 설정했을 때.  해시함수 : key % 8    hash_table = list([0 for i in range(10)]) def hash_function(key): return key \u0026amp; 8 def get_key(data): return hash(data) def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) hash_table[hash_address] = value def read_data(data): hash_address = hash_function(get_key(data)) return hash_table[hash_address] # 데이터 저장 save_data(\u0026#39;Dave\u0026#39;, \u0026#39;01020302000\u0026#39;) save_data(\u0026#39;Andy\u0026#39;, \u0026#39;01033232200\u0026#39;) print(hash_table) ['01033232200', 0, 0, 0, 0, 0, 0, 0, 0, 0] # 데이터 읽기 read_data(\u0026#39;Dave\u0026#39;) '01033232200' \u0026amp;nbsp\n4. 해시 충돌(Hash Collision) 처리를 위한 문제 접근 방법 4.1. Separate Chaining 방식  해시 테이블 저장공간 이외의 공간을 활용함 충돌이 일어났을 때, 데이터를 뒤에 추가로 저장.  이 때, 다양한 자료구조를 활용하며 Linked List가 하나의 예가 될 수 있음. (Separate chaining with linked list)    4.1.1. Separate chaining with Linked List  데이터 저장 시, 동일한 hash_address가 존재하여 충돌이 발생하면, Linked list에 노드를 추가하여 값을 추가함. (파이썬으로는 일반 리스트로 구현함) 데이터 추출 시, hash_address에 대하여 선형 탐색하며, 해당 key에 대한 데이터를 검색 후 결과를 리턴함.  def get_key(data): return hash(data) def hash_function(key): return key % 8 def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) # 이미 Hash table의 공간이 차있어 충돌이 발생할 경우 if hash_table[hash_address] != 0: for index in range(len(hash_table[hash_address])): if hash_table[hash_address][index][0] == index_key: hash_table[hash_address][index][1] == value return None hash_table[hash_address].append([index_key, value]) # 새로 추가 else: hash_table[hash_address] = [[index_key, value]] def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(len(hash_table[hash_address])): if hash_table[hash_address][index][0] == index_key: return hash_table[hash_address][index][1] # value return return None else: return None return hash_table[hash_address] hash_table = list([0 for i in range(10)]) # pring hashed index_key for each data print(hash_function(get_key(\u0026#39;Dash\u0026#39;))) print(hash_function(get_key(\u0026#39;Donald\u0026#39;))) print(hash_function(get_key(\u0026#39;Dave\u0026#39;))) print(hash_function(get_key(\u0026#39;David\u0026#39;))) print(hash_function(get_key(\u0026#39;Dwayne\u0026#39;))) print(hash_function(get_key(\u0026#39;Dusan\u0026#39;))) print(hash_function(get_key(\u0026#39;Don\u0026#39;))) print(hash_function(get_key(\u0026#39;Dean\u0026#39;))) print(hash_function(get_key(\u0026#39;Dingo\u0026#39;))) print(hash_function(get_key(\u0026#39;Johnson\u0026#39;))) 7 6 5 3 2 4 7 6 5 0 # index_key = 1 save_data(\u0026#39;Dash\u0026#39;, \u0026#39;1111111111\u0026#39;) save_data(\u0026#39;Donald\u0026#39;, \u0026#39;2222222222\u0026#39;) # index_key = 3 save_data(\u0026#39;Dave\u0026#39;, \u0026#39;3333333333\u0026#39;) # index_key = 4 save_data(\u0026#39;David\u0026#39;, \u0026#39;4444444444\u0026#39;) save_data(\u0026#39;Dwayne\u0026#39;, \u0026#39;5555555555\u0026#39;) save_data(\u0026#39;Dusan\u0026#39;, \u0026#39;6666666666\u0026#39;) # index_key = 5 save_data(\u0026#39;Don\u0026#39;, \u0026#39;7777777777\u0026#39;) # index_key = 6 save_data(\u0026#39;Dean\u0026#39;, \u0026#39;8888888888\u0026#39;) save_data(\u0026#39;Dingo\u0026#39;, \u0026#39;9999999999\u0026#39;) # index_key = 7 save_data(\u0026#39;Johnson\u0026#39;, \u0026#39;0000000000\u0026#39;) print(hash_table) [[[-2635316466179689368, '0000000000']], 0, [[6215479457786385290, '5555555555']], [[-7904014224011995085, '4444444444']], [[1661792002016286988, '6666666666']], [[-7424204428908836315, '3333333333'], [2760365508324363629, '9999999999']], [[-6894406110985197394, '2222222222'], [-7882665379891136098, '8888888888']], [[-8106021915874705937, '1111111111'], [6082400908374278295, '7777777777']], 0, 0] read_data(\u0026#39;Dusan\u0026#39;) '6666666666' read_data(\u0026#34;Dance\u0026#34;) # 데이터 존재 하지 않음 4.2. Open Addressing 방식  추가 메모리 공간을 사용하지 않고, 해시 테이블의 빈 공간을 사용하는 방법.  Separate chainging에 비해 메모리를 덜 사용함.    4.2.1. Linear probing  충돌이 발생할 시, 해당 hash_address의 다음 hash_address부터 가장 먼저 등장하는 빈 공간에 저장하는 기법 장점  저장공간 활용도를 높일 수 있음. 저장 시 별도의 별도의 공간이나 추가 작업이 필요 없음.   단점  해시 함수의 퍼포먼스에 따라 해시테이블의 성능이 결정됨. 대신 빈 공간을 미리 확보하기 위해 해시 테이블 저장공간을 다시 확대하거나 미리 마련이 되어 있어야 함..    def get_key(data): return hash(data) def hash_function(key): return key % 8 def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) # 이미 Hash table의 공간이 차있어 충돌이 발생할 경우 if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] return elif hash_table[index][0] == index_key: hash_table[index][1] = value return else: hash_table[hash_address] = [index_key, value] def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] else: None hash_table = list([0 for i in range(12)]) # index_key = 1 save_data(\u0026#39;Dash\u0026#39;, \u0026#39;1111111111\u0026#39;) save_data(\u0026#39;Donald\u0026#39;, \u0026#39;2222222222\u0026#39;) # index_key = 3 save_data(\u0026#39;Dave\u0026#39;, \u0026#39;3333333333\u0026#39;) # index_key = 4 save_data(\u0026#39;David\u0026#39;, \u0026#39;4444444444\u0026#39;) save_data(\u0026#39;Dwayne\u0026#39;, \u0026#39;5555555555\u0026#39;) save_data(\u0026#39;Dusan\u0026#39;, \u0026#39;6666666666\u0026#39;) # index_key = 5 save_data(\u0026#39;Don\u0026#39;, \u0026#39;7777777777\u0026#39;) # index_key = 6 save_data(\u0026#39;Dean\u0026#39;, \u0026#39;8888888888\u0026#39;) save_data(\u0026#39;Dingo\u0026#39;, \u0026#39;9999999999\u0026#39;) # index_key = 7 save_data(\u0026#39;Johnson\u0026#39;, \u0026#39;0000000000\u0026#39;) print(hash_table) [[-2635316466179689368, '0000000000'], 0, [6215479457786385290, '5555555555'], [-7904014224011995085, '4444444444'], [1661792002016286988, '6666666666'], [-7424204428908836315, '3333333333'], [-6894406110985197394, '2222222222'], [-8106021915874705937, '1111111111'], [6082400908374278295, '7777777777'], [-7882665379891136098, '8888888888'], [2760365508324363629, '9999999999'], 0] \u0026amp;nbsp\n5. Hash 함수와 Key 생성 5.1. 대표적인 해시 함수들  SHA (Secure Hash Algorithm)  어떠한 데이터도 고정된 크기의 unique한 값으로 리턴하므로, 해시 함수로 유용하게 활용 가능 해시함수들의 모음이기에, 여러가지 함수를 선택할 수 있음.    import hashlib # SHA-1를 사용한 예시 data = \u0026#39;David\u0026#39;.encode() hash_object = hashlib.sha1() hash_object.update(data) hash_address = hash_object.hexdigest() print(hash_address) d27937f914ebe99ee315f04449678eccfb658191 # SHA-256을 사용한 예시 data = \u0026#39;David\u0026#39;.encode() hash_object = hashlib.sha256() hash_object.update(data) hash_address = hash_object.hexdigest() print(hash_address) a6b54c20a7b96eeac1a911e6da3124a560fe6dc042ebf270e3676e7095b95652 5.2. SHA-256 알고리즘을 사용한 Linear Probing 방식구현 import hashlib def get_key(data): hash_object = hashlib.sha256() hash_object.update(data.encode()) hash_address = hash_object.hexdigest() return int(hash_address, 16) def hash_function(key): return key % 8 def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) # 이미 Hash table의 공간이 차있어 충돌이 발생할 경우 if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] return elif hash_table[index][0] == index_key: hash_table[index][1] = value return else: hash_table[hash_address] = [index_key, value] def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] else: None hash_table = list([0 for i in range(12)]) print(hash_function(get_key(\u0026#39;Dash\u0026#39;))) print(hash_function(get_key(\u0026#39;Donald\u0026#39;))) print(hash_function(get_key(\u0026#39;Dave\u0026#39;))) print(hash_function(get_key(\u0026#39;David\u0026#39;))) print(hash_function(get_key(\u0026#39;Dwayne\u0026#39;))) print(hash_function(get_key(\u0026#39;Dusan\u0026#39;))) print(hash_function(get_key(\u0026#39;Don\u0026#39;))) print(hash_function(get_key(\u0026#39;Dean\u0026#39;))) print(hash_function(get_key(\u0026#39;Dingo\u0026#39;))) print(hash_function(get_key(\u0026#39;Johnson\u0026#39;))) 3 6 0 2 2 5 5 4 6 6 save_data(\u0026#39;Dash\u0026#39;, \u0026#39;1111111111\u0026#39;) save_data(\u0026#39;Donald\u0026#39;, \u0026#39;2222222222\u0026#39;) save_data(\u0026#39;Dave\u0026#39;, \u0026#39;3333333333\u0026#39;) save_data(\u0026#39;David\u0026#39;, \u0026#39;4444444444\u0026#39;) save_data(\u0026#39;Dwayne\u0026#39;, \u0026#39;5555555555\u0026#39;) save_data(\u0026#39;Dusan\u0026#39;, \u0026#39;6666666666\u0026#39;) save_data(\u0026#39;Don\u0026#39;, \u0026#39;7777777777\u0026#39;) save_data(\u0026#39;Dean\u0026#39;, \u0026#39;8888888888\u0026#39;) save_data(\u0026#39;Dingo\u0026#39;, \u0026#39;9999999999\u0026#39;) save_data(\u0026#39;Johnson\u0026#39;, \u0026#39;0000000000\u0026#39;) print(hash_table) [[58168926492874022204843410240616221587430711422315320988033179720499944676464, '3333333333'], 0, [75404257596651192996495076349601554552549513252973852817536161452854420788818, '4444444444'], [63434467723890717949172920093925024550717963975746208715791640357658818776859, '1111111111'], [103158016914344531977983463060013032302915828748947913551605310269665217945786, '5555555555'], [90558914996105951668787733552590627218772546758158603367772415150980389476661, '6666666666'], [40513459897764969709188365008375736156728765495033312981181177193702355922238, '2222222222'], [16606146580844896176716406780736496581454102609573324990177790343105877227493, '7777777777'], [88623518743408414412271740834380503561141448764593279404613947210397492361580, '8888888888'], [22241017530888154973558349121945220497843199841401728659273049527650898379222, '9999999999'], [21745812297715092507978491799105903853662369235937786557584049993744107100774, '0000000000'], 0] read_data(\u0026#39;Dingo\u0026#39;) '9999999999' \u0026amp;nbsp\n6. 시간 복잡도  저장 (insertion), 삭제 (deletion), 검색(search)  Collision이 없는 경우: O(1) Collision이 모두 발생하는 최악의 경우: O(n)     해쉬 테이블의 경우, 일반적인 경우를 기대하고 만들기 때문에, 시간 복잡도는 O(1) 이라고 말할 수 있음\n \u0026amp;nbsp\n7. Reference  Fastcampus 알고리즘 / 기술면접 강의 Hash Table Wikipedia  ","permalink":"https://lucaseo.github.io/posts/2021-02-19-python-datastructure-hash-table/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 해쉬 테이블 (Hash Table) 1.1. 해쉬 테이블의 구조   키(Key)에 데이터(Value)가 매핑되어 저장되어 있는 구조\n Key를 통해 데이터를 바로 받아올 수 있으므로, 속도가 빠름 파이썬에서는 딕셔너리(Dictionary)가 해쉬 테이블의 예시.  dict = {\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}   1.2. 해쉬 테이블의 용어  해쉬(Hash)  임의의 값을 고정된 길이로 변환하는 것   해쉬 테이블(Hash Table)  Key값의 연산에 의해 직접 접근이 가능한 데이터 구조   해싱 함수(Hashing Function)  Key에 대해 특정 산술 연산을 이용하여 데이터의 위치(해쉬 주소)가 리턴되는 함수   해쉬 값(Hash Value) 또는 해쉬 주소(Hash Address)  Key를 해싱 함수로 연산하여 얻는 값 Key를 해싱 함수로 연산하여 해쉬 값이 데이터의 위치.","title":"[KR] 자료구조 \u0026 알고리즘 : 해시 테이블(Hash Table)"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.1.3. 알고리즘 시간 복잡도의 주요 요소  반복문이 얼마나 시행되었는지에 따라 시간 복잡도의 성능이 결정된다고 할 수 있음. 입력의 크기가 커지면 커질 수록 반복문이 알고리즘 수행 시간을 지배함.  1.2. 복잡도 표기법 유형 1.2.1. Big-O 표기법 \\(O(N) \\)  알고리즘 최악의 실행시간을 표기함 아무리 최악의 상황이라도 이 정도의 성능은 보장한다는 의미 가장 많이(일반적으로) 사용함  1.2.2. 오메가 표기법 \\(\\Omega(N) \\)  최상의 알고리즘 실행 시간을 표기  1.2.3. 세타 표기법 \\(\\Theta(N) \\)  알고리즘 평균 실행 시간을 표기  1.3. Big-O 표기법 1.3.1. \\( O(n) \\)  입력 \\(n \\)에 따라 결정되는 시간 복잡도 함수.  \\(O(1) \\) \\(O(\\log n) \\) \\(O(n) \\) \\(O(n \\log n) \\) \\(O(n^2) \\) \\(O(2^n) \\) \\(O(n!) \\)   입력에 따라 기하급수적으로 시간 복잡도가 늘어날 수 있음.  \\(O(1) \u0026lt; O(\\log n) \u0026lt; O(n) \u0026lt; O(n \\log n) \u0026lt; O(n^2) \u0026lt; O(2^n) \u0026lt; O(n!) \\)    1.3.2. 계산법  \\(O(1) \\)  단순하게 입력 \\(n \\)에 따라 멸번 실행이 되는지 계산함. 실행은 무조건 2회(또는 상수회) 실행한다.  if n \u0026gt; 10: print(n)  \\(O(n) \\)  \\(n \\)에 따라 \\(n \\)번 또는 \\(k \\cdot n + b \\) 실행한다.  for idx in range(n): print(idx)  \\(O(n^2) \\)  \\(n \\)에 따라 \\(n^2 \\)번 또는 \\(k \\cdot n^2 + b \\) 등을 실행한다.  for num in range(n): for index in range(n): print(index)   1.3.3. 표기 방법  시간복잡도는 결국 입력값 \\(n \\)에 따라 성능이 결정됨. 결국 알고리즘 성능에 가장 영향을 끼치는 값을 기준으로 표기함. 따라서 상수 \\(k, b \\)는 표기할 때 생략함 \\(k \\cdot n^2 + b \\)의 경우 Big-O 표기법으로는 \\(O(n^2) \\)으로 표기함.  2. Reference  Fastcampus 알고리즘 / 기술면접 강의  ","permalink":"https://lucaseo.github.io/posts/2021-02-10-complexity/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.","title":"[KR] 알고리즘 복잡도"},{"content":"Hello World 스타트업에서 기업의 ESG 데이터를 기반으로 하는 ESG 사건사고 분석 서비스와 지속가능 여신 및 신용평가 (ESG CB) 를 개발하는 일을 하는 데이터사이언티스트 겸 엔지니어입니다.\n관심 분야는 MLOps, 데이터 파이프라인, 프로토타입 어플리케이션 개발, 데이터 시각화, 대시보드 구축, 딥러닝 모델 등 등등 중구난방이지만 호기심이 많습니다. (원래 시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\n\u0026amp;nbsp\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다. 평점은 왓챠에서 매기고, 넷플릭스, 왓챠, 디즈니 플러스, 쿠팡플레이를 구독 중입니다. 테니스에 미쳐 살고 있습니다. 흑인음악에 관심이 많고 웹매거진 플랫폼인 HIPHOPLE.com에서 스태프로 일했었습니다. (요즘은 뜸한 상태) 스페인어를 아주 조금 할 줄 압니다. 독일어를 배우고 싶습니다. 언젠가는 유럽에 정착하고 싶습니다. 미디(MIDI)를 아주 조금 다룰 줄 압니다. 유투브를 통해 경험하지 못한 새로운 세상을 접하는 걸 정말 좋아합니다. (그리고 다수의 치즈냥과 보더콜리, 리트리버, 웰시코기 채널을 구독하고 있습니다)  ","permalink":"https://lucaseo.github.io/about/","summary":"Hello World 스타트업에서 기업의 ESG 데이터를 기반으로 하는 ESG 사건사고 분석 서비스와 지속가능 여신 및 신용평가 (ESG CB) 를 개발하는 일을 하는 데이터사이언티스트 겸 엔지니어입니다.\n관심 분야는 MLOps, 데이터 파이프라인, 프로토타입 어플리케이션 개발, 데이터 시각화, 대시보드 구축, 딥러닝 모델 등 등등 중구난방이지만 호기심이 많습니다. (원래 시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\n\u0026amp;nbsp\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다. 평점은 왓챠에서 매기고, 넷플릭스, 왓챠, 디즈니 플러스, 쿠팡플레이를 구독 중입니다.","title":"About"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 링크드 리스트 : Linked List 의 개념 1.1. 링크드 리스트의 구조  데이터와 데이터 사이를 화살표로 연결하여 관리하는 데이터 구조. 배열(Array)와의 차이점  1.2. 링크드 리스트와 배열(Array)와 차이점  배열 : 번호가 붙여진(인덱싱이 된) 칸에 원소들을 채워 넣어 관리함. 링크드 리스트: 각 원소들을 줄줄이 엮어서 관리함.  1.2.1. 배열과 링크드 리스트의 비유적 비교 (출처: 생활코딩)  메모리라는 개념을 우리는 건물에 비유할 수 있을 것 같습니다. 아래 예시는 배열을 사용하는 것과 linked list를 사용하는 것을 비유해서 보여주고 있습니다. 여러분의 회사가 한 건물의 일부를 임대해서 사용한다고 생각해주세요.\n    Source https://opentutorials.org/module/1335/8821    Array list의 첫 번째 회사는 모든 직원이 한곳에 모여있어야 한다는 철학이 있기 때문에 사무실이 모여있습니다. 배열은 건물을 이런 식으로 사용하는 것과 비슷합니다. 만약 회사가 성장해서 사무실이 좁아지면 더 이상 새로운 직원을 뽑을 수 없습니다. 붙어있는 공간이 없기 때문이죠. 만약 더 많은 공간이 필요하다면 더 많은 사람을 수용할 수 있는 공간을 찾아서 전체가 이사해야 합니다. Array list는 엘리먼트가 같은 곳에 모여있습니다. 만약에 3번째 자리로 가고 싶다면 한번에 3번째 방으로 갈 수 있습니다. 찾고자 하는 사무실이 몇 번째에 있는지 알고 있다면 Array list는 매우 빠릅니다.\n    Source https://opentutorials.org/module/1335/8821    Linked List의 두 번째 회사는 한 건물 내에서 한 회사가 임대한 사무실이 서로 떨어져 있습니다. 덕분에 직원이 늘어도 큰 걱정이 없습니다. 건물에서 비어있는 곳 아무데나 임대해서 들어가면 되니까요. 그런데 방문자가 사무실을 찾는 방법이 좀 비효율적입니다. 위의 그림에 있는 방문자가 3번째 사무실을 찾아가려면 우선 첫 번째 화살표의 사무실을 찾아가야 합니다. 이 사무실의 직원에게 다음 사무실이 어딘지 물어봅니다. 그럼 알려주는 사무실로 이동 한 후에 다시 물어봐서 그다음 사무실로 이동합니다. 이렇게 물어물어 사무실을 찾아가야 하는 방식이 Linked List입니다. 그래서 Linked List에서는 몇 번째 엘리먼트를 찾는 것이 느립니다.\n 1.3. 링크드 리스트 관련 용어  노드(node):  데이터가 저장되는 단위. [데이터값, 포인터]로 구성   포인터(pointer):  다음 데이터의 주소를 담고 있는 공간. 노드에서 다음 또는 이전 노드와의 연결 정보를 가지고 있는 공간.    1.4. 링크드 리스트의 장단점 장점:\n 데이터 공간을 미리 할당하지 않아도 됨. (array는 미리 데이터 공간을 할당해야 함.) 삽입과 삭제가 빠름. 따라서 삽입/삭제가 빈번히 일어날 때 많이 사용됨.  단점:\n 데이터 구조 표현에 소요되는 저장공간이 비교적 큼.  연결을 위한 별도의 데이터공간이 필요하기 때문에 저장공간 효율이 높지 않음.   데이터를 찾는 시간이 오래 걸림.  인덱싱이 된 배열와는 달리, 특정 N번째 원소에 접근하려면 링크드 리스트의 처음부터 순차적으로 원소를 훑으며 N번째 원소를 찾아가야 함.   중간에 위치한 데이터 삭제 시, 앞뒤 데이터의 연결을 다시 구현해야하는 부가적인 작업이 필요함.  2. 파이썬에서의 링크드 리스트 pt.1  링크드 리스트는 C언어에서 주요한 데이터구조이지만, 파이썬에서는 리스트 타입이 링크드 리스트의 기능을 모두 지원함.  2.1. 노드 구현하기 # 포인트가 없는 노드를 구현 class Node: def __init__(self, data): self.data = data self.next = None # 포인터가 있는 노드를 구현하기 class Node: def __init__(self, data, next=None): self.data = data self.next = next # 포인터 \u0026amp;nbsp\n2.2. 포인터를 활용하여 노드와 노드를 연결하기 node1 = Node(1) node2 = Node(2) head = node1 # 링크드리스트의 첫 시작은 node1로 지정함. node1.next =node2 # node1의 포인터에 node2를 저장하여 연결함. print(head.data) # node1의 데이터 출력 print(node1.next.data) # node2의 데이터 출력 1 2 \u0026amp;nbsp\n2.3. 링크드 리스트로 데이터 추가하기 class Node: def __init__(self, data, next=None): self.data = data self.next = next def add(data): node = head while node.next: node = node.next node.next = Node(data) node1 = Node(1) # 첫 번째 노드 생성 head = node1 # 첫 번째 노드를 head로 지정 for idx in range(2, 10): # 추가할 데이터를 iteration하며 추가하기 add(idx) 2.4. 링크드 리스트 데이터 접근하여 출력하기 # 한개씩 접근하기 print(head.data) # 1번째 노드 print(head.next.data) # 2번째 노드 print(head.next.next.data) # 3번째 노드 print(head.next.next.next.data) # 4번째 노드 1 2 3 4 \u0026amp;nbsp\n# 한꺼번에 접근하기 node = head while node.next: # 노드의 데이터 순차적으로 출력 print(node.data) node = node.next print(node.data) # next가 없는 마지막 노드의 데이터 출력 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n2.5. 링크드 리스트의 중간 위치에 데이터 추가하기 2.5.1. 단순하게 추가해보기 # 기존의 데이터 출력하기 node = head while node.next: print(node.data) node = node.next print(node.data) 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\nnode3_5 = Node(3.5) # 2와 3 사이에 위치할 데이터(2.5)가 담긴 노드 생성 node = head search = True while search: if node.data == 3: # 새 데이터(3.5)의 직전 데이터가 되는 노드(3)를 찾음. search = False # search 종료 else: node = node.next node_next = node.next # 노드(3)의 다음 노드(4)를 따로 빼기 node.next = node3_5 # 노드(3.5)를 노드(3)의 다음 노드로 연결 node3_5.next = node_next # 노드(4)을 노드(3.5)의 다음 노드로 연결 node = head while node.next: print(node.data) node = node.next print(node.data) 1 2 3 3.5 4 5 6 7 8 9 \u0026amp;nbsp\n3. 파이썬에서의 링크드 리스트 pt.2 3.1. 객체지향 프로그램으로 구현해보기 class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next 링크드 리스트의 노드 모두 순회하며 데이터를 출력하기\nlinkedlist_1 = NodeManagement(0) linkedlist_1.print_all() 0 \u0026amp;nbsp\nfor data in range(1, 10): linkedlist_1.add(data) linkedlist_1.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n3.2. 특정 데이터를 가진 노드 검색하기  search_node() 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: # 노드의 데이터가 찾고자 하는 데이터가 맞다면 그대로 리턴 return node else: node = node.next # 다음 노드로 순회 \u0026amp;nbsp\nlinkedlist_2 = NodeManagement(0) for data in range(1, 10): linkedlist_2.add(data) linkedlist_1.print_all() linkedlist_2.search_node(4).data # 데이터가 4인 노드의 데이터 출력 0 1 2 3 4 5 6 7 8 9 4 \u0026amp;nbsp\n3.3. 특정 인덱스의 노드 검색하기  get_node 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node \u0026amp;nbsp\nlinkedlist_3 = NodeManagement(0) for data in range(1, 10): linkedlist_3.add(data) linkedlist_1.print_all() # 전체 노드 데이터 출력 linkedlist_3.get_node(3).data # 3번째 노드의 데이터 출력  0 1 2 3 4 5 6 7 8 9 3 \u0026amp;nbsp\n3.2. 특정 노드 삭제하기  delete ㅁㅔ서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node def delete(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 삭제 \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: print(\u0026#34;No node avaible\u0026#34;) return if self.head.data == data: # 삭제하고자 하는 데이터가 가장 첫 번째 노드일 때 temp = self.head self.head = self.head.next del temp else: node = self.head while node.next: if node.next.data == data: # 삭제하고자 하는 데이터가 다음 노드에 있을 때 temp = node.next node.next = node.next.next # 다음 노드를 건너 뛰고, 다음다음 노드를 바로 연결 del temp pass else: node = node.next 노드를 한개(헤드) 만들어서 delete 메서드 실험하기\nlinkedlist_4 = NodeManagement(0) linkedlist_4.print_all() 0 \u0026amp;nbsp\nlinkedlist_4.head # 헤드가 존재함 \u0026lt;__main__.Node at 0x107e94d10\u0026gt; \u0026amp;nbsp\nlinkedlist_4.delete(0) # 헤드를 삭제 linkedlist_4.head # 헤드 삭제 후에는 헤드가 출력되지 않음 linkedlist_4.print_all() # 링크드 리스트에 노드가 존재하지 않음 \u0026amp;nbsp\n여러 노드를 추가해서 delete 메서드 실험하기\nlinkedlist_5 = NodeManagement(0) for data in range(1, 10): linkedlist_5.add(data) linkedlist_5.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\nlinkedlist_5.delete(6) # 노드 한개를 삭제함 linkedlist_5.print_all() 0 1 2 3 4 5 7 8 9 \u0026amp;nbsp\nlinkedlist_5.delete(3) linkedlist_5.print_all() 0 1 2 4 5 7 8 9 \u0026amp;nbsp\n3.3. 새 노드 중간에 삽입하기  insert 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node def delete(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 삭제 \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: print(\u0026#34;No node avaible\u0026#34;) return if self.head.data == data: # 삭제하고자 하는 데이터가 가장 첫 번째 노드일 때 temp = self.head self.head = self.head.next del temp else: node = self.head while node.next: if node.next.data == data: # 삭제하고자 하는 데이터가 다음 노드에 있을 때 temp = node.next node.next = node.next.next # 다음 노드를 건너 뛰고, 다음다음 노드를 바로 연결 del temp pass else: node = node.next def insert(self, data, position_index): \u0026#34;\u0026#34;\u0026#34; 지정한 위치에 데이터 노드 삽입하기 \u0026#34;\u0026#34;\u0026#34; if position_index == 0: # head 위치에 삽입하기 temp = self.head self.head = Node(data) self.head.next = temp self.nodeCount += 1 elif position_index \u0026gt; self.nodeCount + 1: print(\u0026#34;Error: Insert index exceeds the length of linked list\u0026#34;) return else: node = self.head search_index = 0 search_status = True while search_status: search_index += 1 if search_index == position_index: search_status = False insert_node = Node(data) node_next = node.next node.next = insert_node insert_node.next = node_next self.nodeCount += 1 else: node = node.next linkedlist_6 = NodeManagement(0) for data in range(1, 10): linkedlist_6.add(data) linkedlist_6.print_all() # 노드 순회하며 데이터 출력하기 linkedlist_6.nodeCount # 노드 개수 출력하기 0 1 2 3 4 5 6 7 8 9 10 \u0026amp;nbsp\nlinkedlist_6.insert(17, 7) # 17이라는 데이터를 7번째 인덱스에 넣기 linkedlist_6.print_all() linkedlist_6.nodeCount # 노드 삽입 후 노드 개수 출력 0 1 2 3 4 5 6 17 7 8 9 11 \u0026amp;nbsp\n4. 링크드 리스트의 개선된 타입 4.1. 더블 링크드 리스트 : Doubly-Linked List  링크드 리스트의 단점을 보완하기 위해 등장함.  4.1.1. 기본 구조  양방향으로 연결되어 있어, 노드 탐색이 양쪽으로 모두 가능한 구조 즉, 다음 노드를 가리키는 포인터(next) 뿐만 아니라, 이전 노드를 가리키는 포인터(prev)도 존재함.  \u0026amp;nbsp\n4.1.2. 더블 링크드 리스트의 장단점 장점:\n 양방향으로 탐색이 가능함.  단점:\n 이전 노드를 가리키는 포인터가 추가되기 때문에 메모리 사용량이 늘어남. 삽입/삭제 연산에 있어, 앞/뒤 연결링크를 조정해줘야 하기 때문에 구조가 복잡해짐.  \u0026amp;nbsp\n4.1.2. 파이썬으로 구현한 더블 링크드 리스트  더블 링크드 리스트의 노드는 이전 노드와 다음 노드를 가리키는 포인터를 포함하는 것이 핵심.  class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next \u0026amp;nbsp\ndouble_linked_list = NodeManagement(0) for data in range(1, 10): double_linked_list.add(data) double_linked_list.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n4.2. 더블 링크드 리스트의 검색과 삽입 4.2.1. 특정 노드 이전에 새로운 노드 삽입하기 class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next def search_from_head(self, data): if self.head == None: return None node = self.head while node: if node.data == data: return node else: node = node.next return None def search_from_tail(self, data): if self.head == None: return None node = self.tail while node: if node.data == data: return node else: node = node.prev return None def insert_before(self, data, prev_data): if self.head == None: self.head = Node(data) return None else: node = self.tail while node.data != prev_data: node = node.prev if node == None: return None new_node = Node(data) prev_new = node.prev prev_new.next = new_node new_node.prev = prev_new new_node.next = node node.prev = new_node return None \u0026amp;nbsp\ndouble_linked_list = NodeManagement(0) for data in range(1, 10): double_linked_list.add(data) double_linked_list.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n# 뒤에서부터 찾기 node_3 = double_linked_list.search_from_tail(3) node_3.data 3 \u0026amp;nbsp\n# 앞에서부터 찾기 node_8 = double_linked_list.search_from_head(8) node_8.data 8 \u0026amp;nbsp\ndouble_linked_list.insert_before(4.5, 5) double_linked_list.print_all() 0 1 2 3 4 4.5 5 6 7 8 9 \u0026amp;nbsp\nnode_4_5 = double_linked_list.search_from_tail(4.5) node_4_5.data 4.5 \u0026amp;nbsp\n4.2.2. 특정 노드 이후에 새로운 노드 삽입하기 class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next def search_from_head(self, data): if self.head == None: return None node = self.head while node: if node.data == data: return node else: node = node.next return None def search_from_tail(self, data): if self.head == None: return None node = self.tail while node: if node.data == data: return node else: node = node.prev return None def insert_before(self, data, prev_data): if self.head == None: self.head = Node(data) return None else: node = self.tail while node.data != prev_data: node = node.prev if node == None: return None new_node = Node(data) prev_new = node.prev prev_new.next = new_node new_node.next = node return None def insert_after(self, data, next_data): if self.head == None: self.head = Node(data) return None else: node = self.head while node.data != next_data: node = node.next if node == None: return None new_node = Node(data) next_new = node.next new_node.next = next_new new_node.prev = node if new_node.next == None: self.tail = new_node else: node.next = new_node return True double_linked_list_2 = NodeManagement(0) for data in range(1, 10): double_linked_list_2.add(data) double_linked_list_2.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\ndouble_linked_list_2.insert_after(2.5, 2) True \u0026amp;nbsp\ndouble_linked_list_2.print_all() 0 1 2 2.5 3 4 5 6 7 8 9 5. 정리해보기  배열 리스트는 일정한 공간을 할당 후, 그 공간 안에서 물리적인 순서를 가지는 자료구조이다. 반대로 링크드 리스트(연결리스트)는 데이터를 담고 있는 노드들의 논리적인 순서를 위한 자료구조이다. 논리적인 구조는 이전/다음 노드를 가리키는 포인터를 통해 정의한다. 링크드 리스트는 처음부터 특정 공간을 할당하지 않아 메모리 활용도가 유연하고, 메모리 낭비를 막을 수 있다. 삽입과 삭제가 빠르다. 구현 작업은 다소 복잡할 지 모르나, 구현 후 실행은 간단하다. 하지만 포인터라는 요소까지 포함하여야 하기 때문에 비교적 더 큰 공간을 잡아먹는다. 또한 배열와는 달리, 순차적으로 노드를 훑으며 노드를 찾아가야 하기 때문에 시간이 좀 더 걸린다. 단순한 링크드 리스트는 한방향으로만 노드를 탐색하기 때문에 비효율적일 수 있다. 양방향 링크드 리스트는 앞/뒤에서부터 동시에 노드를 탐색하기 때문에 단순 링크드 리스트의 단점을 보완할 수 있다.  6. Reference  Fastcampus 알고리즘 / 기술면접 강의 프로그래머스 프로그램밍 강의 어서와! 자료구조는 처음이지? 생활코딩  ","permalink":"https://lucaseo.github.io/posts/2021-02-01-python-datastructure-linked-list/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 링크드 리스트 : Linked List 의 개념 1.1. 링크드 리스트의 구조  데이터와 데이터 사이를 화살표로 연결하여 관리하는 데이터 구조. 배열(Array)와의 차이점  1.2. 링크드 리스트와 배열(Array)와 차이점  배열 : 번호가 붙여진(인덱싱이 된) 칸에 원소들을 채워 넣어 관리함. 링크드 리스트: 각 원소들을 줄줄이 엮어서 관리함.  1.2.1. 배열과 링크드 리스트의 비유적 비교 (출처: 생활코딩)  메모리라는 개념을 우리는 건물에 비유할 수 있을 것 같습니다.","title":"[KR] 자료구조 \u0026 알고리즘 : 링크드 리스트(Linked List)"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 0. 자료구조? 알고리즘?  자료구조 Data Structure  대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조    \u0026amp;nbsp\n 체계적인 데이터 구조화의 필요성  코드 상에서 효율적인 데이터 처리하기 위함 어떤 데이터 구조를 사용하느냐에 따라 효율이 달라짐.    \u0026amp;nbsp\n 알고리즘이란  어떠한 문제를 풀기 위한 절차 / 방법 특정 문제에 해당하는  특정 입력을 넣으면 특정 출력을 얻을 수 있도록 하는 프로그래밍      \u0026amp;nbsp\n 문제를 푸는 방법은 각양각색이지만, 다음을 고려하여 계산을 하고 최적의 방법을 찾는다.  어느 정도의 시간을 쓰는가? 어느 정도의 저장 공간을 활용하는가?    \u0026amp;nbsp\n 자료구조와 알고리즘이 중요한 이유  어떤 자료구조와 알고리즘을 쓰느냐에 따라 성능 면에서 아주 큰 차이가 생김.     1. 배열 : Array  같은 종류의 데이터를 순차적으로 저장하는 형태의 데이터타입  1.1. 배열의 필요성  같은 종류의 데이터를 효율적으로 관리 같은 종류의 데이터를 순차적으로 데이터를 저장  1.2. 배열의 장단점 (파이썬이 아닌 C로 봤을 때)\n장점:\n 구현이 쉬움. 빠른 접근이 가능함.  인덱스index가 매겨지기에, 첫 데이터(인덱스 0)의 위치를 기준으로 상대적인 위치의 데이터에 빠르게 접근 가능 즉, 일단 만들어 놓으면 빠른 접근이 가능.   검색에 용이함.  \u0026amp;nbsp\n단점:\n 데이터 추가와 삭제가 어려움.  미리 최대 길이를 지정해야 하기 때문   데이터를 추가하거나 삭제를 하면 길이에 변화가 생김.  변수를 새로 만드는 수 밖에 없음   즉, 일단 만들어 놓으면 수정이 어렵고 메모리 재사용이 불가함.  1.3. 파이썬에서의 배열  파이썬에서는 리스트(list) 타입  # 1차원 배열 array_1d = [1, 2, 3, 4, 5] # 2차원 배열 array_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] print(array_1d) print(array_2d) [1, 2, 3, 4, 5] [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \u0026amp;nbsp\n### 연습(1) # 2차원 배열에서 9, 8, 7을 순서대로 출력해보기 array_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] print(array_2d[2][2]) print(array_2d[2][1]) print(array_2d[2][0]) 9 8 7 \u0026amp;nbsp\n### 연습(2) # 아래 데이터셋에서 전체 이름 안에서 M은 몇 번 나왔는지 빈도수 출력 dataset = [\u0026#39;Braund, Mr. Owen Harris\u0026#39;, \u0026#39;Cumings, Mrs. John Bradley (Florence Briggs Thayer)\u0026#39;, \u0026#39;Heikkinen, Miss. Laina\u0026#39;, \u0026#39;Futrelle, Mrs. Jacques Heath (Lily May Peel)\u0026#39;, \u0026#39;Allen, Mr. William Henry\u0026#39;, \u0026#39;Moran, Mr. James\u0026#39;, \u0026#39;McCarthy, Mr. Timothy J\u0026#39;, \u0026#39;Palsson, Master. Gosta Leonard\u0026#39;, \u0026#39;Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\u0026#39;, \u0026#39;Nasser, Mrs. Nicholas (Adele Achem)\u0026#39;, \u0026#39;Sandstrom, Miss. Marguerite Rut\u0026#39;, \u0026#39;Bonnell, Miss. Elizabeth\u0026#39;, \u0026#39;Saundercock, Mr. William Henry\u0026#39;, \u0026#39;Andersson, Mr. Anders Johan\u0026#39;, \u0026#39;Vestrom, Miss. Hulda Amanda Adolfina\u0026#39;, \u0026#39;Hewlett, Mrs. (Mary D Kingcome) \u0026#39;, \u0026#39;Rice, Master. Eugene\u0026#39;, \u0026#39;Williams, Mr. Charles Eugene\u0026#39;, \u0026#39;Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\u0026#39;, \u0026#39;Masselmani, Mrs. Fatima\u0026#39;, \u0026#39;Fynney, Mr. Joseph J\u0026#39;, \u0026#39;Beesley, Mr. Lawrence\u0026#39;, \u0026#39;McGowan, Miss. Anna \u0026#34;Annie\u0026#34;\u0026#39;, \u0026#39;Sloper, Mr. William Thompson\u0026#39;, \u0026#39;Palsson, Miss. Torborg Danira\u0026#39;, \u0026#39;Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\u0026#39;, \u0026#39;Emir, Mr. Farred Chehab\u0026#39;, \u0026#39;Fortune, Mr. Charles Alexander\u0026#39;, \u0026#39;Dwyer, Miss. Ellen \u0026#34;Nellie\u0026#34;\u0026#39;, \u0026#39;Todoroff, Mr. Lalio\u0026#39;] count = 0 for data in dataset: for idx in range(len(data)): if data[idx]==\u0026#39;M\u0026#39;: count += 1 print(count) 38  2. 큐 : Queue 2.1. 큐의 구조  가장 먼저 넣은 데이터를 가장 먼저 꺼낼 수 있는 구조  FIFO 또는 LILO 정책을 씀. FIFO를 더 많이 씀.  FIFO : First in, First out LILO : Last in, Last out      2.2. 큐와 관련된 용어  Enqueue: 큐에 데이터를 넣는 기능 Dequeue: 큐에서 데이터를 꺼내는 기능  2.3. 큐가 많이 사용되는 예시  재귀 알고리즘 역추적을 해야할 때 (i.e. 문서 작업 시 실행 취소) 운영체제 멀티태스킹을 위한 프로세스 스케쥴링을 구현할 때  2.4. 큐의 시간 복잡도  삽입 / 삭제  원소를 삽입하거나 삭제하는 경우  O(1)      2.5. 큐의 장단점 장점:\n 데이터의 삽입과 삭제가 빠름  \u0026amp;nbsp\n단점:\n 정책에 따라 가장 위쪽의 원소만 접근 가능함.  i.e. FIFO의 경우 맨 위의 원소만 접근이 가능함.   탐색이 상당히 비효율적임. 다 꺼내보면서 탐색해야 함.  2.6. 파이썬에서의 Queue  queue 라이브러리를 사용.  Queue() : 가장 일반적인 큐(FIFO) 자료구조 LifoQueue() : 나중에 입력된 데이터일 수록 먼저 출력되는 구조 (스택) PriorityQueue() : 입력된 데이터마다 우선순위를 설정, 우선순위 순으로 데이터 출력    2.6.1. queue.Queue() : FIFO # queue 라이브러리 import queue fifo_queue = queue.Queue() # FIFO fifo_queue.put(3) # enqueue fifo_queue.put(15) fifo_queue.put(26) fifo_queue.put(56) print(fifo_queue.qsize()) # 크키 print(fifo_queue.queue) 4 deque([3, 15, 26, 56]) \u0026amp;nbsp\nfifo_queue.get() # dequeue 3 \u0026amp;nbsp\nfifo_queue.get() 15 \u0026amp;nbsp\nfifo_queue.get() 26 \u0026amp;nbsp\nfifo_queue.get() 56 \u0026amp;nbsp\n2.6.2. queue.LifoQueue() : LIFO (Last in, First out) lifo_queue = queue.LifoQueue() # LIFO lifo_queue.put(3) # enqueue lifo_queue.put(15) lifo_queue.put(26) lifo_queue.put(56) print(lifo_queue.qsize()) print(lifo_queue.queue) 4 [3, 15, 26, 56] \u0026amp;nbsp\nlifo_queue.get() # dequeue 56 \u0026amp;nbsp\nlifo_queue.get() 26 \u0026amp;nbsp\nlifo_queue.get() 15 \u0026amp;nbsp\nlifo_queue.get() 3 2.6.3. queue.PriorityQueue() : 우선순위에 따라 dequeue pri_queue = queue.PriorityQueue() pri_queue.put((34, \u0026#39;a\u0026#39;)) # 튜플 형태로 넣음 (우선순위, 데이터) pri_queue.put((14, \u0026#39;b\u0026#39;)) pri_queue.put((72, \u0026#39;c\u0026#39;)) pri_queue.put((11, \u0026#39;d\u0026#39;)) pri_queue.put((48, \u0026#39;e\u0026#39;)) print(pri_queue.qsize()) # size print(pri_queue.queue) # 숫자가 낮을 수록 우선순위가 높음 (i.e. 1순위, 2순위) 5 [(11, 'd'), (14, 'b'), (72, 'c'), (34, 'a'), (48, 'e')] \u0026amp;nbsp\npri_queue.get() # dequeue (11, 'd') \u0026amp;nbsp\npri_queue.get() (14, 'b') \u0026amp;nbsp\npri_queue.get() (34, 'a') \u0026amp;nbsp\npri_queue.get() (48, 'e') \u0026amp;nbsp\npri_queue.get() (72, 'c') \u0026amp;nbsp\n연습\nqueue라이브러리가 아닌, 파이썬의 리스트를 가지고 enqueue, dequeue 구현해본다.\nclass queue_list: def __init__(self): self.queue = list() def __size__(self): return len(self.queue) def __items__(self): return self.queue def enqueue(self, data): self.queue.append(data) def dequeue(self): target_data = self.queue[0] del self.queue[0] return target_data ls_queue = queue_list() ls_queue.enqueue(4) ls_queue.enqueue(9) ls_queue.enqueue(1) print(ls_queue.__size__()) print(ls_queue.__items__()) 3 [4, 9, 1] \u0026amp;nbsp\nls_queue.dequeue() 4 \u0026amp;nbsp\nls_queue.dequeue() 9 \u0026amp;nbsp\nls_queue.dequeue() 1 \u0026amp;nbsp\n 3. 스택 : Stack  데이터를 제한적으로 넣을 수 있는 구조  한쪽 끝에서만 자료를 넣거나 뺄 수 있는 구조    큐와의 차이점\n 큐 : FIFO 정책 스택 : LIFO 정책 \u0026ndash;\u0026gt; Last in, First out  3.1. 스택의 구조 스택은 LIFO, FILO 구조이지만, LIFO, FILO라고 말하기보다는 통상 이러한 구조를 스택 Stack 그 자체로 부름.\n 가장 마지막에 넣은 것을 가장 먼저. 가장 먼저 넣은 것을 가장 마지막에.  LIFO : 마지막에 넣은 데이터를 가장 먼저 추출하는 데이터 관리 정책. FILO : 처음 넣은 데이터를 가장 마지막에 추출하는 데이터 관리 정책.    3.2. 스택 관련 용어  push() : 데이터를 스택에 삽입하는 연산 (넣기) pop() : 데이터를 스택에서 삭제하는 연산 (꺼내기)| stack underflow : 비어있는 스택에서 데이터를 꺼내려고 할 때 생기는 오류 stack overflow : 가득 차있는 스택에 데이터를 삽입하려고 할 때 생기는 오류  3.3. 스택의 활용  컴퓨터 내부의 프로세스의 함수들이 동작하는 방식에 쓰임 실행 취소 : 가장 최근에 했던 작업부터 거슬러 올라가며 취소 웹 브라우저 뒤로 가기 기능 : 가장 최근에 봤던 페이지 순으로 거슬러 가며 브라우징  3.4. 스택의 구현 방법  배열(array)  장점:  구현이 쉬움 접근이 빠름   단점:  데이터의 최대 개수를 미리 정해야 함. 데이터 삽입/삭제 시 매우 비효율적.     연결리스트(linked list)  장점:  데이터 최대 개수가 정해져 있지 않음. 데이터 삽입 삭제가 용이함.   단점:  데이터 접근이 한번에 가능하지 않음. 따라서 시간이 걸림.      3.5. 파이썬에서의 Stack  리스트를 통해서 배열(array) 기반의 스택을 구현해볼 수 있음.  .append() : push .pop() : pop    data_stack = list() data_stack.append(1) # push data_stack.append(3) print(data_stack) [1, 3] \u0026amp;nbsp\ndata_stack.pop() # pop 3 \u0026amp;nbsp\nprint(data_stack) [1] \u0026amp;nbsp\nReference  Fast Campus 알고리즘 / 기술면접 강의 [자료구조] 스택(Stack), 큐(Queue), 덱(Deque)  ","permalink":"https://lucaseo.github.io/posts/2021-01-27-python-datastructure-array-que-stack/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 0. 자료구조? 알고리즘?  자료구조 Data Structure  대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조    \u0026amp;nbsp\n 체계적인 데이터 구조화의 필요성  코드 상에서 효율적인 데이터 처리하기 위함 어떤 데이터 구조를 사용하느냐에 따라 효율이 달라짐.    \u0026amp;nbsp\n 알고리즘이란  어떠한 문제를 풀기 위한 절차 / 방법 특정 문제에 해당하는  특정 입력을 넣으면 특정 출력을 얻을 수 있도록 하는 프로그래밍      \u0026amp;nbsp","title":"[KR] 자료구조 \u0026 알고리즘 : 배열(array), 큐(queue), 스택(stack)"},{"content":"0. 데이터와 librosa 실제로 소리 데이터를 다뤄보기 위해서 음악 데이터를 준비하겠습니다. 음악 장르 분류 데이터셋으로 유명한 GTZAN Dataset을 다운받아 음악 파일을 하나 선택했습니다.\n그리고 Librosa는 오디오와 음악 분석을 위 빠질 수 없는 파이썬 패키지입니다. 음원이나 소리 파일을 불러와 waveform을 시각화 하거나 다른 형태로 변환할 수 있는 기능을 제공합니다.\n( Librosa는 pip install librosa 명령어를 통해 설치할 수 있습니다. )\nimport warnings warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt import IPython.display as ipd import librosa import librosa.display file_path = \u0026#39;disco.00054.wav\u0026#39; # 실습에 사용할 음악 파일 어떤 음악인지 한번 들어볼까요?\nipd.Audio(file_path)   1. Waveform 음원을 아래처럼 간단하게 읽어와보겠습니다. 튜플인 결과값에서 첫 번째는 numpy array형태의 waveform의 amplitude값이고, 두 번째 값은 sampling rate으로, 초당 샘플 갯수를 의미합니다. Sampling rate는 파일을 읽을 때, parameter로 설정할 수 있습니다. Default값은 22050입니다.\nwav, sr = librosa.load(file_path) print(\u0026#34;Amplitude: \\n\u0026#34;, wav) print(\u0026#34;Sampling rate: \u0026#34;, sr) Amplitude: [-0.08001709 -0.07550049 -0.08358765 ... 0.08270264 0.10083008 0.10562134] Sampling rate: 22050 시각화를 통해 데이터가 아래와 같은 형태의 waveform을 띄고 있음을 확인해 볼 수 있습니다.\nfig = plt.figure(figsize = (14,5)) librosa.display.waveplot(wav, sr=sr) plt.ylabel(\u0026#34;Amplitude\u0026#34;) plt.show()    2. FFT (Fast Fourier Transform) 앞서 다룬 포스트에서 time-domain의 waveform을 FFT(Fast Fourier Transform)을 통해 분해(decompose)하고, frequency-domain으로 변환하여 원본 소리 데이터를 형성하는 주파수(frequency)의 정도를 파악하고 시각화 할 수 있다고 이해했습니다. 이번에는 numpy를 통해 앞서 구한 waveform amplitude에 FFT를 적용할 수 있습니다.\nFFT를 적용하여 시각화한 결과는 아래와 같습니다. 주로 1000Hz 이하로 많이 분포해 있는 것을 확인할 수 있네요.\nfft = np.fft.fft(wav) magnitude = np.abs(fft) frequency = np.linspace(0, sr, len(magnitude)) left_frequency = frequency[:int(len(frequency)/2)] left_magnitude = magnitude[:int(len(magnitude)/2)] fig = plt.figure(figsize = (14,5)) plt.plot(left_frequency, left_magnitude) plt.xlabel(\u0026#34;Frequency\u0026#34;) plt.ylabel(\u0026#34;Magnitude\u0026#34;) plt.show()    3. STFT (Short-Time Fourier Transform) STFT(Short-Time Fourier Transform)은 시간 정보가 유실되는 것을 방지하기 위해, 사전에 정의한 시간의 간격(window 또는 frame) 단위로 쪼개어 푸리에 변환을 적용하는 기법입니다. STFT는 librosa를 통해 적용할 수 있습니다. 이때, window의 크기(n_fft)와 window 간에 겹치는 사이즈(hop_length)를 설정해줍니다. 일반적으로는 n_fft의 1/4 정도가 겹치도록 설정한다고 합니다.\nn_fft = 2048 hop_length = 512 stft = librosa.stft(wav, n_fft = n_fft, hop_length = hop_length) spectrogram = np.abs(stft) print(\u0026#34;Spectogram :\\n\u0026#34;, spectrogram) Spectogram : [[1.42030740e+00 7.47260690e-01 5.37097007e-02 ... 1.88164175e-01 1.21684396e+00 2.43966293e+00] [1.24079692e+00 6.81115210e-01 6.51928782e-02 ... 2.08189130e-01 1.22743416e+00 2.52433753e+00] [1.09137118e+00 4.82469022e-01 1.85490116e-01 ... 6.99194148e-02 1.43492615e+00 2.53518319e+00] ... [3.84226470e-04 1.63909295e-04 9.80101977e-05 ... 2.23124225e-04 2.80503096e-04 1.98973445e-04] [3.00532440e-04 1.77996873e-04 1.29194887e-04 ... 9.72686321e-05 2.01086092e-04 9.30428141e-05] [2.59254826e-04 9.42422412e-05 5.96536411e-05 ... 7.49909232e-05 1.41018099e-04 1.10232315e-04]] 3.1. Spectogram STFT를 적용하여 구한 spectogram을 아래와 같이 시각화 해봤습니다. x축은 시간, y축은 주파수, 그리고 주파수의 정도를 색깔로 확인할 수 있습니다. 그런데 값이 너무 미세해서 차이를 파악하고 관찰하기 적합하지 않습니다.\nfig = plt.figure(figsize = (14,5)) librosa.display.specshow(spectrogram, sr=sr, hop_length=hop_length) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.plasma() plt.show()    3.2. Log-spectogram 그래서 보통 푸리에변환 이후 dB(데시벨) scaling을 적용한 Log-spectogram을 구합니다. 다분히 시각적인 이유뿐만 아니라, 사람의 청각 또한 소리를 dB scale 로 인식하기 때문에, 이를 반영하여 spectogram을 나타내는 것이 분석에 용이합니다.\nlibrosa.amplitude_to_db()를 통해 Log-spectogram을 구하여 시각화 한 결과입니다. 대부분의 에너지가 1024Hz이하의 낮은 주파수대역에 모여 있는 것을 볼 수 있네요.\nlog_spectrogram = librosa.amplitude_to_db(spectrogram) fig = plt.figure(figsize = (14,5)) librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length, x_axis=\u0026#39;time\u0026#39;, y_axis=\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;) plt.show()    4. MFCC 마지막으로 MFCC(Mel Frequency Cepstral Coefficient)를 구하고 시각화해보겠습니다. MFCC는 오디오 신호 처리 분야에서 많이 사용되는 소리 데이터의 특징값(Feature)으로, 사람의 청각이 예민하게 반응하는 정보를 강조하여 소리가 가지는 고유한 특징을 추출한 값입니다.\n마찬가지로 librosa.feature.mfcc()를 통해 feature값을 아래와 같이 추출할 수 있습니다. 파라미터 중 n_mfcc는 추출하고자 하는 mfcc의 개수입니다. 이번 실습에서는 13개로 설정했습니다.\nMFCCs = librosa.feature.mfcc(wav, sr = 22050, n_fft = n_fft, hop_length = hop_length, n_mfcc = 13) # number of coefficient we want to extract print(\u0026#34;MFCCs Shape: \u0026#34;, MFCCs.shape) print(\u0026#34;MFCCs: \\n\u0026#34;, MFCCs) MFCCs Shape: (13, 1293) MFCCs: [[-176.91516 -173.07141 -171.01598 ... -144.9992 -153.77185 -155.61522 ] [ 118.94415 117.39079 108.01162 ... 111.50748 108.44453 113.359665 ] [ -12.249197 -16.364796 -20.116379 ... -68.0366 -40.615326 -32.124104 ] ... [ -7.0000467 -8.825797 -10.732431 ... -16.528994 -17.69807 -21.954914 ] [ 10.861979 10.393564 7.8947186 ... -8.206779 -3.6493917 -4.4267316] [ -12.490692 -10.728968 -14.610505 ... -2.9667187 -12.053108 -9.9868355]] fig = plt.figure(figsize = (14,5)) librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length, x_axis=\u0026#39;time\u0026#39;,) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;) plt.show()    5. 한줄 요약  Librosa라는 킹갓제너럴 파이썬 패키지를 이용해서 소리 데이터를 불러오고, 변형할 수 있다!  6. Reference  audio-processing-wave by scpark20  ","permalink":"https://lucaseo.github.io/posts/2021-01-22-hands-on-preprocess-audio-data/","summary":"0. 데이터와 librosa 실제로 소리 데이터를 다뤄보기 위해서 음악 데이터를 준비하겠습니다. 음악 장르 분류 데이터셋으로 유명한 GTZAN Dataset을 다운받아 음악 파일을 하나 선택했습니다.\n그리고 Librosa는 오디오와 음악 분석을 위 빠질 수 없는 파이썬 패키지입니다. 음원이나 소리 파일을 불러와 waveform을 시각화 하거나 다른 형태로 변환할 수 있는 기능을 제공합니다.\n( Librosa는 pip install librosa 명령어를 통해 설치할 수 있습니다. )\nimport warnings warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt import IPython.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(3) - 파이썬으로 음악 데이터 읽어오기"},{"content":"2020년 올해는 \u0026hellip; \u0026amp;nbsp\n1. 올해의 가장 큰 변화 재택근무\nCOVID-19 방역에 다들 지쳐가고 서서히 경각심도 조금씩 희미해질 때쯤, 줄지 않는 확진자수에 결국 재택근무가 주기적으로 자리잡게 되었습니다. 사실 나는 개인적으로 재택근무가 별로라고 생각합니다. 집에서는 집중이 안 되기도 하고, 점심을 제 돈으로 해결해야 합니다. 집에서는 의자도 불편한데 새 의자를 사자니 가격이 만만치 않습니다. 재택근무를 원활하기 하기 위한 문화가 정착이 되지 않다보니, 으레 팀원들끼리 커뮤니케이션도 덜하게 되고 일하다가도 뭔지 모를 답답함을 느끼기도 했습니다.\n그래도 적응해야지 어쩌겠습니까. 춥지만 기분전환을 하기 위해 환기를 하거나 동네를 한바퀴 돌고 들어옵니다. 점심도 돈 아낀다고 직접 준비하기보다는 그냥 나가서 포장을 하구요, 나간 김에 바깥 공기도 좀 쐬고, 커피도 미리 테이크아웃을 해옵니다. 내려놓는게 있으니, 확실히 마음이 편해지는 부분이 확실히 있습니다. 커뮤니케이션도 마찬가지였습니다. 재택 근무 중 동료로부터 응답이 오지 않거나 슬랙에서 2~3 문장 이상의 커뮤니케이션이 이어질 때면, 바로 전화를 걸거나 구글 행아웃 링크를 바로 보내줍니다. 바로 어젠다에 대해 이야기를 나누거나, 바쁘면 조금 이따 이야기 하자고 답변이 옵니다. 전에는 왜 안 그랬는지 잘 모르겠네요. (대충 속이 시원해지는 개비스콘 짤.)\n부모님을 포함해서 제 주변에서는 재택하면 다들 와 좋겠다 하는 반응이지만, 그러나 저러나 저는 여전히 오피스가 더 좋기는 합니다. 그래도 시국이 시국인만큼 재택을 해야하니, 내년에도 계속해서 저만의 방식을 찾아가야겠습니다.\n\u0026amp;nbsp\n2. 올해의 깨달음 하\u0026hellip; 일단은 그냥 내가 하자 될 진 모르겠지만\n사실 2020년은 제가 속해있는 프로젝트의 부족한 부분과 기술부채를 해결했어야 하는 해입니다. 제가 혼자서 전부 하기에는 무리가 있던 부분이었고, 그래서 팀에 도움을 많이 요청했었습니다. 새로운 시니어 분이 오셔서 개발의 전반적인 부분을 손봐주시는 만큼 저도 기대하는 부분이 많았는데요. 하지만 영업으로 인해 생기는 ad-hoc 요청사항과 더 우선순위로 취급되는 프로젝트들이 등장하면서 기존의 프로젝트에는 제동이 걸렸습니다. 점점 우선순위에서 밀려나는걸 느끼게 되면서 스트레스를 많이 받기도 했습니다. 애매한 상태로 몇 개월이 그냥 지나가게 되었고 멘탈 관리도 조금 힘들었던 시기였습니다.\n그렇게 저는 지속적으로 서포트를 요청을 드리고, 이것도 해야하고 저것도 해야하는 상대방은 상대방대로 스트레스를 받았습니다. 교통정리가 되지 않은 채 다들 당장 눈 앞에 놓인 요청사항들을 해결하느라 경주마 같이 달리는 것을 보고, 이게 지금 조직의 체질이라는 것을 깨닫게 되었습니다. 체질이란 건 쉽게 바뀌지 않죠. 저도 지금 회사에서 2년을 일했으니, 그 체질이 형성되는데 기여하던 부분이 분명 있을 것 입니다.\n그래서 늦었지만 일단 저 혼자 어떻게든 해보기로 했습니다. 제가 도입해보고, 문서를 만들어서 공유를 조금씩 하고 있습니다. 시행착오가 많지만 이렇게라도 안 하면 아무것도 개선할 수 없을 것 같았습니다. 나중에 봤을 때 이게 좋아보이면 따라와 줄 것이고, 구리면 피드백을 해줄 것이라 생각합니다. 이도저도 아니고 그냥 아무 말도 없으면 \u0026hellip; 그건 일은 없기를 바랍니다 ㅠㅠ\n사실 이런 문제점은 마지막에 이렇게저렇게 해서 문제를 없앨 수 있었다! 하고 뿌듯하게 마무리 해야하는데, 그게 아니라 이제 시작인 것 같습니다. 그래서 2020년은 답답했고, 2021년에는 답답해하기보다 느려도 나 혼자 해보겠다. 인 것입니다.\n\u0026amp;nbsp\n3. 올해의 사서고생 방송통신대 두번째학기\n2020년 2월에는 방송통신대 정보통계학과에 3학년으로 편입했고, 어느 새 2번째 학기가 끝났습니다. 기존의 온라인 강의 방식을 고수하면서, 코로나로 인해 대면 기말고사는 기말과제물로 대체되었습니다. 호기롭게 7과목을 신청했다가 한 학기 내내 과제물로 허덕인 것이 많이 기억납니다.\n솔직히 방송통신대의 커리큘럼은 다른 4년제 대학교 동일 학부의 커리큘럼과 비교했을 때 깊이가 떨어지는 것이 사실입니다. 누군가에게는 우스워보일 수도 있을 것 같아요. 이렇게까지 해야하나 싶을 정도로 반복적이고 손이 많이 가는 노가다스러운 과제가 있는 반면, 영상 강의에서는 너무 성의없이 대충 짚고 넘어가서, 도저히 이해가 되지 않아 따로 찾아가며 공부해야 했던 과제도 있었습니다. 그렇지만 직장을 다니며 바쁜 와중에도 성실하게 강의를 듣고 과제를 제출했습니다.\n어제 성적이 나왔는데 모든 과목에서 A+를 받았습니다. 중고등학교, 대학교때는 정말 공부를 안 해서 상상도 못 했던 평점인데 말이죠. 사실 기존처럼 객관식 시험을 봤다면 이렇게 좋은 성적을 기대하지 못 했을 수도 있습니다. 성실하게 과제를 작성한 덕분인 것 같아요.\n이번 학기에 들었던 과목들 중 인상적이었던 몇몇 과목에 대해 조금 소감을 남겨보겠습니다.\n 파이썬과 R: 가볍게 생각한 과목이었고 난이도도 높지 않습니다. 하지만 담당이신 K교수님은 역시나 기대를 저버리지 않으시고 어마어마한 양의 과제를 내주셨습니다. 게다가 파이썬과 R로 동일한 문제를 풀고, 스크린샷을 찍고, 코드를 옮겨적고, 설명까지 적어야 하니\u0026hellip; 손가락이 많이 뻐근해졌달까요. 빅데이터의 이해: 이론적인 부분이 많고, 교재의 내용이 비교적 오래됐기 때문에 인상적이지 않았습니다. 하지만 과제물을 작성하면서 많이 생소했던 MapReduce에 대한 직접 그래프를 그려가며 작동원리를 알아갔던 부분과, 추천시스템 케이스 스터디를 하면서 제가 개인적으로 좋아하는 스포티파이에 대해 조사하고 정리했던 것이 정말 재미있었습니다. 시간 가는 줄 몰라서 시간 관리가 안 됐던 것은 함정 \u0026hellip; 통계학의 개념 및 제문제: 내용은 수리통계학인데 왜 과목명을 이렇게 정했는지 의문인 과목이었습니다. 그리고 역시나 많이 어려워서, 종강 후에도 마음을 놓지 않고 다시 관련 서적을 사서 제대로 공부해야겠다는 생각이 들었던 과목이었습니다.  아무튼 이렇게 방송통신대 정보통계학과에서의 두번째 학기가 지나갔습니다. 다음 학기부터는 통계,데이터과학과로 이름이 바뀐다고 하네요. 아마 다음 학기부터 저는 컴공 과목을 주로 듣게 될 것 같습니다.\n\u0026amp;nbsp\n4. 올해의 엉겁결 ADsP 자격증 취득\nADsP는 그냥 한번 봐볼까 하는 생각으로 접수했다가, 응시날이 코앞에 다가와 벼락치기를 했습니다. 퇴근길에 서점에서 교재 뒤쪽에 붙어있던 모의고사를 한번 풀어보는 방식으로 공부하고, 부족한 부분은 인터넷으로 찾아가며 부족한 부분을 위주로 준비했습니다.\nADsP는 데이터 이해, 데이터분석 기획, 데이터분석 의 세가지 과목으로 나뉘어져 있습니다. 응시일까지 과락을 걱정하며 가장 많이 걱정했던 과목은 데이터분석 기획 입니다. 데이터분석의 경우 방송통신대에서 수강한 데이터마이닝 과목과 겹치는 내용이 많기 때문에 걱정이 없었지만, 데이터 이해와 데이터분석 기획의 경우 실무에서는 많이 다루지 않는 암기위주의 이론적인 내용이 대부분이었기 때문에 체화하는 게 쉽지 않았고 오답이 많이 발생했었습니다.\n하지만 합격기준은 60/100점 이상과 과목별 40% 이상 취득 이기 때문에, 합격 기준만 넘자는 심정으로 선택과 집중을 한 결과, 합격을 할 수 있게 되었습니다. 엉겁결에 접수해놓고 너무 바빠서 완전히 까먹고 있었는데, 벼락치기 한 것 치고는 너무 다행이었던 그런 살짝은 부끄러운 자격증 취득이었습니다. 응시일이 생일날이었어서, 아침부터 투덜투덜 응시장 간 건 안 비밀 \u0026hellip;\n\u0026amp;nbsp\n5. 번외 어워즈 시상식 올해의 노래 뱃사공 - 다와가\n쓸데없이 아둥버둥 바득바득 치열하게 살고 있는 것은 아닌지, 나 자신을 다시 되돌아보게 해준 노래.\n\u0026amp;nbsp\n올해의 음식점 익선동 르블란서\n생일을 맞이해서 갔던 곳인데 코로나 스트레스가 날라갔던, 즐거운 시간만 기억나는 곳.\n\u0026amp;nbsp\n올해의 카페 커피온리 영등포구청점\n900원 아이스아메리카노. 재택하면서 아이스아메리카노를 매번 사먹어야 하는 걱정을 조금이나마 해소해준 테이크아웃 커피집.\n( + 커피 맛을 잘 몰라서 신경 안 씀)\n\u0026amp;nbsp\n올해의 IT기기 맥북 프로 16인치\n근데 이제 어마어마한 성능의 M1 맥북이 출시된.\n( + 그리고 떡락하는 중고가)\n\u0026amp;nbsp\n올해의 손떨림 생애 첫 대출로 1억을 넘어가는 후덜덜한 전세대출\n( + 그리고 대출받자마자 추락한 신용등급)\n\u0026amp;nbsp\n올해의 소망 COVID-19 종식과 해외여행\n\u0026amp;nbsp\n 2021년에 기대하는 것. 글또 5기 글또는 글쓰는 또라이가 세상을 바꾼다!고 하는 글쓰는 개발자 모임입니다. 4기에 이어 5기에 참여하고 있고, 글또 활동을 통해 2021년에도 글쓰는 습관을 기를 수 있도록 꾸준히, 제가 만족할 수 있는 퀄리티의 글을 쓰는 것이 목표입니다.\nCS 관련 기초 공부 방송통신대에서의 3번째 학기. 컴퓨터과학과 위주의 수강과목들 생각입니다. 그리고 알고리즘과 코딩테스트 준비도 할 계획입니다.\nSound of AI - Open Source Research Project Sound of AI는 Musimap의 시니어 데이터사이언티스트인 Valerio Velardo 씨가 운영하는 AI \u0026amp; Audio 커뮤니티입니다. Valerio의 튜토리얼을 따라 Audio와 AI에 관련된 공부를 하면서, 2021년에 진행될 Open Source Research 프로젝트에도 참여할 예정입니다. 아직 초기 단계이지만, 많은 기대가 됩니다.\n가짜연구소 스터디 가짜연구소는 머신러닝을 중심으로 스터디, 밋업 등의 이벤트가 이루어지고 있는 커뮤니티입니다. 2021년에는 가짜연구소의 스터디를 통해 캐글에도 도전해 볼 계획입니다.\n","permalink":"https://lucaseo.github.io/posts/2020-12-31-review-2020-2nd-half/","summary":"2020년 올해는 \u0026hellip; \u0026amp;nbsp\n1. 올해의 가장 큰 변화 재택근무\nCOVID-19 방역에 다들 지쳐가고 서서히 경각심도 조금씩 희미해질 때쯤, 줄지 않는 확진자수에 결국 재택근무가 주기적으로 자리잡게 되었습니다. 사실 나는 개인적으로 재택근무가 별로라고 생각합니다. 집에서는 집중이 안 되기도 하고, 점심을 제 돈으로 해결해야 합니다. 집에서는 의자도 불편한데 새 의자를 사자니 가격이 만만치 않습니다. 재택근무를 원활하기 하기 위한 문화가 정착이 되지 않다보니, 으레 팀원들끼리 커뮤니케이션도 덜하게 되고 일하다가도 뭔지 모를 답답함을 느끼기도 했습니다.","title":"[KR] 2020년 하반기가 지났다"},{"content":"이번 포스트에서는 소리의 파형을 분석하기 위해 사용되는 기법인 푸리에 변환과 특징 추출값으로 사용되는 MFCC의 개념에 대해서 알아보겠습니다.\n1. 소리는 주파수의 합산    Piano in Waveform   \u0026amp;nbsp\n위의 이미지는 실제 피아노 소리 파일을 파형(waveform) 형태로 시각화 한 것입니다. 간단한 피아노 소리이지만 매우 복잡한 파형을 그리고 있는 것을 볼 수 있는데요. 사실 우리가 흔히 들을 수 있는 이러한 \u0026ldquo;소리\u0026quot;라는 것은 각기 다른 단일 주파수를 가진 무수히 많은 정현파(sinewave)가 합산되어 형성된 것입니다. 제 경우에는 처음에 이해가 잘 되지 않았는데, 이런 시각화들이 많은 도움이 되었습니다. 직관적인 이해가 되시나요?\n\u0026amp;nbsp\n2. Fourier Transform 무수히 많은 정현파의 모음이 \u0026ldquo;소리\u0026quot;를 구성한다는 것은 이해를 했습니다. 하지만 이와 반대로 \u0026ldquo;소리\u0026quot;를 분석하기 위해서는 복잡한 소리(complex sound)가 어떠한 단일주파수들로 이루어져있는지를 분해(decompose)해봐야겠죠. 이를 위해 원본 소리에 행하는 작업을 푸리에변환(Fourier transform) 이라고 합니다. 다른 말로는 푸리에변환을 통해서 어떤 정현파가 얼마나 원본 소리를 구성하는지 파악할 수 있습니다.\n2.1. Spectrum 파형의 형태를 띄는 원본에 푸리에 변환을 적용하여 산출되는 결과물은 **스펙트럼(spectrum)**입니다. 스펙트럼이란, 각 주파수의 정도를 시각화하여 보이는 기법입니다. 아래 그림은 파형에 푸리에변환 기법 중 하나인 FFT(Fast Fourier Transform)을 적용한 결과를 보여주고 있는데요, 스펙트럼에서 X축은 0~12000 가량의 주파수이고, Y축은 각 주파수의 진폭(amplitude) 또는 그 정도(magnitude)를 나타냅니다.\n푸리에 변환을 통해서 파형이 스펙트럼으로 표현될 때 주목해야하는 점은, 파형은 시간의 흐름에 따라 변화하는 time-domain의 성질을 띄는 반면, 스펙트럼은 각 frequency마다 그 정도가 달라지는 frequency-domain의 성질을 띈다는 것입니다. 아래 그림의 스펙트럼에서는 전체 9초 가량의 시간대에 대한 주파수와 진폭을 모두 보여주고 있습니다. 즉, 스펙트럼에는 시간의 흐름에 따른 정보는 유실되는 것이죠.\n   푸리에변환의 결과 예시   2.2. STFT **STFT(Short-Time Fourier Transform)**는 푸리에변환의 한계를 보완하는 기법입니다. STFT는 전체 길이보다는 짧은 어떠한 시간 간격(window)을 설정한 후, 이 간격을 시간의 흐름에 따라 움직여가며(slide) 복수의 변환을 행하여, 시간의 흐름에 따른 주파수 정보를 얻습니다. 쉽게 말해, 9초의 소리가 있다면, 1초 간격으로 쪼갠 후 1초 간격으로, 푸리에변환을 하는 식인 것이죠. STFT의 산출물은 주파수, 진폭과 더불어 시간의 정보도 포함된 스펙토그램(spectogram)입니다. 아래의 스펙토그램 시각화에서 X축은 시간, Y축은 주파수, 그리고 주파수의 정도가 데시벨(색깔)로 표현되었습니다.\n   STFT의 동작 예시   \u0026amp;nbsp\n3. Mel Frequency Cepstral Coefficient (MFCC) 실제 소리나 음성을 분석할 때는 푸리에 변환만을 하지 않고, 그로부터 차원울 축소하고 분석에 용이한 특징을 추출하는 과정(Feature Extraction)을 거칩니다. 특히 오디오 신호 처리 분야에서 많이 사용되는 소리 데이터의 특징값(Feature)으로는 MFCC가 있습니다.(MFCC의 세부적인 내용과 구하는 방법은 다른 포스트에서 따로 다루도록 하겠습니다) MFCC는 갖가지 신호가 합쳐 생성된 \u0026ldquo;소리\u0026quot;가 가지는 고유한 특징을 추출한 값이라는 장점이 있습니다. MFCC를 이해하기 위해서 멜-스펙토그램과 캡스트럼 분석에 대해서 짚고 가겠습니다.\n3.1. Mel-Spectogram \u0026amp; Mel-scale MFCC를 설명하기 전에 멜-스펙토그램(Mel-Spectogram)에 대해서 간단히 알아보겠습니다. 멜스펙토그램이란, 사람의 달팽이관의 특성을 반영한 Mel-scale을 적용한 스펙토그램 표현법입니다. 달팽이관은 저주파 대역을 감지하는 구간이 조밀하고, 고주파 대역을 감지하는 구간은 넓게 이루어져 있습니다. 따라서 저주파 대역에 의미 있는 정보가 집중되어 있으며, 인간의 청각은 저주파 대역에서 더 민감하게 반응을 한다는 점을 반영하여, 주파수의 대역에 차등적으로 중요도를 적용하는 Mel-scale이 제안 되었습니다.\n$$ \\text{Mel}(f) = 2595 \\log_{10} \\left(1+ \\frac{f}{700}\\right) $$\n3.2. Cepstral Analysis 캡스트럼 분석(Cepstral Analysis)이란, 스펙토그램으로부터 소리의 대표적인 특징을 추출하는 기법입니다. 소리가 큰 진폭으로 진동할 때, 이를 공명(resonance)라고 합니다. 예를 들어 성대가 떨림으로 인해서 어떠한 소리가 최초로 생성될 때, 이를 공명이라고 하죠. 스펙트럼에서 공명은 뾰족한 봉우리(peak)로 표현이 되고, 이 봉우리들을 포먼트(formant)라고 합니다. 우리가 듣는 소리의 대표적인 특징이 스펙토그램의 포먼트로 표현된다는 것이죠. 따라서 캡스트럼 분석은 아래의 그림처럼 스펙토그램으로부터 포먼트를 찾아서 분리하는 기법입니다. (Spectral 에서 spec을 뒤집으면 Cepstral 이 되는 것은 우연일까요?)\n캡스트럼은 스펙트럼에 역푸리에변환(IFT: Inverse Fourier Transform)을 취하여 구합니다.\n   Cepstral Analysis (https://brightwon.tistory.com/11)   3.3. MFCC 구하기 MFCC를 구하는 과정은 다음과 같습니다.\n  주어진 신호(오디오 데이터)를 일정한 간격(window)로 나누어 푸리에변환을 적용하여 스펙토그램을 구한다. 스펙토그램의 제곱(파워 스펙토그램)에 Mel-scale을 기반으로 필터뱅크(Filter banks)를 구성하고 필터뱅크에 로그 변환을 수행한다. 이렇게 하면 Mel-scale에 따라 나누어진 구간 별로 분포한 정보를 확보한다. 주어진 Log Mel-spectogram에 역푸리에 변환을 적용하여 Cepstrum을 구한다. MFCC는 이렇게 구한 결과의 진폭(amplitude)이다.   3.4. MFCC의 의미 MFCC를 구하기 위해 적용된 기법과 과정에 대해 알아보니 머리가 조금 아프네요(\u0026hellip;) 하지만 그 의미를 되새겨보자면 다음과 같이 요약할 수 있을 것 같습니다.\n MFCC는 무수히 많은 단일주파수로 복잡하게 구성된 소리로부터 (1)사람의 청각이 예민하게 반응하는 정보를 강조하고 (2)소리의 대표적인 부분만을 취사 선택한 피처(feature)이다.\n \u0026amp;nbsp\n4. 세줄 \u0026hellip; 아니 쓰다 보니 네줄 요약  소리는 무수히 많은 단일주파수의 합산으로 이루어져있다. 푸리에변환을 소리가 어느 주파수로 이루어져 있는지 분해할 수 있다. 푸리에변환만 수행하면 시간의 정보가 유실되므로 STFT를 적용한다. MFCC는 소리를 분석하는데 많이 사용되는 피쳐로, 인간의 청각에 유의미한 주파수대역을 강조하고, 소리의 대표적인 특징을 추출한 값이다.   이번 포스트에서는 푸리에변환과 MFCC의 개념에 대해서 간단히 알아보았습니다. 다음에는 파이썬으로 직접 소리 데이터를 읽고 처리하는 과정에 대해 정리해보겠습니다.\n\u0026amp;nbsp\n5. Reference  https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%AA%85 https://en.wikipedia.org/wiki/Mel-frequency_cepstrum#cite_note-2 https://en.wikipedia.org/wiki/Mel_scale https://en.wikipedia.org/wiki/Cepstrum https://brightwon.tistory.com/11 https://ratsgo.github.io/speechbook/docs/fe/mfcc https://tech.kakaoenterprise.com/66  ","permalink":"https://lucaseo.github.io/posts/2020-12-26-understanding-audio-data-techniques/","summary":"이번 포스트에서는 소리의 파형을 분석하기 위해 사용되는 기법인 푸리에 변환과 특징 추출값으로 사용되는 MFCC의 개념에 대해서 알아보겠습니다.\n1. 소리는 주파수의 합산    Piano in Waveform   \u0026amp;nbsp\n위의 이미지는 실제 피아노 소리 파일을 파형(waveform) 형태로 시각화 한 것입니다. 간단한 피아노 소리이지만 매우 복잡한 파형을 그리고 있는 것을 볼 수 있는데요. 사실 우리가 흔히 들을 수 있는 이러한 \u0026ldquo;소리\u0026quot;라는 것은 각기 다른 단일 주파수를 가진 무수히 많은 정현파(sinewave)가 합산되어 형성된 것입니다.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(2) - Fourier Transform, MFCC"},{"content":"1. 소리 데이터란 소리는 다음 과정에서 생산된 것을 의미합니다.\n (1) 어떠한 물체 또는 매질(object)의 진동(vibration)으로 인해 공기 입자들이 밀고 당겨지는 반복적인 과정(oscilation)에서 생긴 파동(wave) (2) 공기의 압력이 낮아지면 빈 공간이 생기면서 다른 입자들로 채워지고, 압력이 높아지만 입자들을 밀어내는, 밀고 당기는 반복적인 연쇄 작용(oscillation)으로 인해 생기는 파동(wave)  그리고 위에서 정의한 파동은 아래와 같은 파형(waveform)으로 나타낼 수 있습니다.\n   \u0026amp;nbsp\n2. 소리 데이터의 표현 2.1. 파형의 요소 파형을 통해서 우리는 다음과 같은 정보를 파악할 수 있습니다. 주로 파동을 표현하는 요소들이죠.\n    시간(time): 파동이 진행되는 시간. 주로 X축입니다. 진폭(amplitude): 진동의 중심에서 최대까지의 거리를 나타냅니다. 진동의 크기를 의미하고, $A$라고 나타냅니다. 주기(period): 1회 진동하는데 걸리는 시간입니다. \\(T\\) 라고 나타냅니다. 진동수(또는 주파수. frequency): 한 점이 1초동안 진동한 횟수. \\(f\\) 라고 나타냅니다.  주기와 진동수는 서로 역수인 관계를 보입니다. (\\(f = \\frac{1}{T}\\) 또는 \\(T = \\frac{1}{f}\\)) 따라서, 주기가 길수록 진동수는 작고 주기가 짧을 수록 진동수는 큰 관계를 보입니다.    \u0026amp;nbsp\n2.2. 파형의 수학적 표현 파형은 사인함수를 통해 다음과 같이 수학적으로도 표현할 수 있습니다.\n\\(y(t) = A \\sin(2 \\pi f t + \\phi)\\)\n \\(A\\) : 진폭(amplitude) \\(sin\\) : 사인함수 \\(f\\) : 진동수(frequency) \\(t\\) : 시간(time) \\(\\phi\\) : 위상(phase). 위상이란 진동체의 상대적인 위치변화를 나타내는 부분입니다. 위상값에 따라 파형이 왼쪽으로 또는 오른쪽으로 이동(shift)된 형상인지를 알 수 있습니다.  \u0026amp;nbsp\n2.3. 진폭, 진동수의 관계 진동과 진폭이 소리와 무슨 관계가 있는지 아직 많이 낯선데요. 우선 진동 수와 음의 높낮이(pitch)의 관계에 대해 알아보겠습니다. 진동수가 클 수록 음이 높아집니다. 즉, 진동한 횟수가 많을 수록 높은 음이 구현된다고 이해할 수 있습니다. 반대로 진동한 횟수가 적다면, 그만큼 음의 높이가 낮은 저음이 구현됩니다. 아래와 같이 정리해볼 수 있습니다.\n frequency \\(\\leftrightarrow\\) pitch  longer periods(\\(T\\)) \\(\\rightarrow\\) lower frequency(\\(f\\)) \\(\\rightarrow\\) lower pitch shorter periods(\\(T\\)) \\(\\rightarrow\\) higher frequency(\\(f\\)) \\(\\rightarrow\\) higher pitch    이번에는 진폭과 소리의 크기(loudness)의 관계에 대해 알아보겠습니다. 진폭이 클 수록 소리는 크고, 진폭이 작을 수록 소리도 작아집니다. 아래와 같이 정리해볼 수 있습니다.\n amplitude \\(\\leftrightarrow\\) loudness  larger amplitude(\\(A\\)) \\(\\rightarrow\\) louder smaller amplitude(\\(A\\)) \\(\\rightarrow\\) quietter    \u0026amp;nbsp\n3. ADC (Analog digital conversion) 우리가 듣는 소리 그 자체, 즉 아날로그 소리는 연속적(continuous)입니다. 따라서 소리를 분석하기 위해서는 아날로그 소리를 디지털적인 형태로 변환하는 작업을 거쳐야 하는데 이를 Analog digital conversion(ADC)이라고 합니다. 말 그대로 아날로그를 디지털로 변환하는 작업이죠.\n3.1. Sampling \u0026amp; Quantization ADC에는 샘플링(Sampling)과 양자화(Quantization)이라는 두 가지 과정을 통해 연속적인 아날로그 소리를 이산적인 데이터로 변환합니다.\n Sampling: 샘플링은 시간의 흐름에 따라 진행되는 연속적인 신호를 특정 주기(time intervial)에 맞추어 신호를 이산적인 시간으로 쪼개는 과정입니다. Quantization: 양자화는 샘플링되어 저장된 데이터를 연속적이지 않은 대표값으로 정수화하여 이산적인 값으로 변환합니다.     Müller, Fundamentals of Music Processing, Springer 2015   \u0026amp;nbsp\n3.2. Sample rate \u0026amp; Bit depth 샘플링과 양자화는 적용되는 방법에 따라, 변환된 데이터의 품질에 영향을 끼칠 수 있습니다. 예를 들어 샘플링 주기나 양자화가 너무 크게 적용된다면, 기존의 아날로그 소리가 지닌 세세한 정보가 손실될 수도 있습니다. 이러한 품질을 결정하는 기준에는 샘플링주기(Samplig rate)과 비트뎁스(Bit depth)가 있습니다.\n   Bit Depth and Sample Rate https://youtu.be/-0rIU9FHiU0    Sampling rate: 1초 동안 담긴 샘플의 개수. 샘플링주기가 높을 수록 시간의 흐름에 따른 신호의 손실이 적습니다. Bit depth: 진폭을 쪼개는 개수. 비트(bit) 개수가 클 수록 기존의 신호에 유사한 형태로 데이터를 저장할 수 있습니다.     \u0026amp;nbsp\n4. 세줄 \u0026hellip; 아니 쓰다 보니 다섯줄 요약 우리가 귀로 듣는 소리에 대한 간단한 정의와, 소리를 분석하기 위해 디지털적인 정보로 변환하는 작업에 대해 알아보았습니다.\n 소리는 공기의 진동(vibration)과 반복적인 움직임(oscillation)으로 인해 압력이 변화하면서 생긴 파형이며 연속적인 아날로그 신호이다. 아날로그 소리를 분석하기 위해서는 ADC를 통해 디지털적이고 이산적인 데이터로 변환한다. 샘플링(sampling)은 아날로그 소리의 시간흐름을 특정 주기로 쪼개어 변환하는 작업이며, 샘플링주기(sampling rate)에 따라 1초 동안 기록되는 샘플의 개수를 달리할 수 있다. 양자화(quantization)는 아날로그 소리의 크기를 특정 구간으로 쪼개어 변환하는 작업이며, 비트뎁스(bit depth)에 따라 그 구간의 세밀함을 조절할 수 있다. 샘플링 주기와 비트뎁스는 아날로그 소리를 디지털 데이터로 변환되는 품질을 결정하며, 세밀할 수록 정보의 손실을 줄이고 기존의 소리에 유사한 데이터를 기록할 수 있다.   다음 포스트에서는 파형을 분석하기 위해 사용되는 푸리에 변환 (Fourier Transform)에 대해서 정리해보도록 하겠습니다.\n","permalink":"https://lucaseo.github.io/posts/2020-12-13-understanding-audio-data-sound-waveform-adc/","summary":"1. 소리 데이터란 소리는 다음 과정에서 생산된 것을 의미합니다.\n (1) 어떠한 물체 또는 매질(object)의 진동(vibration)으로 인해 공기 입자들이 밀고 당겨지는 반복적인 과정(oscilation)에서 생긴 파동(wave) (2) 공기의 압력이 낮아지면 빈 공간이 생기면서 다른 입자들로 채워지고, 압력이 높아지만 입자들을 밀어내는, 밀고 당기는 반복적인 연쇄 작용(oscillation)으로 인해 생기는 파동(wave)  그리고 위에서 정의한 파동은 아래와 같은 파형(waveform)으로 나타낼 수 있습니다.\n   \u0026amp;nbsp\n2. 소리 데이터의 표현 2.1. 파형의 요소 파형을 통해서 우리는 다음과 같은 정보를 파악할 수 있습니다.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(1) - Waveform, ADC"},{"content":"항상 내 맘 같지 않은 기술 블로그 개발이 비교적 익숙하지 않은 데이터분석가 또는 데이터사이언티스트가 Github Pages를 활용하여 기술블로그를 운영하기 위해서는 몇 가지 난관이 있습니다. 자료를 찾아보면 주로 Jekyll, Hugo, Hexo, Gatsby.JS 와 같이 낯설고 어려운 프레임워크을 사용해야 합니다. 튜토리얼은 간신히 따라갔다 하더라도, 기존의 테마를 내 입맛에 맞게 커스터마이징을 하거나 기능을 추가하기 위해서는 html, CSS 또는 NodeJS 같은 프레임워크를 알아야 합니다. 마음에 드는 테마가 있어도, 오랜 기간 관리가 되지 않아 Latex 엔진이 제대로 작동하지 않는 경우가 생기기도 하죠. 번거로웠던 적이 많았습니다. 그냥 조용히 다른 쉬운 플랫폼을 선택할 걸 하는 후회가 드는 순간도 있었습니다. 진짜 많은거 바라지 않으니, 가장 심플하게 기본만 딱 하는 테마는 없을까 찾아보기도 했구요. 이러한 와중에 fastpages를 접하게 되었습니다.\n 이번에는 fastpages    Source: https://github.com/fastai/fastpages   장점 fast.ai 에서 개발한 fastpages는 Jekyll을 기반으로 하는 툴입니다. 제가 생각하는 fastpages의 장점은 다음과 같습니다.\n 디자인이 군더더기 없이 깔끔하다. 부족한 것도 없지만 더 덧붙일 것도 없다. 따라서 코드를 수정할 필요가 없다.  Latex, Syntax highlighting 모두 깔끔하다. Altair, Plotly 와 같은 interactive visualization도 embedd 할 수 있다.   html을 쓰지 않아도 된다.  기존의 방식과 같이 markdown(.md) 파일로 블로그 포스트를 작성할 수 있다. jupyter notebook(.ipynb) 파일까지도 그대로 블로그 포스트로 변환해준다. (!!!!!) 아마 많이 사용할 일은 없겠지만, word 파일까지도 가능하다고 한다.   다른 프레임워크 기반 명령어가 따로 필요 없이, 그저 add, commit, push 명령어만으로 포스트를 간편하게 업데이트할 수 있다. 나머지는 Github Action을 배포과정 알아서 진행하기 때문.  단점 단점은 \u0026hellip; 아직까지는 없습니다. 단지, {github username}.github.io 형태의 주소는 지원하지 않고, {github username}.github.io/{user가 생성한 fastpages 블로그 레포지토리 이름} 을 통해서만 블로그가 생성된다는 점이 한계인 것 같네요.\n 한번 만들어봅시다 fastpages 블로그용 레포지토리 생성하기.  fastpages 공식 레포지토리에서 Use this template 버튼을 클릭하고 새로운 레포지토리를 생성합니다. 이때, 레포의 이름은 아무것이나 정해되 상관 없되, {계정명}.github.io는 피해야 합니다.  Generate a copy of this repo by clicking on this link. Name your repo anything you like except {your-username}.github.io.\n         \u0026amp;nbsp\n 잠시 기다리면, \u0026ldquo;Initial Setup\u0026quot;이라는 새로운 Pull Request가 자동으로 생성된 것을 확인할 수 있습니다.  GitHub Actions will automatically open a PR on your new repository ~ 30 seconds after the copy is created. Follow the instructions in that PR to continue.\n      \u0026amp;nbsp\nPR 가이드 따라가기  다음 링크에서 prive key와 public 키를 생성합니다. 이때, 옵션은 RSA와 4069를 선택 후 \u0026ldquo;Generate SSH-Keys\u0026rdquo; 버튼을 클릭합니다.  Create an ssh key-pair. Open this utility. Select: RSA and 4096 and leave Passphrase blank. Click the blue button Generate-SSH-Keys.\n      \u0026amp;nbsp\n 두번째 주어진 링크에서 새 \u0026ldquo;New repository secret\u0026rdquo; 버튼을 클릭하고, Value 입력 칸에 앞서 생성한 Private key 전체를 복사하여 붙여넣습니다. Name 입력 칸에는 \u0026ldquo;SSH_DEPLOY_KEY\u0026quot;라고 입력하고 저장합니다.  Navigate to this link and click New repository secret. Copy and paste the Private Key into the Value field. This includes the \u0026ldquo;\u0026mdash;BEGIN RSA PRIVATE KEY\u0026mdash;\u0026rdquo; and \u0026ldquo;\u0026ndash;END RSA PRIVATE KEY\u0026mdash;\u0026rdquo; portions. In the Name field, name the secret SSH_DEPLOY_KEY.\n   \u0026amp;nbsp\n  세번째 링크에서는 \u0026ldquo;Add deploy key\u0026rdquo; 버튼을 클릭하고, 앞서 생성한 Public key를 복사하여 붙여넣습니다. 이름은 아무렇게나 지정해도 된다고 합니다. 그리고 제일 아래 \u0026ldquo;Allow write access\u0026rdquo; 박스를 꼭 체크합니다.\n Navigate to this link and click the Add deploy key button. Paste your Public Key from step 1 into the Key box. In the Title, name the key anything you want, for example fastpages-key. Finally, make sure you click the checkbox next to Allow write access (pictured below), and click Add key to save the key.\n      \u0026amp;nbsp\n  마지막으로 PR을 merge합니다. 이후 fastpages 블로그가 배포되는 과정은 Github actions에서 아래와 같이 확인할 수 있습니다.         \u0026amp;nbsp\n 완료되면, {github username}.github.io/{레포지토리 이름} 에서 fastpages 기반의 블로그가 배포되었음을 확인할 수 있습니다.     \u0026amp;nbsp\n이것만 알아도 블로깅 문제 없다! (나머지는 저도 아직 잘 몰라요. 몰라도 크게 상관 없더라구요 \u0026hellip;)\n _config.yml  블로그의 이름, Latex사용 여부, 미리보기 여부, 태그 보여주기 여부 등을 설정할 수 있는 파일입니다. 자신의 SNS계정 버튼도 추가할 수 있고, description, pagination의 갯수 등의 사항들도 설정할 수 있습니다.   index.html  블로그 메인 페이지에 보여지는 컨텐츠를 작성하는 파일입니다. 저는 아무것도 입력하지 않았기에 디폴트로 작성되어 있던 fastpages 소개 텍스트를 삭제했습니다.   _pages/about.md  블로그의 자기소개 페이지 About페이지의 내용을 작성하는 파일입니다. markdown 포맷으로 작성하면 됩니다.   포스트를 저장하는 디렉토리는 아래 종류에 따라 달라지므로 디렉토리를 구분해서 작성하시면 됩니다.  _notebooks/ _posts/ _word/ images/    주의! 포스트를 작성할 때는 notebook, markdown, word 파일 포맷에 상관 없이 무조건 YYYY-mm-dd-{아무 이름} 형태로 작성되어야 fastpages가 이를 인식하고 파일을 html로 변환합니다.\n\u0026amp;nbsp\n블로그 로컬에서 확인하기 배포 또는 포스트 최종 업로드 전, 내가 작성한 포스트가 블로그에 잘 보여지는 지 확인하고 싶을 때가 있습니다. 이때는 make server 명령어를 칩니다. localhost:4000 에서 블로그가 생성되었음을 확인할 수 있습니다. (단, docker가 작동 중에 있을 때만 가능합니다.)\n    마무리 하며 기술 블로그를 초반에 시작했을 때는 정말 의욕은 많이 앞섰지만, 늘 \u0026ldquo;이 디자인은 마음에 들지 않아, 이건 이런 기능이 없어, 이건 계속 오류가 생겨\u0026quot;와 같은 쓸데 없는 생각들이 불쑥불쑥 튀어나왔던 것 같습니다. 하지만, 이번 기회에 fastpages가 아주 깔끔하면서도 만족스러워 다짜고짜 블로그를 바꿔버렸습니다. 이제 과거에 작성했던 글들도 슬슬 옮겨놔야할 것 같네요! 새로운 술은 새 포대에 담으라고 했던가요? 새로운 블로그를 만들었으니 글도 술술 써지기를 기대해봅니다!\n Reference  https://github.com/fastai/fastpages  ","permalink":"https://lucaseo.github.io/posts/2020-11-29-fastpages/","summary":"항상 내 맘 같지 않은 기술 블로그 개발이 비교적 익숙하지 않은 데이터분석가 또는 데이터사이언티스트가 Github Pages를 활용하여 기술블로그를 운영하기 위해서는 몇 가지 난관이 있습니다. 자료를 찾아보면 주로 Jekyll, Hugo, Hexo, Gatsby.JS 와 같이 낯설고 어려운 프레임워크을 사용해야 합니다. 튜토리얼은 간신히 따라갔다 하더라도, 기존의 테마를 내 입맛에 맞게 커스터마이징을 하거나 기능을 추가하기 위해서는 html, CSS 또는 NodeJS 같은 프레임워크를 알아야 합니다. 마음에 드는 테마가 있어도, 오랜 기간 관리가 되지 않아 Latex 엔진이 제대로 작동하지 않는 경우가 생기기도 하죠.","title":"[KR] fastpages에서 블로그 시작하기"},{"content":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp\n점검해보자 1. 회사일만 하지 않기 모호하지만, 매우 중요한 항목이다. 작년에는 회사에서 기존에 하던 일과 말도 안 되는 정부과제, 그리고 갑작스러운 혁신금융선정의 삼위일체가 합심하여 나에게 전신마사지를 해주는 바람에 야근과 주말출근을 밥 먹듯이 했다. 결국 기존에 받는 연차 15일과 야근, 주말 근무에 대한 대체휴무가 쌓여, 연말에는 잔여 연차가 23일이 되었다(??) 1년 간 쉰 기억이 나지 않고, 기존 연차보다 잔여연차가 더 많은 상태로 1년을 마무리 하게 되어, 이게 대체 누구를 위한 무슨 짓인가 싶기도 했다. (모든 게 힘듦으로 가득하던 시절 그래서 슬기롭게 내 생활을 챙기기로 했다. 그렇다고 아주 여유롭진 않지만, 너무 힘들지는 않을 만큼, 어떻게 보면 좀 노하우가 생긴 것 같은 삶을 살고 있다.\n\u0026amp;nbsp\n2. 데이터 사이언스 대회 도전   데이콘: 원자력 발전소 상태 판단 알고리즘 대회. 데이콘에서 주관하는 원자력 발전소 상태 판단 대회에 참가했다. 자세한 후기는 이미 블로그에서도 작성한 바 있다. 대회에서 중간에 포기하지 않고 꾸준히 참여했다는 점에서 소기의 목적을 달성했다고 생각하고, 좀 더 열심히 했다면 더 좋은 성적을 거둘 수 있었을 것이라 생각한다.\n  카카오 아레나: Melon Playlist Continuation. 사실 카카오 아레나의 멜론 플레이리스트 추천 대회는 정말 많은 관심을 가지고 있는 분야인 만큼 의욕이 매우 높았던 대회였다. 추천시스템에 대한 개념도 잘 없었기 때문에, 참가와 동시에 추천시스템에 대한 공부를 차근차근 하고자 했다. 하지만 참가만 걸어 놓은 상태로 그 이상의 제출은 해보지 못 하고 대회가 마무리 되어 많은 아쉬움이 남는다. 애초에 제출 이전부터 선행되어야 하는 추천시스템에 대한 공부가 지지부진 했기 때문에, 내가 생각하기에도 변명의 여지가 없는 좀 한심한 결과가 나오게 됐다.\n  \u0026amp;nbsp\n3. 간단한 웹어플리케이션 구축 연초부터 사이드 프로젝트로 여러가지 역할을 수행할 수 있는 유연한 웹어플리케이션을 구축하고자 하는 와중에 Streamlit을 경험하고, 이것저것 시도해보게 되었다. 글또에서도 관련된 포스팅을 작성한 바 있다. 프론트엔드에 대한 걱정 없이 순수 파이썬으로 매우 간편하게 개발할 수 있기 때문에, 실제로 직장 업무에서도 개발팀이 참고할 수 있도록 데이터 QC, 레이블링 기능에 대한 프로토타입이나 대시보드 등을 만들어서 사내 공유하거나 함께 사용하고 있다.\n\u0026amp;nbsp\n4. 독일 가족 방문하기 마지막으로 독일을 방문한건 2018년. 올해만큼은 꼭 시간을 내서 누나의 가족들이 사는 독일을 방문하고자 했지만 망할 COVID-19 때문에 패스하게 되었다. 내가 가야 그나마 조카들이 한국말을 덜 까먹는데, 이렇게 조카들의 한국말은 더욱 뒷걸음질 치는건가 싶다. 언제쯤 가능하게 될까. 이건 기약이 없어서 더욱 안타까운 항목이다.\n\u0026amp;nbsp\n5. 글또 솔직히 말하자면, 글또 생활은 생각보다 어렵다. 글을 잘 쓰는 것도 어렵고, 내가 잘 모르는 내용에 대해 정독하고 피드백이나 감상을 남기는 것도 쉽지는 않다. 모자라서 그런건 아니고 아마 처음해봐서 그럴 수도 있다. 그래서 더더욱 차감 없는 슬기로운 글또 생활을 영위하고자 노력하고 있다. 꾸역꾸역 \u0026hellip; 이라는 의성어는 좀 뉘앙스가 그렇지만, 어쨌든 그런 비슷한 상황이다. 나쁘게 말하면 꾸역꾸역이고 좋게 말하면 조금조금씩 발전을 꾀하는 존버라고 할 수 있겠다. 아무튼 현재까지는 예치금 차감은 없다. 요태까지 그래와고 아패로도 개속.\n\u0026amp;nbsp\n(+) 지난 3월 방송통신대 정보통계학과 3학년에 편입했다. 해외 대학원을 알아보던 중, 비전공자인 나는 데이터사이언스와 직접적인 연관이 있는 학과목의 수강학점이 없기 때문에 지원부터가 제한이 있다는 걸 알게 되어서, 이 부분을 타개해나갈 방법을 찾고 있었다. 경영과 연계된 MIS관련 대학원, 부분 수강, MOOC를 통한 크레딧 등 다양한 방법을 알아봤지만, 역시 편법은 없었고, 3학년으로 편입하여 정식 대학과목을 수강하는 것이 가장 확실했다. 직장생활과 병행하는 방법은 방송통신대 뿐이었는데, 한 학기 등록금이 30만원대로 매우 저렴했기 때문에, 부담 없이 경험해보는 셈 치고 편입을 결정했다.\n방송통신대는 대학교 특성상 COVID-19로 인한 타격도 1도 없이 매우 정상적으로 학사일정이 진행되었다. (1은 있었다고 볼 수도 있다. 기말고사만큼은 오프라인으로 이루어지는데, 이 조차 과제물로 전환되었기 때문.)\n계획에 없었던 편입이고 주변 사람들도 우려를 많이 표했지만, 직장생활 병행과 대학원 진학을 위한 장기 계획의 일부분인 만큼, 잃은 것보다는 얻은 것이 많은 한 학기였다. 방송통신대에 대한 자세한 후기는 이번 2학기가 끝난 이후 포스팅할 계획이다.\n\u0026amp;nbsp\n마무리 하며 난 나름 뭔가 많이 했다고 생각했는데, 막상 돌아보니 매우 소박했다는 걸 느꼈다. 동시에 남은 5개월과 2021년에 대한 구상도 할 수 있게 되어 의미있는 회고였다. 역시 글또를 하지 않았다면 이런 글도 남기지 않았을 거란 생각이 들기도 한다. 남은 5개월은 또 얼마나 빨리 지나갈까. 적절한 타이밍에 남은 연차를 마저 소진하고 (잘 휴식하겠다는 뜻) 글또도 잘 마무리 할 수 있는 (예치금 차감 당하지 않겠다는 뜻) , 그리고 방송통신대 두번 째 학기도 초심 잃지 않고 수강할 수 있는 (A+를 받겠다는 뜻) 하반기가 되었으면 한다.\n","permalink":"https://lucaseo.github.io/posts/2020-08-02-review-2020-1st-half/","summary":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp","title":"[KR] 2020년 상반기가 지났다"},{"content":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)\n대회 개요    수력 원자력 공사인데 배경은 풍력 발전 ... ?   \u0026amp;nbsp\n원자력발전소 상태 판단 대회는 한국수력원자력(주)에서 제공한 발전소 모의 운전 데이터를 통해 원자력 발전소의 상태를 판단하는 것이 태스크로 주어진다.\n평가지표 평가 지표는 Log loss 이다. Log loss 값은 0 ~ 1 사이로 산출되며, 낮고 0에 가까울 수록 모델의 예측력이 좋음을 의미한다. (데이콘 측 평가지표 설명 영상)\n$$ \\text{logloss}(\\cdot) = \\frac{-1}{N}\\sum_i^N \\sum_j^M y_{ij} \\log{p_{ij}} $$\n데이터셋 원자력 발전소 모의 데이터는 기본적으로 828개의 발전소 운전 Train 데이터 파일과 각 파일에 부여된 198가지 상태 레이블이 매핑된 Label 파일이 주어진다. 압축을 해제하면 총 81GB에 달했다.\n![]({{ site.baseurl }}/images/2020-07-05-review-dacon-nuclear-competition/2.png)\n   \u0026amp;nbsp\n각각의 파일에 저장된 Train 데이터셋 위의 그림과 같으며, 발전소가 10분 동안 작동한, 즉 1행 당 1초 즉, 600행으로 이루어진 데이터가 주어진다. 모든 데이터셋은 발전소의 상태가 변하기 전 디폴트 상태_A(999) 와 상태가 변한 후 상태_B 데이터를 담고 있으며, 상태_A에서부터 시작된다. 데이터는 0초에서 15초 사이에서 상태가 변하기 시작한다. 따라서 데이터 상태의 변화가 0초에서 발생한다는 말은 상태_A가 없는 좌측의 데이터셋과 같고, 그 이외에는 우측과 같다고 보면 된다.\n   \u0026amp;nbsp\n다만 위의 그림과 같이, 실제로는 몇 초부터 상태가 변하는지에 대한 정보가 주어지지 않기 때문에, 각 데이터셋이 좌측과 같은지 우측과 같은지는 한 눈에 판단이 어렵다.\n데이터 전처리 EDA 과정에서 의미 있는 인사이트를 도출해내지 못 했다. 그리고 대회 경험이 적고 시간이 촉박했던 관계로, 바로 전처리에 돌입했다.\n Label이 999 인 경우 제외한다. 10분 간의 운전 데이터 기록 컬럼 내 unique한 값이 \u0026lt; 10 인 컬럼은 제외한다. 데이터셋 중 str타입의 데이터가 발생할 경우 NaN 치환한다. 마지막으로 NaN 데이터는 0으로 채운다. Train / Eval 데이터셋은 3:1의 비율로 분리한다.  #1 train = train[train[\u0026#39;label\u0026#39;]!=999].reset_index(drop=True) train_label = train.label train = train.drop([\u0026#39;id\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;label\u0026#39;], axis=1) #2 with open(\u0026#39;filter_col.txt\u0026#39;, \u0026#39;r\u0026#39;) as filehandle: list_ = filehandle.readlines() list_ = [col.replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) for col in list_] train= train[list_] # 3 for col in train.columns: if train[col].dtype != \u0026#39;float64\u0026#39;: train[col] = pd.to_numeric(train[col], errors=\u0026#39;coerce\u0026#39;) #4 train = train.fillna(value=0, axis=1) #5 X_train, X_valid = train_test_split(train, test_size = .25, random_state=42) y_train, y_valid = train_test_split(train_label, test_size = .25, random_state=42) 모델 모델은 LightGBM을 선택했다. Tabular dataset의 분류 문제에는 나무모형 기반의 모델이 가장 적합했고, LightGBM, XGBoost, Random Forest를 초기 테스트 한 결과 LightGBM이 가장 빠르고 성능이 좋았기 때문이다.\n부스팅 방법으로는 가장 기본적인 Gradient Boosted Decision Tree를 선택했다. 또한 데이터셋이 커서 연산이 큰 문제와 오버피팅을 방지하기 위해 bagging_fraction, feature_fraction 파라미터의 설정으로 데이터셋의 행과 열을 0.5 비율로 고정하여 학습 중 샘플링할 수 있도록 했다.\n그 외 objective, num_class, metric과 같이 대회의 목적에 맞게 변경한 파라미터를 제외하고는 default를 가져온 것들이 대부분이다.\nimport lightgbm as lgb X_train = X_train.to_numpy() X_valid = X_valid.to_numpy() y_train = y_train.to_numpy() y_valid = y_valid.to_numpy() train_data = lgb.Dataset(X_train, label=y_train) #, feature_name=X_train.columns) valid_data = lgb.Dataset(X_valid, label=y_valid) #, feature_name=X_valid.columns) param = { \u0026#39;objective\u0026#39;: \u0026#39;multiclass\u0026#39;, \u0026#39;num_class\u0026#39;: 198, \u0026#39;boosting\u0026#39;:\u0026#39;gbdt\u0026#39;, \u0026#39;num_leaves\u0026#39;:32, \u0026#39;max_depth\u0026#39;:20, \u0026#39;min_data_in_leaf\u0026#39;:20, \u0026#39;metric\u0026#39;:\u0026#39;multi_logloss\u0026#39;, \u0026#39;learning_rate\u0026#39; : 0.01, \u0026#39;verbose\u0026#39; : -1, \u0026#39;bagging_freq\u0026#39; : 1, \u0026#39;bagging_fraction\u0026#39; : 0.5, \u0026#39;feature_fraction\u0026#39; : 0.5, } evals_result={} num_round = 2000 lgbst = lgb.train(params=param, train_set=train_data, num_boost_round=num_round, valid_sets=[valid_data], evals_result=evals_result, early_stopping_rounds=1000, verbose_eval=10) lgbst.save_model(\u0026#39;model_lgb.txt\u0026#39;, num_iteration=lgbst.best_iteration) 결과 결과는 가채점 기준 36위 / 201팀, 최종 데이터셋 기준 채점 및 중복 및 부정 제출 등의 여부가 판결 뒤 산출된 최종 순위는 16위 / 187팀를 기록했다. 전체 참가팀만 놓고 보면 1091팀이지만, 실제로 제출한 팀은 20%에 그친 것을 확인할 수 있었다.\n   소감 무엇보다 아쉬운점은 초반의 의욕과는 달리, 데이터사이언스 대회의 best practice를 실습해보지 못 했고, 제출과 점수에 급급한 채로 마무리 했다는 점이다. 시간 부족과 의사소통 부재로 EDA 깊이 있게 하지 못 했고, Hyperparameter tuning과 grid search를 제대로 시행하지 못 햇다.\n마지막으로, 이번 원자력발전소 상태 판단 대회 참가는 데이터사이언스 관련 대회 경험을 쌓기 위해서였지만, 한편으로는 전혀 접하지 못 했던 원자력발전소 관련 데이터를 접하고 발전소 관련 도메인을 조금이나마 얻기 위함이기도 했다. 그러나 데이터셋의 컬럼은 모두 비식별 처리가 되어 있어 무엇을 의미하는지 데이터의 특성과 정보에 대한 접근이 불가했다는 점이 대회를 참가하는 와중에 흥미가 조금 깎이게 된 요인이 되지 않았나 하는 생각이 든다.\n어쨌거나 저쨌거나 완주를 했고, 상위 10% 내 라는 기대도 하지 않았던 성적으로 마무리를 했기 때문에 여기에 의의를 두며, 다음에 참가하는 데이터사이언스 대회는 이번에 아쉬웠던 점들이 꼭 보완될 수 있도록 다짐을 해본다.\n","permalink":"https://lucaseo.github.io/posts/2020-07-05-review-dacon-nuclear-competition/","summary":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)","title":"[KR] 데이콘 원자력발전소 상태 판단 대회 후기"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다. (영상)\n원본의 Figure. The Wagon Wheel Effect에서 하단의 sampling rate 슬라이더를 조절하며 왼쪽의 시계방향으로 회전하는 물체와, 오른쪽의 sampling rate에 따른 스냅샷(혹은 샘플)를 관찰해보자\n   \u0026amp;nbsp\n직관적으로 봤을 때, 왼쪽과 마찬가지로 오른쪽 스냅샷도 시계방향으로 회전하는 듯한 모션을 취하기 위해서는 최소한 왼쪽에서 한번 회전하는 동안 2번 이상의 스냅샷을 찍어야 한다. 만약 sampling rate을 엄청 낮게 설정한다면 스냅샷은 기존의 회전 방향과는 반대로 반시계 방향으로 찍히게 된다.\n회전 방향이 결정되지 않고 정지해 있는 듯한 특이한 케이스도 있다. 만약 sampling rate을 한바퀴당 1번으로 설정한다면 스냅샷에는 움직임이 없을 것이고, sampling rate을 한바퀴당 2번으로 설정한다면 스냅샷은 그냥 앞뒤로만 움직이기 때문에 진행 방향을 유추할 수 없게 될 것이다.\nSine Wave Aliasing : Multiples of the sampling rate Sive wave에는 다음과 같은 법칙이 있다.\n sampling rate이 \\(SR\\) 헤르쯔와 정수 \\(K\\)가 주어졌을 때,\n\\(F\\) 라는 frequency를 지닌 sine wave가 있고, \\(F+(k * SR)\\) 의 진동수를 지닌 sine wave를 샘플링했다고 할 때, 둘은 서로 구별이 가능하지 않다.\n 예를 들어, Sampling rate이 6Hz라고 했을 때, 다음 두 그룹의 sive wave는 서로 구별이 가능하지 않다.\n 샘플링을 거친 1Hz의 sine wave 샘플링을 거친 다음 세 가지의 sine wave  \\( 1+(1 * 6) = 7Hz \\) \\( 1+ (2 * 6) = 13Hz \\) \\( 1+(3 * 6) = 19Hz \\)    (사실 여기까지는 조금 이해가 안 갔는데 \u0026hellip;)\n원본의 Figure. All Sampled Sine Waves Have Aliases을 보면 더욱 직관적이다. Figure 1.의 Time domain 그래프는 1Hz sine wave(파랑색)와, 그 외 sine wave(회색)을 비교하고 있다. 샘플링을 거치고 나면(Frequency domain 그래프), 샘플링이 표현하는 부분은 파랑색과 회색 다 일치하기 때문에, 샘플링을 한 뒤에는 각각 어떤 frequency 였는지 구분을 할 수 가 없게 된다.\n   \u0026amp;nbsp\n이 말인즉슨, 샘플링을 거친 sine wave는 무한대의 alias를 가진다는 것이다. 그저 기존의 frequency에 sampling rate의 배수만 더해주면 기존 frequency에 대한 새로운 alias가 형성된다. 따라서 이 법칙은, 어떠한 신호라도 샘플링을 거치면 다른 샘플링 신호와 구별이 가능하지 않는 상황이 올 수 있다는 것을 의미한다.\n","permalink":"https://lucaseo.github.io/posts/2020-06-20-dsp-basic-s01-9/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Wagon Wheel Effect \u0026 Aliasing"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.\n 특정 신호 내 가장 높은 frequency를 알고 있다면, 샘플링 주기를 그 두배로 설정하면 된다는 뜻이다. 예를 들어 300Hz sine wave 를 샘플링하려 한다면, 우리는 두배 이상 즉, 600Hz 를 샘플링해야 한다. 반대로 샘플링 주파수가 두배보다 작을 경우, 간섭이 일어나며 앞에서 언급한 aliasing(참고)이 발생하게 된다.\n신호 위주가 아닌 sampling rate 위주로 법칙을 정하면 다음과 같다.\n 샘플링 주기 FS hertz에 대하여, FS/2 혹은 그 이상인 frequency를 가진 신호는 적절하게 샘플링할 수 없다.\n 특히 sampling rate의 절반 즉, FS/2 라는 값은 Nyquist Limit 또는 Nyquist Frequency라고도 불리며, 샘플링 법칙을 논할 때 매우 중요하게 다뤄지는 값이다.\n원글의 Figure 1.(링크)의 Sampling period 값을 조절하면서 직관적으로 이해해보자. Sampling rate이 16Hz일 경우, FS/2는 8이다. 모든 네가지 sine wave의 경우 8을 넘지 않으므로 샘플링이 적절하게 가능하다.\n   \u0026amp;nbsp\n하지만 sampling rate을 4Hz으로 설정하면 FS/2의 값은 2이다. 따라서 2Hz, 3Hz, 4Hz의 sine wave는 제대로 샘플링 되지 않는다.\n   \u0026amp;nbsp\n지난 글에서 인간은 20Hz ~ 20,000Hz 사이의 소리만 들을 수 있다고 말한 바 있다. 따라서 샘플링 법칙에 따라 인간이 들을 수 있는 범위 내에서 음악의 sampling rate를 정하고자 하면, 20,000의 두배 즉 40,000Hz 이상이 되어야 한다. 오디오와 음악의 주파수가 40,000Hz 근처(정확하게 말하자면 44,100Hz)인 것은 이 이유 때문이다.\n\u0026amp;nbsp\nNyquist Frequency를 초과하는 경우 샘플링 법칙은 Nyquist Frequency(샘플링주기의 절반)에 한해서는 어떤 신호든 정확히 샘플링할 수 있고, 반대로 샘플링하고자 하는 샘플이 Nyquist frequency보다 높은 frequency를 가지고 있다면, 이 샘플링이 제대로 이루어지지 않는다는 것을 알게 되었다. 그렇다면 Nyquist Frequency를 초과하는 신호는 샘플링이 이루어지지 않는 이유에 대해서 알아보자.\n원글의 Figure 1.(링크)에서 sampling rate은 24Hz이고 Nyquist frequcny는 24Hz의 절반인 12Hz이다. 플레이 했을 때, 신호가 Nyquist frequency를 지나면, 신호의 샘플들은 하나의 파랑색의 파형 뿐 아니라 새롭게 생성된, 점점 감소하는 frequency 형태의 회색의 파형까지도 sampling 결과를 통해 표현이 가능하게 된다.\n   \u0026amp;nbsp\n이렇게 신호의 frequency를 제한하지 않고 Nyquist limit을 넘도록 허용하면, 샘플링된 신호는 Nyquist frequency가 투영된 새로운 sine wave를 표현하게 된다. 결국 실제로는 존재하지도 않는 회색 신호를 샘플링에 포함시키게 되는 결과를 낳게 되는 것이다.\n다음 포스팅에서 alias에 대한 다른 예시를 더 깊게 다루어서 정리를 할 예정이다.\n Additional material https://youtu.be/5wyYgy6LPyQ\n","permalink":"https://lucaseo.github.io/posts/2020-06-07-dsp-basic-s01-8/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 나이퀴스트 샘플링 법칙 (Nyquist Sampling Theorum)"},{"content":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.5 ~ 5점 사이로 평가를 내릴 수 있다. 넷플릭스에서 보유한 사용자 평가 데이터에는 MAP가 사용될 수 있지만, 왓챠에서는 MAP가 적절하지 않을 것으로 생각된다.\n반면에 nDCG(Normalized Discounted Cumulative Gain)의 경우 복수의 컨텐츠가 relevance를 가지고 있다고 하더라도, 그 정도(점수)에 따라 어떠한 컨텐츠가 더 관련있고, 덜 관련 있는지를 평가할 수 있다. 대부분의 검색과 추천시스템의 경우 다수의 사용자들은 1, 2페이지 또는 상위의 리스트만 참조할 것이기 때문에 상위 리스트 사이에서 변별력을 갖춰야 하는 경우 nDCG를 통한 평가가 설득력을 얻는다.\n\u0026amp;nbsp\nCumulative Gain (CG) nDCG에서는 우선 CG이란 개념이 등장한다. CG는 Cumulative Gain 이라는 이름에서도 알 수 있듯, 전체 추천된 리스트에 대하여 gain의 총 합을 구한 것이다.\n$$CP_{p} = \\sum_{i=1}^{p} rel_{i}$$\n \\(p\\) : 추천된 아이템 \\(rel_{i}\\) : i번 째 아이템의 relevance 정도. Gain 이라고 한다.  예시) 추천시스템의 결과와 Relevance\n   Rank Relevance     1 3   2 3   3 3   4 4   5 2   6 2    $$CG = 17$$\n\u0026amp;nbsp\nDiscounted Cumulateive Gain (DCG) DCG 에서는 각 추천된 아이템의 relevance를 log함수로 나누어 값을 구한다. log 함수로 나누어주는 부분은, 랭킹의 위치에 따른 페널티를 주는 효과를 가진다. 순위의 값이 클 수록(즉, 순위가 낮을 수록) DCG의 값은 작아진다. 하지만, 높은 순위의 경우 간격이 크고, 낮은 순위의 경우 실제 체감하는 차이는 낮다.\n$$DCG_{p} = \\sum_{i=1}^p \\frac{rel_i}{log_2(i+1)} = rel_1 + \\sum_{i=2}^p \\frac{rel_i}{log_2i}$$\n예시) 각 추천된 컨텐츠 당 Discounted Gain\n   Rank Relevance Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\)   2 3 \\(3 / \\log_2(2+1) = 1.89\\)   3 3 \\(3 / \\log_2(3+1) = 1.5\\)   4 4 \\(4 / \\log_2(4+1) = 1.72\\)   5 2 \\(2 / \\log_2(5+1) = 0.77\\)   6 2 \\(2 / \\log_2(6+1) = 0.71\\)    $$DCG = 9.59$$\n\u0026amp;nbsp\nNormalized DCG (nDCG) DCG는 현재 추천시스템이 추천한 결과에 대한 상태를 보여주는데, DCG만 놓고 볼 경우 추천된 아이템의 갯수에 따라 DCG가 다를 수 있으므로, 이를 0~1사이의 값으로 정규화 해줄 필요성이 있다. 따라서 현재 추천된 리스트의 결과에 기반한 DCG를 현재 추천된 결과의 가장 이상적인 형태를 가정했을 때의 DCG(ideal DCG, iDCG)로 나누어서 정규화한다.\n사용자가 컨텐츠를 평가하지 않은 경우와, 관련성이 아예 없는 경우 manual적으로 값을 0으로 설정하거나, 적절하게 imputation을 취해주어야 한다는 취약점이 있다. 하지만, nDCG는 relevance가 등급이나 범위로 매겨지거나(graded relevance), 이분법적인 경우(binary relevance) 둘 다 평가가 가능하다. 또한 log함수를 통해 순서에 대한 가중치가 주어지므로 추천시스템에 적용하기 매우 적절한 평가지표라고 할 수 있다.\n$$IDCG_p = \\sum_{i=1}^{|REL_p|} \\frac{2^{rel_i} - 1}{log_2(i+1)}$$\n$$nDCG_p = \\frac{DCG_p}{IDCG_p}$$\n예시) Ideal Relevance와 Discounted Gain\n   Rank Relevance Discounted Gain Ideal Relevance Ideal Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\) 4 4   2 3 \\(3 / \\log_2(2+1) = 1.89\\) 3 1.89   3 3 \\(3 / \\log_2(3+1) = 1.5\\) 3 1.5   4 4 \\(4 / \\log_2(4+1) = 1.72\\) 3 1.16   6 2 \\(2 / \\log_2(6+1) = 0.71\\) 2 0.71   5 2 \\(2 / \\log_2(5+1) = 0.77\\) 2 0.77    $$ nDCG_p = \\frac{DCG_p}{IDCG_p} = \\frac{9.59}{10.03} = 0.95 $$\nReference  위키피디아 MRR vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them  ","permalink":"https://lucaseo.github.io/posts/2020-05-10-normalized-discounted-cumulative-gain/","summary":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.","title":"[KR] 추천시스템의 평가 지표 : nDCG"},{"content":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)\n추천 시스템을 통해 추천되는 아이템의 경우 추천의 정도가 동일하지 않다. 대부분의 추천 결과는 다음처럼 나올 수 있다고 생각해 볼 수 있다.\n 1순위 : 가장 관심을 가질만한 것.\n2순위 : 그 다음 차선책으로 관심을 가질만한 것.\n3순위 : 그 다음으로 사용자가 관심을 가질만한 것.\n4순위 : 또 그 다음 \u0026hellip; \u0026hellip;\n \u0026amp;nbsp\nMean Average Precision (MAP) 은 순서 또는 순위를 감안하는 부분을 반영하여 추천 시스템의 성능을 평가하는 지표로서, 과거 캐글의 Stander Product Recommendation, 카카오아레나의 브런치 사용자를 위한 글 추천 대회 등 추천 시스템 관련 컴퍼티션에서 채점 방식으로 적용되었다. 특히 분류 문제에서 흔히 언급되는 Precision과 Recall이 적용된 성능평가 방법으로, 아주 낯설지는 않다.\n\u0026amp;nbsp\nPrecision \u0026amp; Recall     Predict Positive Predict Negative     Actual Positive True Positive False Negative   Actual Negetave False Positive True Negative    MAP에 대한 개념는 Precision과 Recall에서부터 시작한다. 일반적으로 위와과 같이 confusion matrix가 있다고 할 때, Precision과 Recall은 다음과 같다. (더 자세한 설명은 링크를 참조하도록 하자)\n$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$$\n$$\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$$\n\u0026amp;nbsp\n추천시스템 관점에서의 Precision \u0026amp; Recall 추천시스템에서는 Precision과 Recall을 다음과 같이 해석할 수 있다. 추천시스템에서는 분자 부분을 relevant(관련있는) 라고 표현하기도 한다.\n     Precision 또는 \\(P\\):\n 추천한 아이템 중, 실제로 사용자의 관심사와 겹치는 아이템의 비율 \\(\\text{Precision} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{Total items from recommendation}}\\)    Recall 또는 \\(r\\):\n 실제로 사용자가 관심을 가진 아이템 중, 추천된 아이템이 겹치는 비율 \\(\\text{Recall} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{User\u0026rsquo;s interest}}\\)    \u0026amp;nbsp\nCutoff (@K) MAP에서는 Cutoff의 개념이 등장한다. Cutoff는 \u0026ldquo;잘라낸다\u0026quot;는 뜻으로, 쉽게 말하면 \u0026ldquo;상위 K개만 고려하고 그 아래로는 쳐내기\u0026rdquo; 라고 이해하면 된다. Cutoff를 가질 경우에는, @K 를 덧붙여서 표기한다.\n어떠한 사용자의 기록을 통해서 자동차 용품와 관련된 아이템을 추천한 결과가 다음과 같다고 하자.\n   순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 운전자 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    다음 예시의 추천시스템의 결과에 대하여 \\(k\\)개 Cutoff를 적용하여 Precision을 구한다면, 이를 Precision@K라고 한다. Precision@K는 Cutoff에 따라 달라질 수 있다.\n\u0026amp;nbsp\n Cutoff k=10인 경우 \\(\\rightarrow P(k=10) = 0.6\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 자동차 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    \u0026amp;nbsp\n Cutoff k=3인 경우 \\(\\rightarrow P(k=3) = 1\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답    \u0026amp;nbsp\n Cutoff k=5인 경우 \\(\\rightarrow P(k=5) = 0.8\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답    \u0026amp;nbsp\n Cutoff k=7인 경우 \\(\\rightarrow P(k=7) = 0.714\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답    \u0026amp;nbsp\nAverage Precision (AP@K) Cutoff가 \\(K\\)개인 Average Precision(AP@K)은 Precision@K의 평균을 구하는 과정이다.\n$$ AP@K = \\frac{1}{m} \\sum_{j=1}^K P(j) \\cdot rel(j) \\dots \\begin{cases} rel(j)=1 \u0026amp; \\text{if } j^{th} \\text{ item is relevant}, \\cr rel(j)=0 \u0026amp; \\text{if } j^{th} \\text{ item is not relevant}, \\cr \\end{cases} $$\n \\(K\\) : Cutoff 갯수 \\(m\\) : 추천 아이템 중 relevance가 있는 아이템의 갯수 (number of relevant document) \\(j\\) : 전체 추천 아이템 리스트 중, 해당 추천 아이템의 index \\(P(j)\\) : \\(j\\)번째 까지의 precision값 \\(rel(j)\\) : \\(j\\)번째의 relevance 여부  \u0026amp;nbsp\n위에서 예시로 들었던 자동차용품 추천결과를 통해, [\\(AP@5\\), \\(AP@7\\), \\(AP@9\\), \\(AP@10\\)] 을 계산해 보았다. 특히 \\(AP@7\\) 와 \\(AP@9\\) 의 결과에서 관찰할 수 있듯이, \\(\\frac{1}{m}\\)에서 \\(m\\)은 relevance가 있는 경우만을 포함하기 때문에, 뒤에서 오답이 추가되어도 AP의 값이 페널티를 받지는 않는다.\n$$AP@5 = \\frac{1}{4} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5}\\right) = 0.95$$\n$$AP@7 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} \\right) = 0.90$$\n$$AP@9 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 \\right) = 0.90$$\n$$AP@10 = \\frac{1}{6} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 + \\frac{6}{10}\\right) = 0.85$$\n\u0026amp;nbsp\n또한, 아래의 예시에서 Case A와 Case B를 비교해보면, 순위가 높은 추천 아이템이 정확할 수록 높은 AP값이 계산되므로, 추천의 순서 또는 순위가 평가 지표에 영향을 끼침을 알 수 있다.\n$$AP@5 = \\frac{1}{3} \\cdot \\left(\\frac{1}{1} + 0 + \\frac{2}{3} + 0 + \\frac{3}{5}\\right) = 0.75 \\dots \\text{(Case A)}$$\n$$AP@5 = \\frac{1}{3} \\cdot \\left(0 + \\frac{1}{2} + 0 + \\frac{2}{4} +\\frac{3}{5} \\right) = 0.53 \\dots \\text{(Case B)}$$\n\u0026amp;nbsp\nMean Average Precision (MAP@K) AP는 각각의 사용자(또는 쿼리)에 대하여 계산한 것이므로, 각 사용자에 따라 AP값이 산출된다. Mean Average Precision(MAP)은 AP값들의 Mean을 구한 것으로, 식은 다음과 같다.\n$$MAP@K = \\frac{1}{U} \\sum_{u=1}^{U} (AP@K)_u$$\n \\(U\\) : 총 사용자의 수  \u0026amp;nbsp \u0026amp;nbsp\n마무리하며 이번 MAP에 대해 알아보았다. 다음 포스트에서는 역시나 추천시스템의 평가지표로 자주 등장하는 DCG(Discounted Cumulative Gain)에 대해서 공부하고 정리 할 예정이다.\nAP, MAP의 파이썬 코드로 된 구현체는 링크를 통해 참조할 수 있다.\n\u0026amp;nbsp \u0026amp;nbsp\nReference   https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\n  http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n  http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html\n  ","permalink":"https://lucaseo.github.io/posts/2020-04-23-mean-average-precision/","summary":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)","title":"[KR] 추천시스템의 평가 지표 : MAP"},{"content":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움. 하지만 아무도 두려워하지 않는 듯해 보여서 더 두려운 고요한 두려움.\n\u0026ldquo;대체 언제까지 슬랙으로 엑셀 파일을 주고 받아야 하는가?\u0026ldquo;에 대한 해답을 찾던 중, 구글 스프레드시트(Google Spreadsheet)와 연동하는 것으로 몇가지 고민을 해결할 수 있게 되었다. gspread라는 라이브러리를 찾게 되었다. 별 거 아닌데, 왜 여태 사용해보지 않았을까. 이거다 싶었다.\n   바로 적용한 아주 간단한 예시를 들자면, 리서쳐들이 구글 스프레드시트에서 작업한 것들을 DB에 적재하는 과정을 자동화할 수 있었고, 데이터를 누가 작업했고, 업데이트가 되었는지에 대한 여부 또한 스프레드시트를 기반으로 작업하게 되니 해결 되었다. Python과 gspread 통해서 구글 스프레드시트와 연동하는 과정을 다룬 사내 튜토리얼을 다시 정리해보았다.\n\u0026amp;nbsp\n1. 튜토리얼 1.1. GCP에서 사용자 인증 설정 Python으로 스프레드시트를 연동하기 위해서는, GCP(Google Cloud Platform)에서 사용자 인증과 API 사용 인증이 준비되어야 한다.\n\u0026amp;nbsp\n1.1.1. 구글 클라우드 플랫폼에 로그인  회사에서는 팀원들과 드라이브를 공유하기에 Gsuite 계정으로 로그인했다.     \u0026amp;nbsp\n1.1.2. 새로운 프로젝트 생성  구글 드라이브, 구글 스프레드시트와 연동할 새로운 프로젝트를 새로 생성한다.              \u0026amp;nbsp\n1.1.3. 구글 드라이브 API 사용 설정  새로 생성한 프로젝트를 선택하고, API 개요 이동하여 구글 드라이브 API 사용을 설정한다.                     \u0026amp;nbsp\n1.1.4. 구글 드라이브 API에 대한 인증정보 생성.  구글 드라이브 API를 사용함에 있어서 필요한 사용자 인증정보를 추가한다. 새 서비스 계정을 만들면, 서비스 계정 및 Key가 JSON 형태의 파일이 받아지게 된다.  특히 추후 파이썬 코드 내에서 Key가 필요하게 되니, 적절한 디렉토리에 저장하도록 한다.                 \u0026amp;nbsp\n1.1.5. 구글 스프레드시트 API 사용 설정  구글 드라이브 API 사용 설정한 방법과 동일하게, 구글 스프레드시트 API 사용설정을 활성화 한다.        \u0026amp;nbsp \u0026amp;nbsp\n1.2. 파이썬 패키지 설치  파이썬에서는 gspread와 oauth2client가 필요하다.  gspread: 파이썬을 통해 구글 스프레드시트와 연동하고, 제어할 수 있게 하는 패키지. oauth2client: OAuth2을 통해 사용자 인증을 하기 위해 설치함.   터미널 또는 CMD에서 pip를 통해 설치하도록 한다.  pip install gspread pip install --upgrade oauth2client \u0026amp;nbsp \u0026amp;nbsp\n1.3. 구글 스프레드시트 파일 설정  파이썬을 통해서 접근하고 연동하고자 하는 해당 구글 시트에서 사용자 인증 정보를 설정하다. 앞서 내려 받은 JSON파일에서 client_email의 값을 복사한 뒤, 해당 스프레드시트 우측 상단 공유를 클릭한 뒤 입력하여 권한을 부여한다..         복잡하지는 않지만, 스프레드시트마다 이 작업을 해주어야 한다는 점이 약간 번거로운 부분이다. 공유 권한부여까지 완료되었다면, 이제 파이썬으로 스프레드시트의 데이터를 읽어와보자!  \u0026amp;nbsp \u0026amp;nbsp\n1.4. 파이썬에서 테스트 1.4.1. 인증과 연동  사용자 인증 파일(JSON)을 통해 연동을 한다.  from oauth2client.service_account import ServiceAccountCredentials scope = [\u0026#34;https://spreadsheets.google.com/feeds\u0026#34;, \u0026#34;https://www.googleapis.com/auth/spreadsheets\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive.file\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive\u0026#34;] creds = ServiceAccountCredentials.from_json_keyfile_name(\u0026#34;{your_JSON_filename}.json\u0026#34;, scope) \u0026amp;nbsp\n1.4.2. 스프레드 시트 선택  테스트를 위해서 샘플 스프레드시트를 생성한 상태이다.  빈 시트인 시트1, 그리고 상장기업종목코드와 기업명이 담긴 sample_data 시트가 있다.        gspread 패키지를 통해 인증 후, 접근하고자 하는 시트의 이름을 패스한다.  import gspread spreadsheet_name = \u0026#34;{your target spreadsheet name}\u0026#34; client = gspread.authorize(creds) spreadsheet = client.open(spreadsheet_name) \u0026amp;nbsp\n1.4.3. 시트 불러오기 for sheet in spreadsheet.worksheets(): print(sheet) \u0026lt;Worksheet '시트1' id:1966713574\u0026gt; \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.4. 시트 선택하기  이름 또는 인덱스를 통해 시트를 선택할 수 있다.  ## by name sheet = spreadsheet.worksheet(\u0026#34;sample_data\u0026#34;) ## OR by index sheet = spreadsheet.get_worksheet(1) print(sheet) \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.5. 시트 내 데이터 읽어오기  get_all_values() 함수는 시트 내 데이터를 모두 출력한다.  sheet.get_all_records()[:3] [{'CompanyStockCode': 'S030190', 'PA_CompanyID': 1, 'IA_CompanyID': 'ID00001', 'DelistStatus': '', 'CompanyKorName': 'NICE평가정보'}, {'CompanyStockCode': 'S038620', 'PA_CompanyID': 2, 'IA_CompanyID': 'ID00002', 'DelistStatus': '', 'CompanyKorName': '위즈코프'}, {'CompanyStockCode': 'S039020', 'PA_CompanyID': 3, 'IA_CompanyID': 'ID00003', 'DelistStatus': '', 'CompanyKorName': '이건홀딩스'}] \u0026amp;nbsp\n1.4.6. Pandas DataFrame 형태로 변환  간단한 함수를 작성하여 Pandas의 DataFrame형태로 데이터를 불러올 수 있다.  def gsheet2df(sheet): df = pd.DataFrame(columns=list(sheet.get_all_records()[0].keys())) for item in sheet.get_all_records(): df.loc[len(df)] = item return df df_sample_data = gsheet2df(sheet) df_sample_data.head()    \u0026amp;nbsp \u0026amp;nbsp\n2. 마무리하며 리서쳐와 데이터를 주고 받으며 일하던 와중에, gspread의 적용은 매우 빛과 소금 같은 시원한, 그리고 매우 신속했던 해결책이었다. 내 업무영역에 적용한 뒤, 바로 다른 분석가 동료들에게 튜토리얼을 통해 공유하는 세션을 가지기도 했던 만큼 적용되는 범위가 컸다.\n누구든 일을 하다 잠재적인 기술 부채를 맞닥뜨리면, 미래의 나를 위해서 어떻게든 털어내버리고 싶을 것이다. 하지만, 함께 해결해나갈 인력이 없는 환경에서 비개발자인 동료들도 불편해하지 않게끔 적절한 방법을 찾고 도입하는 것은 생각보다 쉽지 않았다. 답답할 때가 없지는 않지만, 이번 케이스처럼 또 하나하나씩 도입해나가면 더 매끄러워지지 않을까라는 생각을 해보며 또 다른 기술 부채를 맞닥뜨릴 마음의 준비를 해본다.\n\u0026amp;nbsp\n 3. Reference   gspread 공식 도큐먼트\n  Manage Google Spreadsheets with Python and Gspread\n  ","permalink":"https://lucaseo.github.io/posts/2020-04-12-python-spreadsheet-gspread/","summary":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움.","title":"[KR] Python으로 구글 스프레드시트 연동하기 (ft. gspread)"},{"content":"   Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.   Raw Data  테이블 형태의 데이터셋을 확인할 수 있다.   Map: Confirmed  1월 22일부터 현재까지 전세계의 확진자 현황을 지도로 시각화한다.       (데이터셋은 Johns Hopkins CSSE의 Github에 제공된 COVID-19 데이터를 사용하였다.)\n\u0026amp;nbsp\n 사전 준비 2: Heroku Heroku는 웹사이트나 어플리케이션을 빌드하고 배포할 수 있는 PaaS이다. 파이썬, 자바, 루비, PHP, Node.js, Go 등 여러 언어를 지원하고 있다. 소규모 어플리케이션이라면 무료로도 배포할 수 있다.\nHeroku 가입 Heroku를 통해 배포를 하기 위해서는 우선 계정이 있어야 한다. 아래 링크를 통해 Heroku 계정을 생성하자.\n https://www.heroku.com/  Heroku CLI 설치 설치 계정을 생성하였다면, 현재 개발 환경에 Heroku Command Line Interface를 설치한다. 아래의 링크에는 각자의 OS에 맞추어 Heroku CLI의 설치 가이드가 제공되어 있다.\n https://devcenter.heroku.com/articles/getting-started-with-python#set-up  CLI를 통한 로그인 설치가 완성되었다면, 쉘에서 heroku 커맨드를 사용할 수 있다. 다음 명령어로 Heroku에 로그인 해보자.\nheroku login    아래와 같은 결과가 출력되며, 브라우저를 열어 로그인 하도록 한다. Heroku에 대한 준비작업은 이로서 완료되었다.\n   \u0026amp;nbsp\n 배포를 위한 파일 생성하기 Heroku에 배포를 하기 위해서는 몇 가지 준비사항이 충족되어야 한다.\n app.py가 위치한 가상환경에서 Git Repository를 생성한다.  git init 배포를 위해 필요한 파일들을 생성한다.  파일 #1. requirements.txt pip freeze \u0026gt; requirements.txt 파일 #2. .gitignore venv *.pyc .DS_Store .env 파일 #3. setup.sh setup.sh에서는 Streamlit에 대한 config.toml 파일을 생성한다.\nmkdir -p ~/.streamlit/ echo \u0026#34;\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT\\n\\ \u0026#34; \u0026gt; ~/.streamlit/config.toml 파일 #4. Procfile Procfile에서는 Heroku로 하여금 웹어플리케이션을 시작시키기 위해 명령어를 순서대로 실행하도록 한다.\nweb: sh setup.sh \u0026amp;\u0026amp; streamlit run app.py (Procfile에 대한 자세한 사항은 다음 링크에서 참조할 수 있다.)\n\u0026amp;nbsp\n Heroku에 배포하기 배포를 위한 파일이 모두 준비되었다면, 이제 본격적으로 Heroku에 배포를 할 수 있게 되었다.\nHeroku App 생성 heroku create {your app name} 앞서 Heroku CLI를 생성했을 때와 같은 Heroku 로그인 과정을 거쳐, Heroku App이 생성 되었다.\n   생성된 App은 Heroku 사이트의 대시보드에서도 다음과 같이 확인이 가능하다.\n   Heroku repository 생성 Heroku는 배포를 위해 Git을 사용하므로, 방식도 같다.\ngit add . git commit -m \u0026#39;Init app boilerplate\u0026#39; git push heroku master       Heroku 의 프로세스 인스턴스를 1개로 제한하는 것을 권장한다. 프로세스가 많을 수록 유료가 될 수 있기 때문에, 무료인 상태에서 계속 사용하고 싶다면, 아래의 명령어를 실행시켜준다.\nheroku ps:scale web=1    Heroku 배포 확인 드디어 배포가 완료되었다! 이제 배포된 웹 어플리케이션을 https://your_app.herokuapp.com 에서 확인할 수 있다. 본 튜토리얼을 위해 배포된 웹 어플리케이션은 https://tuto-covid19-map.herokuapp.com/ 에서 확인할 수 있다.\n   \u0026amp;nbsp\n 마무리 하며 간단한 과정을 거쳐 웹 어플리케이션을 배포까지 해보았다. AWS와 같은 클라우드에 배포하는 과정은 익숙하지 않았고, 현재 Streamlit이 자체적으로 특정 배포 방식을 권장하고 있지는 않기에, Dash(Plotly에서 개발한 웹어플리케이션 프레임워크)에서 웹어플리케이션을 Heroku에 배포하는 가이드를 적용해 보았다. 큰 어려움 없이 배포를 완료하였기에, 많은 분들도 이 과정을 참고하시어 Streamlit을 활용한 다양한 프로젝트를 진행할 수 있으리라 생각된다.\n\u0026amp;nbsp\n 코드  Github  \u0026amp;nbsp\n참고자료  Dash 공식 페이지 Johns Hopkins CSSE Github Repository \u0026lt;em\u0026gt;Mapping the Spread of Coronavirus COVID-19 with python and Plotly\u0026lt;/em\u0026gt; by. Babak Fard  ","permalink":"https://lucaseo.github.io/posts/2020-03-29-deploy-streamlit-to-heroku/","summary":"Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.","title":"[KR] Streamlit 웹 어플리케이션 배포하기 (feat. Heroku)"},{"content":" Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;\n웹개발을 몰라도 된다.\n 웹개발에 대해 아는 것이 전혀 없는 나 같은 사람도 페이지를 띄울 수 있다. 주로 사내용으로 이용되기 때문에 UI/UX적인 측면에서 뛰어나지 않아도 되기 때문에 일정 수준의 미적인 요소들이 기본적으로 적용되있는 점이 매우 편리하다. 간결하고 명확한 API 덕분에 다른 웹프레임워크와 비교해서 상대적으로 진입장벽이 낮다. 일정한 수준의 결과를 내기 위해 투자하는 시간이 매우 절약된다.  전달력이 매우 좋다.\n 웹어플리케이션은 사용자에게도 진입장벽이 낮다. 특히 interactive한 성향 덕분에, 슬라이드 포맷의 레포트나 자료보다 전달력과 만족도가 높은 것을 볼 수 있었다.  \u0026amp;nbsp\n덕분에 Streamlit은 조직 내부적으로 탐색적 데이터 분석(EDA) 결과를 공유하거나, 간단한 ML 모델을 배포하고 테스트를 하는 용도에 부합하는 툴이라고 할 수 있다. Streamlit API에서 제공하는 기능들을 간단하게 훑어보자.\n Streamlit 간단 맛 보기 설치 pip를 통해 설치하기 $ pip install streamlit \u0026amp;nbsp\n실행하기 Streamlit은 8501 포트에 앱이 실행된다. 일단 지금 아무 것도 없는 상황에서는 우측 상단 버튼만 있는 페이지를 볼 수 있다.\n$ streamlit run {your app}.py You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://{your_network}:8501    Streamlit 불러오기 Streamlit은 st라는 alias로 불러온다.\n# import Streamlit Library import streamlit as st 소스에 변경이 생길 경우 경우 상단에 알림이 뜬다. Rerun을 해주도록 하자.\n   \u0026amp;nbsp\n텍스트 출력 Header와 Text  Title, Header \u0026amp; Subheader  Header와 Subheader를 다음과 같이 달 수 있다. 다만 Header의 경우 subheader 레벨까지만 가능하다. Title, Header, Subheader이 각각 Header, Subheader, Subsubheader인 것으로 인식하면 될 것 같다.    ## Title st.title(\u0026#39;Streamlit Tutorial\u0026#39;) ## Header/Subheader st.header(\u0026#39;This is header\u0026#39;) st.subheader(\u0026#39;This is subheader\u0026#39;) ## Text st.text(\u0026#34;Hello Streamlit! 이 글은 튜토리얼 입니다.\u0026#34;)    \u0026amp;nbsp\nMarkdown Streamlit도 Dash와 마찬가지로 Markdown을 지원한다.\n## Markdown syntax st.markdown(\u0026#34;# This is a Markdown title\u0026#34;) st.markdown(\u0026#34;## This is a Markdown header\u0026#34;) st.markdown(\u0026#34;### This is a Markdown subheader\u0026#34;) st.markdown(\u0026#34;- item 1\\n\u0026#34; \u0026#34; - item 1.1\\n\u0026#34; \u0026#34; - item 1.2\\n\u0026#34; \u0026#34;- item 2\\n\u0026#34; \u0026#34;- item 3\u0026#34;) st.markdown(\u0026#34;1. item 1\\n\u0026#34; \u0026#34; 1. item 1.1\\n\u0026#34; \u0026#34; 2. item 1.2\\n\u0026#34; \u0026#34;2. item 2\\n\u0026#34; \u0026#34;3. item 3\u0026#34;) 2020-03-13-intro-to-streamlit/4\n\u0026amp;nbsp\nLatex Latex의 경우 백슬래시(\\)를 빈번히 사용되기 때문에, 일반 string 대신 raw string을 붙여주는 편이 좋다.\n## Latex st.latex(r\u0026#34;Y = \\alpha + \\beta X_i\u0026#34;) ## Latex-inline st.markdown(r\u0026#34;회귀분석에서 잔차식은 다음과 같습니다 $e_i = y_i - \\hat{y}_i$\u0026#34;)    \u0026amp;nbsp\n메세지와 에러메세지, 예외처리 메세지 기본적으로 포맷된 메세지 박스 기능을 제공한다.\n## Error/message text st.success(\u0026#34;Successful\u0026#34;) st.info(\u0026#34;Information!\u0026#34;) st.warning(\u0026#34;This is a warning\u0026#34;) st.error(\u0026#34;This is an error!\u0026#34;) st.exception(\u0026#34;NameError(\u0026#39;Error name is not defined\u0026#39;)\u0026#34;)    \u0026amp;nbsp \u0026amp;nbsp\n데이터프레임과 테이블 출력. 데이터를 출력하는 방법에는 3가지 방법이 있다.\n  st.table:\n 단순히 입력 테이블 전체를 리턴한다.    st.dataframe:\n 적절히 10개의 행을 기준으로 스크롤을 통해 데이터를 관찰 할 수 있고 각 열마다 정렬도 가능하다. 각 테이블의 우측 상단의 확대 버튼을 통해 테이블을 더 크게 볼 수 있고,    st.write:\n st.dataframe과 똑같은 결과를 리턴한다.    ## Load data import pandas as pd from sklearn.datasets import load_iris iris = load_iris() iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df[\u0026#39;target\u0026#39;] = iris[\u0026#39;target\u0026#39;] iris_df[\u0026#39;target\u0026#39;] = iris_df[\u0026#39;target\u0026#39;].apply(lambda x: \u0026#39;setosa\u0026#39; if x == 0 else (\u0026#39;versicolor\u0026#39; if x == 1 else \u0026#39;virginica\u0026#39;)) ## Return table/dataframe # table st.table(iris_df.head()) # dataframe st.dataframe(iris_df) st.write(iris_df)    \u0026amp;nbsp \u0026amp;nbsp\n이미지, 오디오, 비디오 파일 출력. 이미지, 영상, 오디오 파일을 열어서 재생할 수 있다.\n st.image : 파이썬 이미지 라이브러리와 함께 쓸 수 있다. st.video :  파일의 포맷을 지정해야 하며, 디폴트로는 video/mp4가 설정되어 있다. start_time 변수를 통해 재생시작점을 조절할 수 있다.   st.audio :  파일 포맷은 audio/wav 가 디폴트로 설정되어 있다. 마찬가지로 start_time 변수를 통해 재생시작점을 조절할 수 있다.    ##Show image from PIL import Image img = Image.open(\u0026#34;files/example_cat.jpeg\u0026#34;) st.image(img, width=400, caption=\u0026#34;Image example: Cat\u0026#34;) ## Show videos vid_file = open(\u0026#34;files/example_vid_cat.mp4\u0026#34;, \u0026#34;rb\u0026#34;).read() st.video(vid_file, start_time=2) ## Play audio file. audio_file = open(\u0026#34;files/loop_w_bass.mp3\u0026#34;, \u0026#34;rb\u0026#34;).read() st.audio(audio_file, format=\u0026#39;audio/mp3\u0026#39;, start_time=10)    \u0026amp;nbsp \u0026amp;nbsp\n위젯 st.checkbox - 체크박스 ## Checkbox if st.checkbox(\u0026#34;Show/Hide\u0026#34;): st.write(\u0026#34;체크박스가 선택되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.radio - 라디오버튼 ## Radio button status = st.radio(\u0026#34;Select status.\u0026#34;, (\u0026#34;Active\u0026#34;, \u0026#34;Inactive\u0026#34;)) if status == \u0026#34;Active\u0026#34;: st.success(\u0026#34;활성화 되었습니다.\u0026#34;) else: st.warning(\u0026#34;비활성화 되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.selectbox - 드랍다운 선택 ## Select Box occupation = st.selectbox(\u0026#34;직군을 선택하세요.\u0026#34;, [\u0026#34;Backend Developer\u0026#34;, \u0026#34;Frontend Developer\u0026#34;, \u0026#34;ML Engineer\u0026#34;, \u0026#34;Data Engineer\u0026#34;, \u0026#34;Database Administrator\u0026#34;, \u0026#34;Data Scientist\u0026#34;, \u0026#34;Data Analyst\u0026#34;, \u0026#34;Security Engineer\u0026#34;]) st.write(\u0026#34;당신의 직군은 \u0026#34;, occupation, \u0026#34; 입니다.\u0026#34;)       \u0026amp;nbsp\nst.multiselect - 드랍다운 다중 선택 ## MultiSelect location = st.multiselect(\u0026#34;선호하는 유투브 채널을 선택하세요.\u0026#34;, (\u0026#34;운동\u0026#34;, \u0026#34;IT기기\u0026#34;, \u0026#34;브이로그\u0026#34;, \u0026#34;먹방\u0026#34;, \u0026#34;반려동물\u0026#34;, \u0026#34;맛집 리뷰\u0026#34;)) st.write(len(location), \u0026#34;가지를 선택했습니다.\u0026#34;)       \u0026amp;nbsp\nst.slider - 슬라이더 ## Slider level = st.slider(\u0026#34;레벨을 선택하세요.\u0026#34;, 1, 5)    \u0026amp;nbsp\nst.button - 버튼 ## Buttons if st.button(\u0026#34;About\u0026#34;): st.text(\u0026#34;Streamlit을 이용한 튜토리얼입니다.\u0026#34;)    \u0026amp;nbsp\n텍스트 입력 # Text Input first_name = st.text_input(\u0026#34;Enter Your First Name\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;first_name\u0026#39;): result = first_name.title() st.success(result) # Text Area message = st.text_area(\u0026#34;메세지를 입력하세요.\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;message\u0026#39;): result = message.title() st.success(result)    \u0026amp;nbsp\n날짜와 시간 입력 ## Date Input import datetime today = st.date_input(\u0026#34;날짜를 선택하세요.\u0026#34;, datetime.datetime.now()) the_time = st.time_input(\u0026#34;시간을 입력하세요.\u0026#34;, datetime.time())       \u0026amp;nbsp\n코드와 JSON 출력  with st.echo(): 이하의 코드는 코드블럭으로 출력된다.  ## Display Raw Code - one line st.subheader(\u0026#34;Display one-line code\u0026#34;) st.code(\u0026#34;import numpy as np\u0026#34;) # Display Raw Code - snippet st.subheader(\u0026#34;Display code snippet\u0026#34;) with st.echo(): # 여기서부터 아래의 코드를 출력합니다. import pandas as pd df = pd.DataFrame() ## Display JSON st.subheader(\u0026#34;Display JSON\u0026#34;) st.json({\u0026#39;name\u0026#39; : \u0026#39;민수\u0026#39;, \u0026#39;gender\u0026#39;:\u0026#39;male\u0026#39;, \u0026#39;Age\u0026#39;: 29})    \u0026amp;nbsp\n사이드바 st.sidebar에서도 대부분의 위젯을 지원하므로, 다양하게 사이드바를 구성할 수 있다. (단, st.echo, st.spinner, st.write제외)\n## Sidebars st.sidebar.header(\u0026#34;사이드바 메뉴\u0026#34;) st.sidebar.selectbox(\u0026#34;메뉴를 선택하세요.\u0026#34;, [\u0026#34;데이터\u0026#34;, \u0026#34;EDA\u0026#34;, \u0026#34;코드\u0026#34;])    \u0026amp;nbsp\n차트 그리기 Streamlit은 자체 내장된 기본적인 차트 외 matplotlib, plot.ly, altair, vega_ilte, bokeh, deck_gl, pydeck, graph_viz 등 다양한 시각화 패키지를 지원한다.\n(Streamlit은 EDA 용도로 많이 사용되는 만큼, 시각화 부분은 따로 다룰 계획이다.)\n## Plotting st.subheader(\u0026#34;Matplotlib으로 차트 그리기\u0026#34;) iris_df[iris_df[\u0026#39;target\u0026#39;]==\u0026#39;virginica\u0026#39;][\u0026#39;petal length (cm)\u0026#39;].hist() st.pyplot()    \u0026amp;nbsp \u0026amp;nbsp\n 마무리 Streamlit의 API를 훑어보면서, 전체적으로 많은 부분이 간결하고, 쉽다고 느껴졌다. Flask나 Django로 개발하는 개발자 입장에서 Streamlit 같은 프레임워크는 자유도가 제한된다고 느껴질 수도 있겠다. 하지만 등장한지 얼마 안 되는 만큼 커뮤니티 포럼에서는 활발한 토론과 기능 추가에 대한 요청이 이어지고 있는 중이다. Streamlit 개발자들이 적극적으로 피드백을 반영하는 모습을 보이고 있으므로, 앞으로의 발전이 더 기대된다.\n이번 소개 글에 이어, 개인적으로 Streamlit으로 개발하면서 얻은 팁이나, 클라우드 또는 Heroku에 배포하는 과정, 데이터사이언스 프로젝트를 위해 웹어플리케이션을 구성하는 팁 등을 시리즈로 작성할 예정이다. 나처럼 웹개발은 모르지만 데이터분석 결과를 그럴 듯하게 구성하고 싶은 분들께 도움이 되었으면 한다.\n\u0026amp;nbsp\n코드  Github  \u0026amp;nbsp\n참고자료  Streamlit 공식 페이지  ","permalink":"https://lucaseo.github.io/posts/2020-03-13-intro-to-streamlit/","summary":"Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;","title":"[KR] 파이썬 웹어플리케이션 맛보기 (feat. Streamlit)"},{"content":" 신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다. 회사에 시니어가 없어서 그런지 이 루트로 도움을 많이 받고 있음을 체감한다. 자연스럽게 ‘나도 할 수 있을까?’ 라는 생각이 들었다. 데이터를 다루는 일을 하고 있지만, 내가 하는 일을, 내가 아는 것들을 글로서 풀어낼 수 있을지 스스로 물어보면 돌아오는 답은 ‘자신이 없다’ 였다. 솔직히 아직은 자신이 많이 없다.\n   \u0026#34;핑계는 수백만개라도 댈 수 있죠\u0026#34; Source: The Office   \u0026amp;nbsp\n이런 와중에 글또라는 모임은 1년 전부터 여러 커뮤니티와 변성윤님을 통해 몇 차례 접하고 있었다. 글 쓰는 또라이가 세상을 바꾼다라니… 너무 멋있다고 생각했다 (역시 대단한 사람들!). 마침 글또 4기의 모집 글을 접하게 되었고, 글쓰는 것에 대해 여러 생각을 하는 중 타이밍이 맞았다. 다른 때 같았으면 모집글만 보고 “내가 저런 거 할 시간이 어딨어” 등등 갖가지 이유를 대며 안 할 수도 있긴 했다. 그래서 일단 질렀다. 글또 지원은 2020년에 강림하신 첫 지름신이었다.\n글또를 통해 이루고자 하는 것 글쓰는 꾸준한 습관 몇 년 전부터 글을 써보고자 다양한 플랫폼에서 시도했었다. 페이스북, 워드프레스 블로그, 인스타그램, 힙합엘이…. 여러 가지 이유가 있었겠지만, 가장 주요한 이유를 냉정하게 꼽아보자면 끈기가 부족했다. 글 쓰는 체력이 부족하고, 쓰다가 아니다 싶으면 접어버리는 안 좋은 습관도 있다는 것을 인지하게 됐다.\n반면에 글또는 다 함께 정한 규칙과 약간의 강제성이 있고, 글을 읽고 피드백을 해주는 분들이 있다. 나 또한 다른 분들의 글을 읽고 피드백을 해야 한다. 지금 나에게 필요한 것은 약간의 강제성을 부여받고 다른 멋진 분들을 통해 자극을 받는 것이니까, fit이 딱 맞는다. 예치금을 삭감당하지 않겠다. 기필코.\n\u0026amp;nbsp\nClear할 때까지 글또를 통해서 얻고자 또 다른 목표는, 글쓰기를 통해 이해도와 설명력을 끌어올리는 것이다. 설명력은 높은 이해를 기반으로 이루어지는 것으로 생각하는데, 여태까지 회사에서 일하고 개인적으로 공부를 하면서 늘 어중간하게 알고 있는 것 같은 찝찝함을 버리지 못했다. 반면에 공부를 잘하는 친구들은 (1) 교과서만 봤어요, (2)질문을 많이 했어요, (3)다른 친구들에게 (혹은 부모님을 앉혀놓고) 설명을 해주면서 스스로 공부가 많이 됐어요 라고 들 많이 하더라. 생각해보면 난 학창 시절에 저런 걸 하나도 안 했다. 하지만 글또를 통해서 나 자신이 더 공부하고 성장을 할 수 있을 것이라 기대한다. 무언가에 대해 글을 쓰고 남에게 보이려면, 어쭙잖게 쓰면 안 된다는 것은 본능적으로 알고 있으니까.\n\u0026amp;nbsp\n글또 4기에서 쓰고자 하는 글 글또에 참여하시는 다른 분들을 보고, 나는 어떤 글을 써볼지 고민해봤다. 무엇보다 내가 배우고 느낀 점을 기록하는 것이 현시점에서는 우선이라 신속한 정보 전달의 목적은 2순위로 두기로 했다. 일단 떠오르는 주제가 여러 가지 있지만 좀 더 구체적인 정리가 더 필요하다.\n\u0026amp;nbsp\n스스로 돌아보는 반기 별 회고 글\n 회고를 하지 않다 보니 작년 한 해 동안 대체 무엇을 한 건지…. 남는 게 많이 없었고 개인적으로 허무하다는 느낌을 많이 받았다. 이번에는 회고 글을 정리하면서 작년과 무엇이 다른지 한번 느껴보고 싶다.  \u0026amp;nbsp\n회사에서 삽질한 경험\n 시니어가 없고, 늘 개인적으로 고군분투하다 보니까 이러한 경험도 글로 남기면 좋겠다고 생각해본다. 나만 그런 것은 아닐 테니까. 그리고 증거로 남겨야겠다.  \u0026amp;nbsp\n데이터 사이언스 대회 참가 후기\n 한 번도 on-going 데이터 사이언스 대회에 참가해 본 적이 없는데, 이 또한 갖가지 핑계를 대며 하지 않았던 거라고 자평해본다. 데이터 사이언스 대회 참가를 통해 배운 점, 삽질한 점들을 돌아보고 정리해보고자 한다.  \u0026amp;nbsp\n개발 관련 책 리뷰\n 책은 자주 많이 사는데, 늘 훑어보기만 했던 게 좀 찝찝했다. 책에 대한 리뷰글을 틈틈이 쓰고자 한다. 출판사 분들 지켜봐 주세요  \u0026amp;nbsp\n글을 마치며 작년 말 느꼈던 성장에 대한 고민, 답답함을 기반으로 올해 1, 2월 사이 내린 결정들이 있는데, 글또 참여도 그중 하나였다. 벌린 일이 많아 걱정은 되지만, 앞으로 글또에서 만나게 될 많은 멋진 분들이 계시니까 아주 큰 걱정은 아니라고 생각된다. 개복치처럼 터지지 말고, 꾸준히 존버하자.\n","permalink":"https://lucaseo.github.io/posts/2020-02-23-init-geultto-4th/","summary":"신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다.","title":"[KR] 글또 4기 다짐글"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.) 원을 두 바퀴, 세 바퀴 돌면 720도, 1080도가 되는데 이런 식의 계산은 편리하지가 않다. 두바퀴는 2 * 2pi randians으로 계산하는 것이 더 편하다고 하는데 일단 지켜보자.\n아래의 Figure 1.에서처럼 45도의 각도는 45/360 * 2pi = 0.785 radians 로 표현할 수 있다.\n   \u0026amp;nbsp\n삼각함수 복습 삼각함수를 좀 복습해야겠다. 위의 그림의 삼각형을 보고 다음과 같이 부른다면\n Hypotenuse 빗변 (파랑, 원의 반지름) : r Adjacent 밑변 (빨강, x축 길이) : x Opposite 높이 (초록, y축 길이) : y  피타고라스의 정리를 통해 다음과 같이 말할 수 있다.\n$$x^2 + y^2 = r^2$$\n그렇다면, sine, cosine도 다음과 같이 구할 수 있다.\n$$\\sin = \\dfrac{y}{r}$$\n$$\\cos = \\dfrac{x}{r}$$\n단위원(unit circle)를 기준으로 본다면 r=1 이므로 다음과 같다.\n$$\\sin = \\dfrac{y}{1}$$\n$$\\cos = \\dfrac{x}{1}$$\n","permalink":"https://lucaseo.github.io/posts/2020-01-25-dsp-basic-s01-7/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Trigonomatry"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.\n      \u0026amp;nbsp\n선이 회전하는 속도는 곧 frequency로 나타낼 수 있다. 앞서 frequency는 1초에 몇 사이클을 회전할 수 있는지를 통해 측정한다고 했는데, 원문의 Figure 1.을 통해서 확인할 수 있듯이 frequency가 높을 수록 회전하는 선이 속력이 빨라지고, 주기도 빨라지게 된다. 반대로 frequency가 낮으면 주기가 느려진다.\n      \u0026amp;nbsp\nThe Cosine Wave : The Counterpart to Sine 반대로 cosine wave(코사인파)는 원을 그리며 회전하는 선과 X축에서 수평의 길이와 관계 있다. 사실 sine wave와 cosine wave는 90도로 회전하면 서로 같은 모양을 가지고 있다.\n   \u0026amp;nbsp\nSine wave와 cosine wave는 밀접한 관계를 맺고 있으며, 둘 다 주기성을 띄는 주기함수(periodic signal)임을 이해하는 것이 매우 중요하다.\n","permalink":"https://lucaseo.github.io/posts/2020-01-23-dsp-basic-s01-6/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sine Wave"},{"content":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,\ndf = pd.DataFrame({ \u0026#39;A\u0026#39;: [1,2,3], \u0026#39;B\u0026#39;: [\u0026#34;goodbye\u0026#34;, \u0026#34;cruel\u0026#34;, \u0026#34;world\u0026#34;], \u0026#39;C\u0026#39;: [False, True, False] }) df.info() 결과물 출력 비교\npandas 0.x.x\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): A 3 non-null int64 B 3 non-null object C 3 non-null bool dtypes: bool(1), int64(1), object(1) memory usage: 179.0+ bytes pandas 1.0.0\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 A 3 non-null int64 1 B 3 non-null object 2 C 3 non-null object dtypes: int64(1), object(2) memory usage: 200.0+ bytes \u0026amp;nbsp\n.to_markdown() : DataFrame을 Markdown 형식으로 DataFrame을 바로 Markdown 형식으로 출력할 수 있게 되었다. 문서화 작업을 할 때 항상 markdown table generator 같은 도구를 썼었는데, 매우 반갑고 편리한 기능!\ndf = pd.DataFrame({\u0026#34;A\u0026#34;: [1, 2, 3], \u0026#34;B\u0026#34;: [1, 2, 3]}, index=[\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) print(df.to_markdown()) | | A | B | |:---|----:|----:| | a | 1 | 1 | | a | 2 | 2 | | b | 3 | 3 | \u0026amp;nbsp\ningore_index : index reset 파라미터 추가 기존에는 정렬이나 중복값 제거 후 .reset_index(drop=True)를 추가적으로 해줘야 했으나, ignore_index 파라미터를 통해 index를 리셋할 수 있게 되었다. default는 False. 다음 기능들에서 찾아볼 수 있다.\n .sort_values() .sort_index() .drop_duplicates()  \u0026amp;nbsp\npd.NA : 새로운 missing value의 실험 기존에 Pandas에서 missing value를 처리할 때는 np.nan이나 None이라는 싱글턴이 사용되었다. 그러나 data type이 float일 때는 np.nan , object일 경우에는 np.nan이나 None, datetime일 경우에는 np.NaT가 사용된다. 데이터타입마다 Null 데이터의 표현이 각기 달랐기 때문에, pd.NA는 datatype이 각기 달라도 missing data를 통일되게 표현할 수 있기 위해 도입되었다. 현재는 pd.NA는 data type 중 integer, boolean, 그리고 새로 도입된 string에서 사용 가능하다. pd.NA 값은 \u0026lt;NA\u0026gt;로 리턴된다. 실험적으로 도입했다고 하니, 지켜보면 좋을 듯.\n\u0026amp;nbsp\nstring : 새로운 data type 도입 기존에는 object 이라는 data type으로 뭉뚱그려진 느낌이 있었으나, string 이라고 따로 지정할 수 있게 됨으로서 EDA나 wrangling 측면에서 더욱 편해질 것 같다. (pd.NA도 확인 가능함)\npd.Series([\u0026#39;abc\u0026#39;, None, \u0026#39;def\u0026#39;], dtype=pd.StringDtype()) 0 abc 1 \u0026lt;NA\u0026gt; 2 def Length: 3, dtype: string \u0026amp;nbsp\nbool : missing value 표현 가능 기존에 boolean은 True / False 만 표기가 가능했으나, Pandas 1.0.0 에서는 missing value도 가능하다. (pd.NA도 가능함)\npd.Series([True, False, None], dtype=pd.BooleanDtype()) 0 True 1 False 2 \u0026lt;NA\u0026gt; Length: 3, dtype: boolean \u0026amp;nbsp\n Reference https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html\n","permalink":"https://lucaseo.github.io/posts/2020-01-16-pandas-new-release/","summary":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,","title":"[KR] Pandas 1.0.0 : 바뀐 점을 ARABOJA"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다. 음색이라는 개념은 음의 높이나 강도와는 별개의 특성이다. 플룻과 바이올린으로 똑같은 음을 연주하더라도, 소리는 서로 확연이 다른 것과 연관이 있다. 음색은 주로 배음(overtone)과 고조파(harmonic)의 유무에 따라 결정 된다.\n대부분의 음악은 fundamental frequency와 다수의 fundamental frequency에 위치한 harmonics로 이루어져 있다. 예를 들어 A4의 fundamental frequency는 440Hz이고, harmonics는 880Hz, 1320Hz, 1760Hz \u0026hellip; 의 주기로 이루어져 있다. 악기들은 대개 연주가 될 때, fundamental frequency와 harmonics에 위치한 소리를 생성한다.\n신호처리에서 sound wave를 논할 때는 주로 3가지 기본 sound wave를 지칭한다.\n sine wave: overtone이 없는 순음(pure tone) squire wave: fundamental frequency와 fundamental frequency의 홀수 배음 harmonics saw wave: fundamental frequency와 fundamental frequency의 전체 harmonics  신호를 관찰해보면 fundamental frequency가 가장 소리가 큰 부분을 차지하고 있고, harmonics는 frequency가 증가할 수록 감소한다.\n원본의 실습에서는 각기 다른 sound wave의 소리를 직접 들어보고 이에 따른 스펙트럼(spectrum)을 확인할 수 있다. 스펙트럼이란 특정 신호 안에 담긴 frequency들을 시각화된 형태이다.\n   \u0026amp;nbsp\n실제로 실행을 하고 스펙트럼을 확인해보면, 앞에서 설명한 바와 같이 Sine Wave는 단 하나의 440Hz frequency만 있는 순음이다. 반대로 Noise Wave는 어떠한 형태의 frequency도 구별을 할 수가 없다.\nSquare Wave 와 Saw Wave를 보면 Sine Wave를 여러번 반복한 형태와 비슷하게 보인다. 또한 spectrum에서도 여러 frequency 가 공존하는 것을 볼 수 있다.\n뒤 따르는 내용에서는 어떻게 하면 여러 개의 sine wave가 모여 complex wave(복합파형)를 이루는 지를 알아보자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-15-dsp-basic-s01-5/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Timbre"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다. 이런 밀고 당김의 연쇄작용이 진동을 발생하고 공기 중으로 전파가 나아갈 수 있게 한다.\n원문의 Figure 1.을 직접 참고해보기를 바란다.\n   \u0026amp;nbsp\n왼쪽 상단의 회색 직사각형이 파동의 진원지인 물체이고 무수히 많은 점들이 공기 중의 입자라고 보자. 원문의 이미지에서 이 입자들의 움직임을 잘 관찰해보면 물체의 움직임에 따라 특정 공간 내에서만 좌우로 움직이고 입자 자체가 자리를 이동하지는 않는 것을 볼 수 있다. 움직이는 물체로 인해 입자들도 비슷한 움직을을 보이게 되는데, 이것을 공명한다고 한다.\n입자 예시 아래 보이는 곡선은 기압의 정도를 나타낸다. 곡선이 수평축의 위를 지나갈 때는 공기의 입자들이 서로 컴팩트하게 모여있어 기압이 높은 상태(compression: 압축, 고밀도)이고, 반대인 경우 기압이 낮은 상태(rarefaction: 저밀도, 희박한 상태)이다.\n진동의 속도를 frequency(주파수)라고 한다. Frequency는 1초에 몇 사이클을 지났는지를 측정한 것이고, Hertz(Hz : 헤르쯔)라는 단위를 쓴다. Compression과 rarefaction의 반복 사이클을 1초에 몇번 왔다갔다 했는지를 측정하면 frequency를 구할 수 있다. 아래 Figure 1a.를 보면, compression과 rarefaction을 지나간 사이클 1번 이라고 볼 수 있겠다.\n   \u0026amp;nbsp\nFigure 1.을 통해 이것저것 시도해 보다 보면, frequency가 증가할 수록 wavelength(파장)이 줄어드는 것을 볼 수 있다. 파형(wave)의 길이(length)는 각 파형의 꼭지점이나 골짜기 사이의 거리를 측정하면 되는데, 이 거리는 frequency와 반비례 한다. 또한, frequency가 아무리 변해도 파형이 이동하는 속도는 일정하다. 해수면을 기준으로 소리는 공기에서 340m/s, 물과 같은 물질에서는 1500m/s 정도로 이동한다.\n직접 들어보자 Figure 1.처럼 부드럽게 진동하는 파형의 경우, 우리의 청각은 이 파형을 순음(pure tone)으로 받아들인다. Frequency가 낮은 음파는 낮은 음역(bass), 높은 frequency는 높은 음역(treble)이라고 하는데, 청각이 뛰어난 사람의 경우에 20 ~20,000Hz의 음역대를 들을 수 있다. 나이가 들어가면서 청력이 저하되면 들을 수 있는 음역대도 당연히 줄어들게 된다. 원문의 Figure 2.를 통해 여러가지 frequency를 한번 시도해보자.\n(실행이 되지 않는다면 아래의 링크에서 따로 실험해보자. 20,000 이상의 소리를 들어보자. 나는 들리지 않는다.)\nOnline Tone Generator - generate pure tones of any frequency\n   \u0026amp;nbsp\nX축을 보면, frequency의 눈금이 linear하지 않고 logarithmic한 것을 알 수 있다.\n440Hz는 우리가 흔히 말하는 음 \u0026ldquo;라\u0026quot;인데 A4라고 표현한다. 880Hz도 마찬가지로 \u0026ldquo;라\u0026quot;인데 한 옥타브가 높은 A5라고 한다. 여기에서 자세한 설명은 없지만, 왜 간격이 linear하지 않고 logarithmic한지 직관적으로 느낌은 알 수 있을 듯 하다\n우리의 귀는 대부분 20~8,000Hz 사이의 음역대에 반응하고, 사람의 목소리는 300~3,000Hz 사이에 속한다. 88건반 피아노는 22~4,000Hz의 기본 주파수의 소리를 내는데, 그 이상의 소리를 내는 경우도 있으며, 이를 overtone이라 한다.\n","permalink":"https://lucaseo.github.io/posts/2020-01-13-dsp-basic-s01-4/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sound Waves"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다. 그 외의 대답은 모두 거짓말이다.\n현재 우리가 가지고 있는 이산신호의 맥락에서 봤을 때, 65분 당시의 고도에 대한 기록은 어디에도 없으므로, 우리는 확신을 가지고 이야기 할 수가 없다.\naltitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] 위와 같이 10분 주기로 기록된 현재의 이산신호는 빨강, 파랑, 주황, 초록으로 기록된 아래의 다양한 비행고도의 변화를 모두 표현(represent)할 수 있다. 연속신호로 봤을 때는 고도의 변화가 제각기 다르지만, 샘플링 주기만 봤을 때는 모두 같은 신호로 볼 수 있다. 따라서 60분과 70분의 점을 이어서 31,000 이라고 답을 하는 것은 엄청난 착각을 불러일으킬 수 있다.\n   각기 다른 네 가지 연속신호를 나타내는 빨강, 파랑, 주황, 초록 곡선      네 가지 연속신호를 10분 샘플링 주기의 이산신호로 표현했을 때   \u0026amp;nbsp\nAlias 위의 빨강, 파랑, 주황, 초록 곡선은 10분 샘플링 주기의 이산신호로 표현 될 때 서로 구별이 불가능하며, 이를 각기 서로의 alias라고 부른다. 위의 네가지 곡선은 서로 다르지만, 10분 주기로 샘플링 되었을 때, 정확히 똑같아 보이기 때문이다.\n본 자료에서 두 점을 선으로 잇지 말라고 강조하고 있으나, 이해하기 쉽도록 시각화를 하기 위해서 앞으로도 지속적으로 점과 점 사이를 선으로 이어서 예를 들 것임. 글쓴이가 선을 잇는다고 해서 선을 잇는 행동은 하지 말 것 추천함.\nRemoving Uncertainty : Frequency and Context 샘플과 샘플 사이에 간격이 있다면, 불확실성(uncertainty)가 동반된다. 샘플 간격 사이에 벌어지는 급격한(rapid fluctuation) 변화에 대한 정보가 손실 될 수 있기 때문에, 간격이 크면 클 수록 우리는 샘플링이 된 현재의 이산신호가 실제 현상을 그대로 표현한다고 확신할 수가 없게 된다. 이러한 불확실성을 줄이는 방법은 반대로 간격을 줄이는 것 즉, 샘플링 주기를 줄이는 것이다.\n비행고도 예시에서, 샘플링 주기를 10분이 아닌 5분 단위로 설정한다고 해보자. 이렇게 되면 샘플링 주기가 적용된 파랑색 곡선의 이산신호를 봤을 때, 빨강, 주황, 초록 곡선은 더 이상 파랑 곡선의 alias라고 할 수 없다. (이제는 서로 구별할 수 있다!)\n   \u0026amp;nbsp\n샘플링 주기를 무작정 줄일 수도 있겠지만, 샘플링에는 감수해야할 부분도 있다. 샘플들이 저장되는 메모리가 확보되어야 한다는 것. 따라서 샘플링 주기를 줄일 때에는 꼭 주의를 기울어야 한다.\n샘플링 주기를 제대로 설정하는 방법은 말은 쉽지만 정보의 손실이 없을 정도로 설정하는 것이다. 만약 필요한 만큼보다 더 자잘하게 샘플링 주기를 설정한다면 오버샘플링(oversampling)이 되어 메모리나 연산 리소스를 낭비하게 되고, 너무 뜸하게 설정하면 언더샘플링(undersampling)으로 정보 손실이 발생할 수 있게 된다.\n샘플링에 대한 이론은 frequency(주파수) 개념을 이해하면 좀 더 쉬워질 수 있을 것 같다. 다음에는 주파수에 대한 개념을 접할 수 있는 sound wave(음파)를 한번 알아보자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-12-dsp-basic-s01-3/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sampling \u0026 Aliasing"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다. (\u0026lt;strong\u0026gt;원문 Figure 1. 참조\u0026lt;/strong\u0026gt;)\n   altitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] Figure 1.의 파랑색 점 하나하나가 샘플이며, 그래프말고도 다음과 같이 이산값의 리스트로도 표현하고, 인덱싱(indexing)을 할 수도 있다.\n altitude[4] → 35,000 altitude[8] → 27,000  Sampling Period (샘플링 주기) 샘플링 주기(sampling period)는 연속적인 신호 사이의 지속기간(duration)을 뜻한다. 위의 비행 고도 샘플링 예시에서 각 고도를 기록하는 주기를 10분으로 놨었는데, 이때 샘플링 주기가 10분이고, 아래와 같이 나타낼 수 있다.\n$$\\text{sampling period} = 10\\text{ minutes} / 1\\text{ sample}$$\n샘플링 주기를 알 수 있다면, index에 샘플링 주기를 곱함으로서 몇 번 째 샘플이 언제 기록 되었는지를 추적(?)할 수 있다.\n$$\\text{time of 3rd sample} = 2 \\cdot 10 \\text{ minutes} / 1 \\text{ sample} = 20 \\text{ minutes}$$\n이산신호를 해석하기 위해서는 context를 파악하는 것이 매우 중요하다. 샘플링 주기를 알아야지만, 이산신호의 값들이 의미를 가지고 make sense할 수 있는 것이고, sampling period를 알지 못 한다면, 이러한 값들이 의미를 잃게 된다.\n추가적으로 \u0026hellip;    원본의 Figure 1.에서 비행기의 실제 고도를 주의깊게 봤다면, 60-70분 사이 급격한 하강 후 고도를 회복하는 부분이 있었음을 관찰할 수 있는데, 우리의 샘플링 주기는 10분이었기 때문에 정작 샘플링 당시에는 기록되지 못 했다. 샘플링 주기가 적절하지 않았기 때문에 중요한 정보가 손실 된 것이다.\n따라서 특정 물리적인 현상을 관측하기 위한 이산신호를 기록하려면, 이산신호가 그 현상을 제대로 나타낼 수 있도록 샘플링 주기를 적절하게 선택해야 한다. 이렇듯 샘플링 주기의 결정은 신호처리 분야에서도 매우 중요하게 다루는 부분 중 하나이다.\n다음 번에는 신호주기를 매우 뜸하게 설정 했을 때 나타날 수 있는 결과에 대해 좀 더 알아보도록 하자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-11-dsp-basic-s01-2/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Discrete Signals"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다. 컴퓨터는 인간과 비교해서 겁나 빠르다는 장점이 있지만 또 반대로 컴퓨터는 겁나 단순해서 오직 이산값(discrete values)만 읽고 처리가 가능하다.\n하지만 현실은 그렇지 않다. 실생활에서 발생하는 대부분 연속적(continuous) 신호이다. 따라서 컴퓨터에서 분석하기 이전에 연속 신호를 이산적인, 디지털의, 딱딱 떨어지는 값으로 변환(translate)하는 과정을 거쳐야 한다.\n\u0026lt;strong\u0026gt;원문의 Figure2\u0026lt;/strong\u0026gt;를 보면 이산 신호를 통해서 연속 신호를 완벽하게 재현하는 것은 불가능 해보일 수 있다. 그리고 실제로 이는 근사화(approximation)에 그친다고 주장하는 사람도 있다.\n      하지만 DSP를 공부한다면, 이산 지점를 이용해서 연속 신호를 완벽하게 표현하는 것이 불가능하지 않다고 하니, 열심히 공부해보자!\n","permalink":"https://lucaseo.github.io/posts/2020-01-10-dsp-basic-s01-1/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 신호란?"},{"content":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.name: my-local-es node.name: local ### For Response by External Request network.host: 0.0.0.0 ### For Head http.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/elasticsearch-6.6.1 $ nohup bin/elasticsearch \u0026amp; \u0026amp;nbsp\n정상적으로 실행 중인지 확인 $ ps ax | grep elasticsearch $ curl localhost:9200 http://localhost:9200 실행\n\u0026amp;nbsp\n 02. 키바나 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-linux-x86_64.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-darwin-x86_64.tar.gz \u0026amp;nbsp\n압축 풀기 $ tar -xzvf kibana-6.6.1-linux-x86_64.tar.gz $ rm kibana-6.4.0-linux-x86_64.tar.gz \u0026amp;nbsp\n환경 설정 $ cd elasticstack/kibana-6.6.1-linux-x86_64/config $ vi kibana.yml server.host : 0.0.0.0 elasticsearch.url : \u0026#34;http://localhost:9200\u0026#34; kibana.index : \u0026#34;.kibana\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/kibana-6.6.1-linux-x86_64 $ bin/kibana \u0026amp;nbsp\n실행확인 http://localhost:5601 실행\n","permalink":"https://lucaseo.github.io/posts/2020-01-09-elasticsearch-kibana-local-env/","summary":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.","title":"[KR] 로컬 환경에 엘라스틱서치, 키바나 설치하기"},{"content":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인. 특히 로그를 운반하는 역할.   Beats 데이터 수집, 운송 Logstash와 비슷하나, 변환 기능이 제외되어 있음. 보다 가볍게 사용할 수 있음.    이외에 Elastic Cloud와 X-pack이 추가로 있으며, 기업을 대상으로 한 Enterprise 솔루션도 확대되고 있는 추세다.\n X-pack의 경우 유료이며, 보안을 강화하여 유저에게 권한까지 부여 가능하다. X-pack의 머신러닝 기능은 현재로서는 데이터의 이상징후를 탐지하는 수준이다.  Elastic, 어떻게 좋은가?   Near Realtime\n 데이터 색인 후 약 1초 후 Refresh 시점부터 거의 실시간으로 검색결과에 반영됨    Fast\n 기본적으로 모든 Field에 대해 Indexing(색인) 처리를 하기 때문에 검색 처리 시간이 짧다    Horizontal Scalability\n Elastic Cluster에 Elasticsearch Node를 1개씩 추가하며 수평적으로 확장하기에 용이하다    Distributed Operation\n 데이터를 조각(shard)로 세분화 하여 분산 저장하기 때문에 처리 속도가 향상된다.    Replica Shard\n 데이터 조각을 복제하여 다른 Node에도 저장하기 때문에, 특정 Node가 다운되거나 손실이 생겨도 데이터 유실 없이 운영할 수 있다.    Elastic Stack에서의 용어 비교    RDBMS Excel Elastic Elastic에서의 개념     Database Excel File Index 최상위 데이터 계층. Document의 덩어리   Table Sheet Type Document를 담고 있는 컨테이너 (*)   Row Row Document 데이터 검색을 위한 최소의 단위   Column Column Field JSON으로 이루어진 데이터의 property   Schema 없음 Mapping Index Document의 저장 규칙을 의미     RDBMS, Excel과는 달리 엘라스틱에서는 1 Index에 1개의 Type만 할당되어 사실상 의미가 사라진 상태이며, 7.0버전으로 업그레이드 시 Type이란 개념은 폐지 될 예정이다.  Elastic의 Work Flow      Elasticsearch\n Mapping 설정    Logstash\n 데이터 전처리 \u0026amp; 전송    Elasticsearch\n 데이터 저장    Kibana\n Index 등록 EDA 차트 선택 Aggregation 선택 데이터 시각화 대시보드 생성    ","permalink":"https://lucaseo.github.io/posts/2020-01-05-elastic-stack/","summary":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인.","title":"[KR] 엘라스틱스택(Elastic Stack) 소개"},{"content":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.\n","permalink":"https://lucaseo.github.io/posts/2020-01-02-first-post/","summary":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.","title":"[KR] 첫 포스팅"}]