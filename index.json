[{"content":"Helm  쿠버네티스는 하나의 애플리케이션 배포를 위해 수많은 yaml 파일을 작성 수정해야 함 helm 은 위의 과정을 해결해줄 수 있는 패키징 툴  ubuntu - apt, mac - brew, python - pip 같은 거라고 보면 됨   helm-chart는 쿠버네티스 리소스를 하나로 묶은 패키지  Helm 설치하기 Helm | Installing Helm\nHelm을 통해 애플리케이션 설치하기 # 설치 helm install wordpress # 업그레이드 helm upgrade wordpress # 롤백 helm rollback wordpress # 삭제 helm uninstall wordpress values.yaml 개념  쿠버네티스에서 어플리케이션 배포하기 위해서 설정해야 하는 값들이 무수히 많음  컨테이너 이미지 버전 PV, PVC 의 용량 Service의 포트 시크릿 키 등등   이 값들은 {{ .values.\u0026lt;var-name\u0026gt; }} 로 템플릿화 됨 그리고 사용자는 values.yaml 에 설정값을 저장하고 변경하면 됨  Chart.yaml 개념  패키지에 대한 정보를 기입한 metadata 파일 차트 작성 또는 다른 유저들에 의해 업로드 된 차트 확인은 다음을 참고  https://artifacthub.io Repository 또는 helm search hub \u0026lt;chart-name\u0026gt;    artifacthub 이외 다른 Repository를 추가하고자 한다면\n예시)\n 추가: helm repo add bitnami https://charts.bitnami.com/bitnami 조회: helm repo list Repository 내 차트 검색: helm search repo \u0026lt;chart-name\u0026gt;  artifacthub, bitnami, hashicorp 등 여러가지가 있음.\nRepository 통해서 Helm chart 설치하기 helm install \u0026lt;release-name\u0026gt; \u0026lt;chart-name\u0026gt;\n 다운, 추출, 설치가 진행됨  # 로컬로 받아진 차트 조회 helm list # 차트로 생성된 애플리케이션 삭제 helm uninstall \u0026lt;release\u0026gt; # install은 하지 않고 다운받고 압축만 풀기 helm pull --untar \u0026lt;chart-name\u0026gt; # 압축 푼 chart 설치하기 helm install \u0026lt;release\u0026gt; \u0026lt;chart-path\u0026gt;  helm pull 을 통해 helm chart 파일을 압축해제+다운 받고 values.yaml 파일의 값을 수정해 배포하는 방식으로 주로 사용함.  Resource  Helm Docs - Introduction  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-28-k8s-helm/","summary":"Helm  쿠버네티스는 하나의 애플리케이션 배포를 위해 수많은 yaml 파일을 작성 수정해야 함 helm 은 위의 과정을 해결해줄 수 있는 패키징 툴  ubuntu - apt, mac - brew, python - pip 같은 거라고 보면 됨   helm-chart는 쿠버네티스 리소스를 하나로 묶은 패키지  Helm 설치하기 Helm | Installing Helm\nHelm을 통해 애플리케이션 설치하기 # 설치 helm install wordpress # 업그레이드 helm upgrade wordpress # 롤백 helm rollback wordpress # 삭제 helm uninstall wordpress values.","title":"[KR] Kubernetes - Helm"},{"content":"Stateful Sets  Pod template 으로 생성, 스케일업 다운이 가능해서 deployment 와 비슷함 Pod에 저장된 데이터 또는 Pod의 설정을 stateful하게 유지해야 하는 경우 사용됨 Stateful Set은 스케쥴링 될 Pod에 특정 규칙에 따라 유니크한 인덱스를 부여함  이렇게 생성된 Pod는 재생성 되는 상황에서도 랜덤한 문자열이 아닌 규칙에 따른 이름이 부여됨   따라서 인덱스에 따라 Pod 스케줄링에 순서 또는 특정한 패턴을 부여할 수 있음 Stateful Set으로 생성한 Pod의 스토리지는 PVC로만 연결 가능  Stateful Set이 필요한 이유  인스턴스들이 특정 순서로 실행되어야 하는 경우  예) 멀티노드 구조로 데이터베이스를 구축하고자 할 때 - Master 노드 먼저 생성 → Slave 노드 이후 생성   Pod 이름, 네트워크 등의 정보를 초기화하지 않고, 유지해야 하는 경우  Stateful Set 생성 방법 ex) statefulset-definition apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql labels: app: mysql spec: template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql replicas: 3 selector: matchLabels: app: mysql serviceName: mysql-h # 서비스 이름 지정 podManagementPolicy: OrderedReady # default. 순서대로 배포. \u0026#34;Parallel\u0026#34; 일 경우 병렬로 배포 Headless Service  로드밸런싱의 역할로 Service를 연결하는 경우, Service의 역할은 들어온 요청을 여러 개의 Pod 중 하나에만 전달하는 것임. 하지만 요청을 모든 pod에 전달해야 하는 경우에는 적합하지 않음.  예) 멀티노드 데이터베이스에서 read를 할 경우 하나의 Pod에만 접근하면 되지만, 쓰기를 수행하고자 하는 경우에는 모든 Pod에 쓰기를 해야 함   특히 StatefulSet에 의해 생성된 Pod들은 service를 이용해서 로드 밸런싱을 하는 것이 아니기 때문에 로드 밸런서의 역할은 필요없고 논리적으로 pod들을 묶어 줄 수 있는 service만 있으면 되기 때문에 헤드리스 서비스를 사용함.  Headless Service 생성하기 Headless Service 정의\n# ex) headless-service-definition.yaml apiVersion: v1 kind: Service metadata: name: mysql-h spec: ports: - port: 3306 selector: app: mysql clusterIP: None Pod 정의\nex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: mysql-pod labels: app: mysql spec: containers: - name: mysql image: mysql subdomain: mysql-h hostname: mysql-pod Stateful Set 정의\napiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-deployment labels: app: mysql spec: serviceName: mysql-h  # headless service name 으로 연결 replicas: 3 matchLabels: app: mysql template: metadata: name: mysql-pod labels: app: mysql spec: containers: - name: mysql image: mysql  클러스터 IP 가 없는 것이 Headless Service의 핵심 대신 Service 에 대한 DNS Lookup 요청이 DNS 서버에 오게 될 때, Service 에 소속된 Pod IP 주소 목록을 전부 반환함 위의 예시에서 spec.hostname과 spec.subdomain 을 통해 DNS 레코드가 생성됨  Stateful Set에서의 Storage\n StatefulSet 상황에서 StorageClass 를 통해 동적 프로비저닝으로 PV의 생성이 자동화 된 상태에서, PVC또한 마찬가지로 이에 맞게 생성하고자 한다면 VolumeClaimTemplate 을 사용함.  VolumeClaimTemplate: PVC의 템플릿화된 PVC 명세로서, Pod template 처럼 StatefulSet에 기재하면 됨   이렇게 생성된 PV/PVC 는 나중에 Pod가 삭제되어도, 해당 Pod가 재생성되었을 때 직전에 사용되었던 Storage를 똑같이 연결해야하기 때문에 Pod 삭제 시 함께 삭제되지 않음  Resource  Kubernetes Docs - Stateful Set Kubernetes Docs - Stateful Set Basics  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-26-k8s-stateful-set/","summary":"Stateful Sets  Pod template 으로 생성, 스케일업 다운이 가능해서 deployment 와 비슷함 Pod에 저장된 데이터 또는 Pod의 설정을 stateful하게 유지해야 하는 경우 사용됨 Stateful Set은 스케쥴링 될 Pod에 특정 규칙에 따라 유니크한 인덱스를 부여함  이렇게 생성된 Pod는 재생성 되는 상황에서도 랜덤한 문자열이 아닌 규칙에 따른 이름이 부여됨   따라서 인덱스에 따라 Pod 스케줄링에 순서 또는 특정한 패턴을 부여할 수 있음 Stateful Set으로 생성한 Pod의 스토리지는 PVC로만 연결 가능  Stateful Set이 필요한 이유  인스턴스들이 특정 순서로 실행되어야 하는 경우  예) 멀티노드 구조로 데이터베이스를 구축하고자 할 때 - Master 노드 먼저 생성 → Slave 노드 이후 생성   Pod 이름, 네트워크 등의 정보를 초기화하지 않고, 유지해야 하는 경우  Stateful Set 생성 방법 ex) statefulset-definition apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql labels: app: mysql spec: template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql replicas: 3 selector: matchLabels: app: mysql serviceName: mysql-h # 서비스 이름 지정 podManagementPolicy: OrderedReady # default.","title":"[KR] Kubernetes - Stateful Sets"},{"content":"Job  단발성으로 연산, 데이터 처리 등을 이유로 자원을 써야하는 경우도 있음 이러한 작업을 Pod로 구현한다면, Pod는 중료 후 재생성 되는 사이클을 반복함  restartPolicy: Always가 디폴트이기 때문에 사용자가 삭제하지 않는 한 Pod 는 실행 상태를 유지하는 것이 K8s 의 특성 restartPolicy: Never 로 설정하면 한번 실행하고 종료가 되긴 함.    만일 다수의 파드를 생성하고 배치로 스케쥴링 및 관리하고 싶다면 Job 오브젝트를 사용하면 됨. 싶다면?\nJob 정의 apiVersion: batch/v1  # 버전 주의 kind: Job metadata: name: my-job spec: template: spec: cotainers: - name: math-add image: ubuntu command: [\u0026#39;expr\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;+\u0026#39;, \u0026#39;2\u0026#39;] restartPolicy: Never completions: 3 parallelism: 3   completion\n Job을 반복하고자 하는 횟수 파드가 한개씩 생성되며 Job을 실행함. 반복 중 에러가 발생하면 또 다른 파드를 생성해 다시 시도함. 횟수를 채울 때까지 반복됨    parallelism\n 병렬로 실행하고자 하는 파드의 수. completion의 수 내로 설정. 예를 들어 completion=3, parrelism=3 일 경우 파드 3개가 동시에 실행되어 Job을 수행함. 만일 3개 중 2개 완료, 1개 에러일 경우 다시 파드 1개를 생성해서 Job을 실행함. completion 횟수를 채울 때까지 작업 반복.    Job 관련 명령어\n kubectl get jobs kubectl logs \u0026lt;pod-name\u0026gt; - Job의 output 조회하기 kubectl delete jobs \u0026lt;job-name\u0026gt; - Job에 사용된 파드도 함께 삭제됨  CronJob Job을 주기적으로 실행하고 싶을 때 사용하는 오브젝트\napiVersion: batch/v1betav1  # 버전 주의 kind: CronJob metadata: name: my-cron-job spec: schedule: \u0026#34;*/ * * * *\u0026#34; # 크론 주기 표기함. jobTemplate: # Job 생성 때 사용했던 내용 그대로 기입.  spec: template: spec: cotainers: - name: math-add image: ubuntu command: [\u0026#39;expr\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;+\u0026#39;, \u0026#39;2\u0026#39;] restartPolicy: Never completions: 3 parallelism: 3 Resource  Kubernetes Docs - Job Kubernetes Docs - CronJob  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-23-k8s-job/","summary":"Job  단발성으로 연산, 데이터 처리 등을 이유로 자원을 써야하는 경우도 있음 이러한 작업을 Pod로 구현한다면, Pod는 중료 후 재생성 되는 사이클을 반복함  restartPolicy: Always가 디폴트이기 때문에 사용자가 삭제하지 않는 한 Pod 는 실행 상태를 유지하는 것이 K8s 의 특성 restartPolicy: Never 로 설정하면 한번 실행하고 종료가 되긴 함.    만일 다수의 파드를 생성하고 배치로 스케쥴링 및 관리하고 싶다면 Job 오브젝트를 사용하면 됨. 싶다면?\nJob 정의 apiVersion: batch/v1  # 버전 주의 kind: Job metadata: name: my-job spec: template: spec: cotainers: - name: math-add image: ubuntu command: [\u0026#39;expr\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;+\u0026#39;, \u0026#39;2\u0026#39;] restartPolicy: Never completions: 3 parallelism: 3   completion","title":"[KR] Kubernetes - Job / CronJob"},{"content":"Status와 Conditions 쿠버네티스에서 Pod Status와 Conditions 은 파드의 상태를 확인하는 가장 기본적인 방법이다.\n\u0026amp;nbsp\nPod Status\n kubectl get pods 상태 예시  Pending - 스케줄러가 파드를 스케줄링할 노드를 찾지 못 하면 ContainerCreating - 스케줄링되면, 이미지를 받아 컨테이너를 실행하는 상태 Running - 실행 중       \u0026amp;nbsp\nPod Condition\n kubectl describe pods Pod Status 에 비해 더 세부적인 현상태를 조회할 수 있음. 4가지 상태 카테고리를 True/False 로 나타냄.  PodScheduled - 파드가 노드에 스케줄 됨. Initialized - 초기화 컨테이너가 성공적으로 실행완료 됨. ContainersReady - 파드 내 모든 컨테이너가 준비 됨. Ready  컨테이너 내 애플리케이션이 실행 중. Pod Status를 조회할 때 함께 표기됨.         \u0026amp;nbsp  Note : Ready Condition 에 대한 주의사항 실제 애플리케이션 상태와 Ready Condition의 괴리\n 애플리케이션에 따라 실제로 사용할 수 있기까지 수초 이상의 시간이 더 걸릴 수 있음. 그러나 쿠버네티스는 Pod condition을 Ready 로 표기하는 경우가 있음. 이 경우, 쿠버네티스는 파드 배포를 완료하여 유저 트래픽을 허용하고 요청을 받지만, 유저는 애플리케이션에 접근할 수 없는 상황이 발생함.  Probes 개발자는 실제로 파드가 Ready 상태인지 확인할 수 있도록 Probe를 띄울 수 있음. Readiness Probe, Liveness Probe가 존재함.\nReadiness Probe Pod 정의 과정에서 readinessProbe 를 설정하면, 개발자는 Probe를 통해 의도한 결과를 얻기 전까지는 쿠버네티스는 Pod condition을 Ready로 표시하지 않음\n예시)\n 웹 애플리케이션일 경우 HTTP test를 수행 데이터베이스일 경우 TCP소켓이 반응하는지 테스트 테스트 스크립트가 성공적으로 실행 및 종료 되는지 테스트  # ex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: my-webapp labels: name: my-webapp spec: containers: - name: my-webapp image: my-webapp ports: -containerPort: 8080 readinessProbe: # HTTP 테스트 httpGet: path: /api/ready port: 80 # 초기 실행되는 시간이 있음을 인지하고 있는 경우, 다음과 같이 설정 가능 initialDelaySeconds: 10 # 반복 수행 주기 설정 periodSeconds: 5 # 반복 수행 횟수 상한 설정 (설정하지 않는 경우 디폴트 3회) failureThreshold: 10 # TCP 테스트 tcpSocket: port: 3306 # 스크립트 실행 커맨드 exec: command: - cat - /app/is_ready.sh Liveness Probe 애플리케이션 내부 코드의 버그 또는 그 외 예상치 못 한 문제로 인해 컨테이너 자체는 실행 중이지만 애플리케이션이 준비가 되지 못 한 경우가 발생할 수 있음. 무한대로 로딩되는 경우도 마찬가지.\n이러한 경우 컨테이너는 재시작이 되거나 배포를 중지하고 내려야 하는 상황이 생김. Readiness Probe와 마찬가지로 테스트를 수행하여, 애플리케이션이 제대로 실행 중인지 테스트할 수 있음.\n예시)\n 웹애플리케이션일 경우 HTTP health check을 날려봄 데이터베이스일 경우 TCP소켓이 반응하는지 테스트 테스트 스크립트가 성공적으로 실행 및 종료 되는지 테스트  # ex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: my-webapp labels: name: my-webapp spec: containers: - name: my-webapp image: my-webapp ports: -containerPort: 8080 livenessProbe: # http test httpGet: path: /api/health port: 8080 # 초기 실행되는 시간이 있음을 인지하고 있는 경우, 다음과 같이 설정 가능 initialDelaySeconds: 10 # 반복 수행 주기 설정 periodSeconds: 5 # 반복 수행 횟수 상한 설정 (설정하지 않는 경우 디폴트 3회) failureThreshold: 10 # TCP Test tcpSocket: port: 3306 # Execution command exec: command: - cat - /app/is_healthy.sh Reference  Kubernetes Docs - Pod Lifecycle Kubernetes Docs - Container Probes  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-20-k8s-readiness-liveness-probes/","summary":"Status와 Conditions 쿠버네티스에서 Pod Status와 Conditions 은 파드의 상태를 확인하는 가장 기본적인 방법이다.\n\u0026amp;nbsp\nPod Status\n kubectl get pods 상태 예시  Pending - 스케줄러가 파드를 스케줄링할 노드를 찾지 못 하면 ContainerCreating - 스케줄링되면, 이미지를 받아 컨테이너를 실행하는 상태 Running - 실행 중       \u0026amp;nbsp\nPod Condition\n kubectl describe pods Pod Status 에 비해 더 세부적인 현상태를 조회할 수 있음. 4가지 상태 카테고리를 True/False 로 나타냄.","title":"[KR] Kubernetes - Readiness \u0026 Liveness Probes"},{"content":" CKAD 를 준비하는 선에서 과정에서 정리한 내용으로, 실제로 쿠버네티스에서 로깅과 모니터링과 관련된 내용은 훨씬 더 방대할 수 있습니다.\n Monitoring  쿠버네티스의 클러스터를 모니터링하고자 한다면, 주로 다음과 같은 데이터를 보고자 함일 것이다.  Node 별 리소스 사용 현황 Node 별 Pod의 갯수 Pod 별 리소스 사용 현황 etc \u0026hellip;   쿠버네티스는 빌트인 모니터링 기능을 제공하고 있지 않지만, 여러가지 툴이 존재함.  Metrics Server Prometheus Elastic Stack Data Dog dynatrace etc \u0026hellip;    Metrics Server (모니터링 툴 예시)  하나의 클러스터를 기준으로 작동함. kubelet 내부 모듈인 cAdvisor (Container Advisor) 가 Pod의 퍼포먼스 메트릭을 수집하여 kubelet에 넘기는 역할을 하고, Metrics Server는 이를 수집 후, 통합하여 메모리에 저장함.  설치 방법  Minikube를 사용할 경우  minikube addons enable metrics-server   그 외  kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml    메트릭 조회 방법  Node 메트릭 조회 - kubectl top node Pod 메트릭 조회 - kubectl top pod  Logging 쿠버네티스에서 실행되고 있는 컨테이너의 로그는 다음과 같이 확인이 가능하다.\n  Pod 내 container 에서 발생하는 애플리케이션 로그 보기.\n kubectl logs -f \u0026lt;pod-name\u0026gt;    Pod 내 두 개 이상의 container 가 존재할 경우, 로그를 보고자 하는 컨테이너 이름도 명시해야 함.\n kubectl logs -f \u0026lt;pod-name\u0026gt; \u0026lt;container-name\u0026gt;    Reference  Kubernetes Metrics Server Kubernetes Docs - Logging Architecture  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-20-k8s-logging-monitoring/","summary":"CKAD 를 준비하는 선에서 과정에서 정리한 내용으로, 실제로 쿠버네티스에서 로깅과 모니터링과 관련된 내용은 훨씬 더 방대할 수 있습니다.\n Monitoring  쿠버네티스의 클러스터를 모니터링하고자 한다면, 주로 다음과 같은 데이터를 보고자 함일 것이다.  Node 별 리소스 사용 현황 Node 별 Pod의 갯수 Pod 별 리소스 사용 현황 etc \u0026hellip;   쿠버네티스는 빌트인 모니터링 기능을 제공하고 있지 않지만, 여러가지 툴이 존재함.  Metrics Server Prometheus Elastic Stack Data Dog dynatrace etc \u0026hellip;    Metrics Server (모니터링 툴 예시)  하나의 클러스터를 기준으로 작동함.","title":"[KR] Kubernetes - Monitoring \u0026 Logging"},{"content":"최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라해보았다.\n참고 튜토리얼 - LangChain: QnA with RAG\n\u0026amp;nbsp\n테니스GPT 챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 튜토리얼을 기반으로 테니스와 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다.\n\u0026amp;nbsp\n전처리 Loader 테니스와 관련된 문서를 그 자리에서 간략하게 수집했다. 위키파일도 있으며, 동호인 웹사이트에 존재하는 문서도 있기에 모두 txt 파일로 저장했다.\nText Splitter Text Splitter의 경우 그대로 RecursiveCharacterTxtSplitter 를 사용했다. Chunk가 충분히 작아질 때까지 단락, 문단, 문장, 단어 순으로 split을 반복 시도하는, 가장 범용으로 사용되는 splitter이다.\nEmbedding Embedding의 경우 가장 가성비가 좋은 text-embedding-3-small을 사용했다. 디폴트가 가성비가 더 안 좋은 text-embedding-ada-002-v2 인 것을 나중에 알았다. (pricing 참고)\nVector Database Vector DB 는 FAISS를 사용해보았다. 튜토리얼의 흐름에서는 애플리케이션이 실행될 때마다 문서 인덱싱을 새로 하게 되는 것 같아서, 사전에 생성된 인덱스를 로컬에 저장하고 애플리케이션 실행 시 로드하도록 했다.\n\u0026amp;nbsp\nRetrieval \u0026amp; Generation Prompt 프롬프트 템플릿으로는 튜토리얼에서도 사용되었고 가장 보편적인 형태를 보이는 rlm/rag-prompt 를 사용했다. (이 기회에 LangSmith도 경험이 되었다.)\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question}` Context: {context}` Answer: LLM Model LLM모델은 OpenAI의 gpt-3.5-turbo, gpt-4-turbo-preview 두가지 모두 사용해보았다. 아무래도 gpt4 가 한국어 생성 측면에서 전자에 비해 더 매끄럽고 자연스러웠다. 과금은 좀 더 비싸지만.\nApplication 애플리케이션 프레임워크로는 Streamlit을 사용해서 정말 최소x3의 형태로 만들었다.\nChatGPT-3.5와의 비교 동일한 프롬프트를 적용하고, ChatGPT에서는 context에 해당하는 retrieval output을 제외하고 동일한 질문을 했을 때 출력되는 답변을 비교 해볼 수 있다. (** 참고로 RAG에 쓰인 텍스트 데이터의 시점은 ChatGPT와 거의 동일한 2021년이다.)\n\u0026amp;nbsp\n 테니스 단체 정보     \u0026amp;nbsp    \u0026amp;nbsp\n2024년 마지막이 될 것 같은 전설 같은 선수, 라파엘 나달에 관한 정보.     \u0026amp;nbsp    \u0026amp;nbsp\n동호인 대회 나가서 가장 스트레스 받는 점     \u0026amp;nbsp    \u0026amp;nbsp\n테린이 최대 관심사 중 하나     \u0026amp;nbsp    \u0026amp;nbsp\nAI 또는 LLM에 대한 배경지식 없는 테니스 동호인이 실제로 사용한다고 가정했을 때, ChatGPT는 다소 두루뭉술한 답을 한다. 반면에 RAG로 구축한 Q\u0026amp;A 애플리케이션은 같은 질문에 대해서 훨씬 직접적이고, 간결한 답을 구사한다고 판단할 수 있다. 마지막 코트 예약과 관련된 질문에 대해서 ChatGPT는 아주 일반적인 대답을 지어낸 반면, RAG 기반 애플리케이션에서는 할루시네이션 없이, 제공된 정보가 없다고 답했다. ( 역시 코트 예약은 \u0026hellip; 빠르게 \u0026hellip; 잘 \u0026hellip; 정직하게 \u0026hellip; )\n애초에 동작하는 방식과 목적, 적용 방안이 다르기에 직접적인 비교는 무리가 있지만, 제너럴한 답변을 하는 ChatGPT와, VectorDB에 인덱싱 된 문서로부터 답변을 추출한 RAG 사이 이 정도 차이점이 있다는 것을 확인할 수 있었다.\nRAG 찍먹 후기  랭체인을 활용한 RAG는 정말이 너무 간편했다. 간편함을 레버리지 삼아. 퀄리티를 높일 수 있는 고민거리에 시간과 에너지를 더 사용할 수 있을 것 같다. 가장 간편한 텍스트의 형태의 데이터만 사용했지만, 실무에서는 다양한 형태의 문서, 특히 이미지와 표가 함께 섞여 있는 PDF 같은 데이터를 “제대로”, “잘” 파싱하는 것이 실무에서는 매우 중요할 것 같다. Text Splitter - 실무를 한다면 어떤 것을 사용할 지, 이 부분이 가장 고민 될 것 같고, 실험을 많이 해봐야 할 것 같다. 가장 기본적으로 사용해볼 수 있는 CharacterTextSplitter, RecursiveCharacterTextSplitter 만 비교해서 사용해봤으나, HuggingFace의 토크나이저까지도 고려대상이 될 수 있을 것 같다. 가장 기본적으로 OpenAI의 Embedding을 사용했으나, 나중에는 과금의 요소를 무시하지 못 할 것 같다. (현재의 가격도 PoC나 토이프로젝트 레벨에서는 그리 부담되는 정도는 아니긴 하다.) 더 찾아보니 CashBackedEmbedding 을 통해서 동일한 임베딩의 경우에는 캐싱이 가능해 속도가 빠르고, 중복 과금을 피할 수 있다고 하니 이 부분은 기본으로 장착 하는 것이 좋아보인다. 결과적으로 약간의 과금을 감수하고서라도, 간편함과 가벼움에 이점이 있어서 OpenAI의 임베딩과 모델을 사용했다. Huggingface 모델도 사용해보려 했으나, 현재 자원에서 모델 로드 자체가 제한적이었다. (Colab에서도 모델의 사이즈 때문에 로드가 되지 않더라.) 인프라가 뒷받침이 된다면 HF에서 여러가지 모델을 실험, 비교해볼 수 있을 것 같다.  Reference  LangChain - QnA with RAG OpenAI API Documentation  Code  Github  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-19-langchain-rag-qa-toy-tennis-app/","summary":"최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라해보았다.\n참고 튜토리얼 - LangChain: QnA with RAG\n\u0026amp;nbsp\n테니스GPT 챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 튜토리얼을 기반으로 테니스와 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다.\n\u0026amp;nbsp\n전처리 Loader 테니스와 관련된 문서를 그 자리에서 간략하게 수집했다. 위키파일도 있으며, 동호인 웹사이트에 존재하는 문서도 있기에 모두 txt 파일로 저장했다.","title":"[KR] LangChain 과 RAG 찍먹 후기"},{"content":"Node Selector와 Node Affinity는 Pod에 제한을 걸어 특정 노드에만 스케줄링 될 수 있도록 하는 설정임\n예시)\n 만일 클러스터 중에 하나의 노드에 GPU가 장착이 되어 있다면 딥러닝 훈련 Pod는 해당 노드에 스케줄링되어 구동되도록 함  NodeSelector  간단하고, 가볍게 하나의 Pod에 적용시킬 수 있는 설정    Node에 레이블 적용하기\n kubectl label nodes \u0026lt;node-name\u0026gt; \u0026lt;label-key\u0026gt;=\u0026lt;label-value\u0026gt; 예시) kubectl label nodes node01 size=Large    Pod에도 \u0026lt;label-value\u0026gt; 명시하기. Node에 적용된 Label 과 매칭되어 스케줄링 됨.\n  # ex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: data-processor spec: containers: - name: data-processor image: data-processor nodeSelector: size: Large  # node 의 selector 따라 매칭  NodeSelector의 경우 한가지 label 매칭만 가능하여, 특정 노드를 제외한 다른 노드 모두, 같은 설정은 제한이 있음. OR 또는 NOT 과 같은 조건은 설정이 불가능  → 이때는 Node Affinity를 사용\nNode Affinity # ex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: data-processor spec: containers: - name: data-processor image: data-processor affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: Size operator: In values: - Large - Medium Operator 유형\n In - values array에 명시된 selector에 해당하면 적용. OR 역할의 operator NotIn - values array에 명시된 selector라면 제외 Exists - value와는 무관, 해당 key 를 가지는 node에만 스케줄링  조건 조합에 따른 Node Affinity 유형   requiredDuringSchedulingIgnoredDuringExecution\n required during Scheduling  Pod가 생성될 때, 조건이 꼭 부합해야 함 없다면, 생성되지 않음   ignore during Execution  node label이 변경되어도, 이를 무시하고 Pod는 계속 구동됨      preferredDuringSchedulingIgnoredDuringExecution\n preferred during Scheduling  조건에 부합하는 노드가 없는 경우 명시된 Node Affinity 규칙을 무시함 다른 노드에 스케쥴링 됨   ignore during Execution  node의 label이 변경되어도, 이를 무시하고 Pod는 계속 구동됨      requiredDuringSchdulingRequiredDuringExecution (적용 예정)\n required during Scheduling  Pod가 생성될 때, 조건이 꼭 부합해야 함 없다면, 생성되지 않음   required during Execution  node의 label이 변경되면, 조건에 부합하지 않게 된 Pod는 퇴출(evicted) 되거나 종료됨      Resource  Kubernetes Doc - Assigning Pods to Nodes  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-18-k8s-node-affinity/","summary":"Node Selector와 Node Affinity는 Pod에 제한을 걸어 특정 노드에만 스케줄링 될 수 있도록 하는 설정임\n예시)\n 만일 클러스터 중에 하나의 노드에 GPU가 장착이 되어 있다면 딥러닝 훈련 Pod는 해당 노드에 스케줄링되어 구동되도록 함  NodeSelector  간단하고, 가볍게 하나의 Pod에 적용시킬 수 있는 설정    Node에 레이블 적용하기\n kubectl label nodes \u0026lt;node-name\u0026gt; \u0026lt;label-key\u0026gt;=\u0026lt;label-value\u0026gt; 예시) kubectl label nodes node01 size=Large    Pod에도 \u0026lt;label-value\u0026gt; 명시하기. Node에 적용된 Label 과 매칭되어 스케줄링 됨.","title":"[KR] Kubernetes - Node Selector \u0026 Node Affinity"},{"content":"Taints and Tolerance  특정 노드에 파드의 무작위 스케줄링을 제한하고, 특정 유형의 파드의 스케줄링만 허용할 때 사용되는 설정 노드를 taint (오염) 시켜서 파드가 접근하지 못 하게 한 후, 노드에 적용된 taint에 대한 tolerance(내성) 을 가진 파드만 스케줄링 된다는 개념  Node에 Taint 적용하기 kubectl taint nodes \u0026lt;node-name\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;:\u0026lt;taint-effect\u0026gt; 예시)\n# node1에 taint 적용 kubectl taint nodes node1 app=blue:NoSchedule # taint 제거 kubectl taint nodes node1 app=blue:NoSchedule- taint-effect 의 유형  NoSchedule - 포드가 스케쥴되지 않음 PreferNoSchedule - 해당 노드는 피하지만 보장된 것은 아님 NoExecute - 파드가 스케쥴되지 않으며, 현재 존재하는 파드도 tolerant가 없다면 제외함.  Pod에 Tolerance 적용하기 # ex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: sleeper spec: container: - name: ubuntu image: ubuntu tolerations: # key와 value가 모두 명시된 경우  - key: \u0026#34;app\u0026#34; value: \u0026#34;blue\u0026#34; operator: \u0026#34;Equal\u0026#34; # value와 매치 될 수 있도록 \u0026#34;Equal\u0026#34; operator 사용 effect: \u0026#34;NoSchedule\u0026#34; # value가 주어지지 않은 경우 - key: \u0026#34;app\u0026#34; operator: \u0026#34;Exists\u0026#34; # 해당 key를 가진 노드에 대해 toleration을 가짐 effect: \u0026#34;NoSchedule\u0026#34;  key 가 비워져있고, operator 가 Exists 인 경우 → 모든 노드에 대해 toleration을 가짐 effect가 비워져있을 경우 → Node taint 로 설정된 effect를 따라감  NOTE Taint \u0026amp; Tolerance\n 특정 Pod 외 다른 Pod 의 스케줄링을 제한하기 위한 설정. 특정 Pod 가 해당 노드에 바로 스케줄링 되도록 하는 설정이 아님. (오해하기 쉬움) 따라서 Tolerance를 적용한 Pod라고 해도, kube-scheduler가 다른 노드에 스케줄링 할 수도 있음. 특정 파드를 특정 노드에 스케줄링 하고자 한다면 Node Affinity가 적절함.  Reference  Kubernetes Doc - Taints and Tolerations  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-17-k8s-taints-tolerance/","summary":"Taints and Tolerance  특정 노드에 파드의 무작위 스케줄링을 제한하고, 특정 유형의 파드의 스케줄링만 허용할 때 사용되는 설정 노드를 taint (오염) 시켜서 파드가 접근하지 못 하게 한 후, 노드에 적용된 taint에 대한 tolerance(내성) 을 가진 파드만 스케줄링 된다는 개념  Node에 Taint 적용하기 kubectl taint nodes \u0026lt;node-name\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;:\u0026lt;taint-effect\u0026gt; 예시)\n# node1에 taint 적용 kubectl taint nodes node1 app=blue:NoSchedule # taint 제거 kubectl taint nodes node1 app=blue:NoSchedule- taint-effect 의 유형  NoSchedule - 포드가 스케쥴되지 않음 PreferNoSchedule - 해당 노드는 피하지만 보장된 것은 아님 NoExecute - 파드가 스케쥴되지 않으며, 현재 존재하는 파드도 tolerant가 없다면 제외함.","title":"[KR] Kubernetes - Taints \u0026 Tolerance"},{"content":"쿠버네티스 상에서의 자원 할당 쿠버네티스에서 Pod가 구동되기 위해서는 스케줄링 된 노드 내 가용할 수 있는 리소스를 사용한다.\n kube-schduler 는 Pod가 필요로 하는 자원을 어느 노드에서 구동시킬 지 결정함. 만일 모든 노드에서 Pod를 구동시킬 수 있는 자원이 충분히 남아있지 않을 경우  스케줄러는 Pod를 할당하지 않고 스케줄링을 멈춤. Pod를 Pending State 이 됨.  STATUS=pending 표기가 되며, Event 에도 Insufficient resource로 표기됨      Resource Requirements  Resource Requirements 는 Pod를 구동시키기 위해 필요한 최소 자원을 명시하는 것. CPU 수, 메모리의 크기를 명시함. CPU는 경우에 따라 의미가 달라질 수 있음  Core 1 AWS vCPU 1 GCP Core 1 Azure Core 1 Hyperthread 코어   메모리의 경우, 크기를 나타냄  Mi 메가바이트 Gi 기가바이트  1G = 1000MB 1Gi = 1024MB      리소스 리퀘스트 request 명시하기 # pod-definition apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper labels: name: ubuntu-sleeper spec: containers: - name: ubuntu image: ubuntu command: - \u0026#34;sleep\u0026#34; - \u0026#34;3600\u0026#34; resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: 1 리소스 한도 limit 지정하기 # pod-definition apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper labels: name: ubuntu-sleeper spec: containers: - name: ubuntu image: ubuntu command: - \u0026#34;sleep\u0026#34; - \u0026#34;3600\u0026#34; resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: 1 limits: memory: \u0026#34;4Gi\u0026#34; cpu: 2  Pod 애플리케이션이 한도를 초과하게 될 경우  메모리 한도 초과  Pod 종료됨 OOM (Out Of Memory) 발생   CPU 한도 초과  CPU는 시스템이 쓰로틀로 제한함      request \u0026amp; limit 설정 조합에 따른 차이   CPU 설정\n requests X - limit X  있는대로 다 사용함   requests X - limit O  리퀘스트는 리밋만큼 설정됨   requests O - limit O  리퀘스트와 리밋이 모두 요구한대로 설정됨 PodA는 자원이 더 요구되는데 리밋 때문에 제한이 되고, PodB의 자원은 그냥 놀고 있다면 , 자원이 묶이는 경우가 발생함 불필요하게 자원을 제한하는 경우도 생김   requests O - limit X  최소한의 자원은 보장 받고, 경우에 따라 놀고 있는 자원도 유연하게 쓸 수 있는 설정. 보편적으로는 가장 권장됨.      메모리 설정\n requests X - limit X  있는대로 다 사용함   requests X - limit O  리퀘스트는 리밋만큼 설정됨   requests O - limit O  리퀘스트와 리밋이 모두 요구한대로 설정됨 Pod A와 B가 있다고 가정할 때, Pod A는 자원이 더 요구되는데 리밋 때문에 제한이 되고, Pod B의 자원은 그냥 놀고 있다면 , 자원이 묶이는 경우가 발생함 불필요하게 자원을 제한하는 경우도 생김   requests O - limit X  최소한의 자원은 보장 받음 허용된 자원 이상으로 사용되는 경우 OOM과 함께 종료됨      Resource Quota - Namespace에 리소스 제한 걸기  ResourceQuota는 클러스터 레벨에서 Namespace의 총 리소스 사용을 제한하는 오브젝트 적용하고자 하는 Namespace에서 해당 오브젝트를 생성하면 됨.  # ex) resource-quota.yaml apiVersion: V1 kind: ResourceQuota metadata: name: my-resource-quota spec: hard: requests.cpu: 4 requests.memory: 4Gi limits.cpu: 10 limits.memory: 10Gi LimitRange - Namespace 리소스 설정하기  LimitRange 는 Namespace 레벨에서 Pod가 사용할 리소스에 대한 설정을 할 수 있는 오브젝트 디폴트 request와 limit, 상한, 하한선 설정 가능 CPU와 메모리에 대한 설정을 개별로 하는 것이 권장됨.  # ex) limit-range-cpu.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: # default limit on pod cpu: 500m defaultRequest: # default request on pod cpu: 500m max: # max limit on a container in a pod cpu: \u0026#34;1\u0026#34; min: # min request on a container in a pod cpu: 100m type: Container # ex) limit-range-memory.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: # default limit on pod memory: 1Gi defaultRequest: # default request on pod memory: 1Gi max: # max limit on a container in a pod memory: 1Gi min: # min request on a container in a pod memory: 500Mi type: Container Reference  Kubernetes Doc - Limit Ranges, Resource Quota  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-16-k8s-resource-requirements/","summary":"쿠버네티스 상에서의 자원 할당 쿠버네티스에서 Pod가 구동되기 위해서는 스케줄링 된 노드 내 가용할 수 있는 리소스를 사용한다.\n kube-schduler 는 Pod가 필요로 하는 자원을 어느 노드에서 구동시킬 지 결정함. 만일 모든 노드에서 Pod를 구동시킬 수 있는 자원이 충분히 남아있지 않을 경우  스케줄러는 Pod를 할당하지 않고 스케줄링을 멈춤. Pod를 Pending State 이 됨.  STATUS=pending 표기가 되며, Event 에도 Insufficient resource로 표기됨      Resource Requirements  Resource Requirements 는 Pod를 구동시키기 위해 필요한 최소 자원을 명시하는 것.","title":"[KR] Kubernetes - Resource Requirements"},{"content":"K8s에서의 컨테이너 보안 기능 도커에서는 Namespace를 통해 user (User Namespace), file system (Mount Namespace) 등을 분리하고, Capabilities 에서 선택적인 권한을 부여하는 등 보안 기능이 있는데, 쿠버네티스에서도 Security Context를 통해 동일하게 적용이 가능하다.\n 쿠버네티스에서 컨테이너는 Pod로 캡슐화 되므로, 위의 보안 적용은 컨테이너 레벨 또는 Pod 레벨 모두 적용 가능함. Pod 레벨에서 적용할 경우, Pod 내 모든 컨테이너에 동일하게 적용됨. 만일 서로 다른 설정을 컨테이너와 Pod에 설정하면, 컨테이너 설정이 Pod를 오버라이드 함.  securityContext 적용하기    Pod Level # pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: sleeper # Pod 레벨 설정 - container 명시와 동일한 레벨에서 적용 spec: containers: - name: ubuntu image: ubuntu command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] securityContext: runAsUser: 1000 # 유저 설정 Container Level apiVersion: v1 kind: Pod metadata: name: sleeper # 컨테이너 레벨 설정 spec: containers: - name: ubuntu image: ubuntu command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] securityContext: runAsUser: 1000 # 유저 설정 capabilities: # 단, Capability는 컨테이너 레벨에서만 설정 가능함. add: - \u0026#34;MAC_ADMIN\u0026#34; - \u0026#34;SYS_ADMIN\u0026#34; capabilities는 컨테이너 레벨에서만 설정 가능하므로, 동일한 설정이라 하더라도 각 컨테이너마다 지정해줘야 함.\nReference  Configure a Security Context for a Pod or Container KodeKloud  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-14-securitycontext/","summary":"K8s에서의 컨테이너 보안 기능 도커에서는 Namespace를 통해 user (User Namespace), file system (Mount Namespace) 등을 분리하고, Capabilities 에서 선택적인 권한을 부여하는 등 보안 기능이 있는데, 쿠버네티스에서도 Security Context를 통해 동일하게 적용이 가능하다.\n 쿠버네티스에서 컨테이너는 Pod로 캡슐화 되므로, 위의 보안 적용은 컨테이너 레벨 또는 Pod 레벨 모두 적용 가능함. Pod 레벨에서 적용할 경우, Pod 내 모든 컨테이너에 동일하게 적용됨. 만일 서로 다른 설정을 컨테이너와 Pod에 설정하면, 컨테이너 설정이 Pod를 오버라이드 함.","title":"[KR] Kubernetes - SecurityContext"},{"content":"Command \u0026amp; Arguments Docker Image를 기반으로 Pod 를 생성하면서 command 와 argument를 주입하려는 경우\n  예시 Docker Image\n# 예시 DockerfileFROMubuntuENTRYPOINT [\u0026#34;sleep\u0026#34;]CMD [\u0026#34;5\u0026#34;]  Pod definition 파일에서 CMD 오버라이드 하기\napiVersion: v1 kind: Pod metadata: name: ubuntu-sleep-pod spec: containers: - name: ubuntu-sleep-pod image: ubuntu-sleep-pod args: [\u0026#34;10\u0026#34;] ## \u0026lt;--- CMD override   Pod definition 파일에서 ENTRYPOINT 오버라이드 하기\napiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-pod spec: containers: - name: ubuntu-sleeper-pod image: ubuntu-sleeper-pod command: [\u0026#34;echo\u0026#34;] # \u0026lt;--- ENTRYPOINT override args: [\u0026#34;10\u0026#34;] # \u0026lt;--- CMD override   Environment Variables 환경변수 K8s Pod을 생성하는 과정에서 환경 변수 타입은 크게 3가지가 있음\nPlain Key-Value  env 아래 Key-Value 형태로 명시함.  apiVersion: v1  kind: Pod  metadata: name: webapp-color labels: name: webapp-color spec: containers: - name: webapp-color image: webapp-color ports: - containerPort: 8080 # environment variable env: - name: APP_COLOR value: blue command: [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] args: [\u0026#34;--color\u0026#34;, \u0026#34;blue\u0026#34;] ConfigMap  ConfigMap은 K8s에서 config를 따로 저장 및 관리할 수 있는 오브젝트. Key-Value 페어로 정의하고 pod가 생성될 때 주입할 수 있음  ConfigMap 생성 방법   CLI를 통해 바로 생성\n kubectl create configmap {NAME} --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod    기존에 정의된 파일로부터 생성\n kubectl create configmap {NAME} --from-file={FILE_PATH}    ConfigMap 오브젝트 생성\n# ex) config-map.yaml apiVersion: v1 kind: ConfigMap metadata: name: app-config data: APP_COLOR: blue APP_MODE: prod yaml 파일 작성 이후 오브젝트 생성\n kubectl create -f config-map.yaml    ConfigMap 주입하기   ConfigMap 전체 주입\n# pod definition 상위 부분 생략 ... spec: containers: ... env: - name: APP_COLOR envFrom: - configMapRef: name: app-config  # config map 이름을 통해 전체 주입   ConfigMap 내 특정 값만 주입\n# pod definition 상위 부분 생략 ... spec: containers: ... env: - name: APP_COLOR valueFrom: - configMapKeyRef: name: app-config  key: APP_COLOR  # config map 중에서 APP_COLOR 라는 하나의 변수만 주입   ConfigMap을 애플리케이션과 동일한 Storage에 위치할 수 있도록 Volume에 주입\n# pod definition 상위 부분 생략 ... spec: containers: ... volumes: - name: app-config-volume configMap: name: app-config  # config를 하나의 파일로서 pod의 volume에 포함시키고자 할 때   Secrets  민감한 정보를 담을 때 사용하는 오브젝트 ConfigMap과 마찬가지로 Key-Value 페어로 정의함 pod가 생성될 때 주입할 수 있음  Secrets 생성하기   CLI를 통해 바로 생성\n kubectl create secret generic {SECRETE_NAME} —-from-literal={KEY}={VALUE} kubectl create secret generic db-secret —-from-literal=DB_USER=ROOT --from-literal=DB_PASSWORD=passwrd    기존 파일을 사용하여 생성\n kubectl create secret generic {SECRET_NAME} --from-file={PATH_TO_SECRET_FILE} kubectl create secret generic db-secret --from-file=app_secret.properties    Secret 오브젝트 생성\n# ex) db-secret.yaml apiVersion: v1 kind: Secret metadata: name: db-secret data: DB_HOST: postgres DB_USER: root DB_PASSWORD: passwrd yaml 파일 작성 이후 오브젝트 생성\n kubectl create -f db-secret.yaml    Secret 관련 주의 사항  secret 을 저장하기 위해서는 encoded format이 효과적임  echo -n ‘passwrd’ | base64 와 같이 인코딩해서 표기하기  디코딩은 echo -n ‘cG9zdGdyZXM=’ | base64 —decode 추후 kubectl describe secret 을 통해 오브젝트의 정보를 조회하더라도 민감한 정보가 그대로 표기되지 않고, 형태만 bytes 로 표기됨      # ex) db-secret.yaml (인코딩 한 후 파일 작성한 예시) apiVersion: v1 kind: Secret metadata: name: db-secret data: DB_HOST: cG9zdGdyZXM= DB_USER: cm9vdA== DB_PASSWORD: cGFzc3dyZA== Secret 주입하기   Secret 전체 주입\n# pod definition 상위 부분 생략 ... spec: containers: ... envFrom: - secretRef: name: db-secret   Secret 내 특정 값만 주입\n# pod definition 상위 부분 생략 ... spec: containers: ... env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: db-secret  key: DB_PASSWORD # secret 중에서 하나의 값만 주입   Secret 을 애플리케이션과 동일한 Storage에 위치할 수 있도록 Volume에 주입\n# pod definition 상위 부분 생략 ... spec: containers: ... volumes: - name: db-secret-volume secret: secretName: db-secret  # config를 하나의 파일로서 pod의 volume에 포함시키고자 할 때   Note  Kubernetes 내에서도 표현하는 Secret은 다음과 같음  ConfigMap과 비슷하다 하지만 confidential data, sensitive information을 담는데 유용함  별도로 생성 및 관리가 가능함. 코드에 드러나지 않도록 사용하는데 적절함   그야말로 \u0026ldquo;safer option\u0026rdquo;   기본적으로 Secret 오브젝트는 encryption이 되는 것이 아니기 때문에, 디코딩을 통해 얼마든지 값을 알아낼 수 있으므로, secret object 또는 yaml 파일을 공개적으로 push 등의 작업은 권장되지 않음. 보안을 위해서는 다음과 같은 방법들이 제안 됨. (이 부분은 따로 공부하고 정리 예정)  Enable Encryption at REST Roll Based Access Control 적용 Access 제안 적용 (위와 비슷) Cloud Provider 또는 Thrid-party에서 제공하는 encryption secret store 를 사용    Reference  Kubernetes Documentation  https://kubernetes.io/docs/concepts/configuration/configmap/ https://kubernetes.io/docs/concepts/configuration/secret/   KodeKloud  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-12-k8s-configmap-secret/","summary":"Command \u0026amp; Arguments Docker Image를 기반으로 Pod 를 생성하면서 command 와 argument를 주입하려는 경우\n  예시 Docker Image\n# 예시 DockerfileFROMubuntuENTRYPOINT [\u0026#34;sleep\u0026#34;]CMD [\u0026#34;5\u0026#34;]  Pod definition 파일에서 CMD 오버라이드 하기\napiVersion: v1 kind: Pod metadata: name: ubuntu-sleep-pod spec: containers: - name: ubuntu-sleep-pod image: ubuntu-sleep-pod args: [\u0026#34;10\u0026#34;] ## \u0026lt;--- CMD override   Pod definition 파일에서 ENTRYPOINT 오버라이드 하기\napiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-pod spec: containers: - name: ubuntu-sleeper-pod image: ubuntu-sleeper-pod command: [\u0026#34;echo\u0026#34;] # \u0026lt;--- ENTRYPOINT override args: [\u0026#34;10\u0026#34;] # \u0026lt;--- CMD override   Environment Variables 환경변수 K8s Pod을 생성하는 과정에서 환경 변수 타입은 크게 3가지가 있음","title":"Kubernetes - Command / Args / Configmap / Secret"},{"content":"Multi-Container Pod  (처음 쿠버네티스와 Pod의 개념에 대해 설명할 때) 파드는 최소의 배포단위이기에 여러 개의 어플리케이션 컨테이너를 묶어 하나의 파드로 배포하는 것은 적절한 사용법이 아님. 그러나, 하나의 파드에 복수의 컨테이너를 함께 구성하여 배포하는 방식이 충분히 고려될 수 있음.  다른 컨테이너를 통해 실행되는 서비스가 메인 어플리케이션을 보조하는 경우 동일 스토리지 또는 네트워크의 공유하는 서비스가 필요한 경우 동일한 라이프사이클을 이루는 경우  생성, 배포, 삭제, 스케일 업다운이 동일하게 적용되어야 하는 경우.   예) 웹서버와 별도로 로깅 서비스가 함께 페어를 이룬 경우   따라서 굳이 별도의 파드에 분리할 필요가 없음. 이러한 경우를 Multi-Container Pod 구성 또는 디자인 패턴이라 함.  Multi-Container Pod 생성 방법 Pod를 정의할 때, 컨테이너를 array의 형태로 정의함.\n# example apiVersion: 1 kind: Pod metadata: name: my-webapp labels: name: my-webapp spec: containers: - name: my-webapp image: my-webapp ports: - containerPort: 8080 - name: log-agent image: log-agent Multi-Container Pod Design Pattern Multi-Container Pod 에는 다음과 같은 디자인 패턴이 제시됨.\n   Source: Multi-Container Pod Design Patterns in Kubernetes\n   Sidecar Container  기존 파드의 기능을 보조하고 스토리지 또는 파일시스템을 공유하는 형태의 보조 컨테이너를 사용하는 패턴   Adapter Container  파드에서 실행 중인 애플리케이션의 출력을 특정 형태나 규격에 맞게 표준화하는 용도   Ambassador Container  파드 외부의 서비스에 대한 엑세스를 간소화하는 프록시 컨테이너 유형    Init Container 하나의 파드에 한개 이상의 포드가 정의되는 또다른 유형.\n  Init Container는 실제로 동작할 어플리케이션의 컨테이너가 생성되기 이전에 동작하고 사라질 컨테이너\n 예)  코드를 pull하고 역할만 하는 컨테이너 데이터베이스를 초기 설정하는 컨테이너 (비슷한 예로 Airflow 예제를 보면 컨테이너 실행할 때 init container에서 postgres를 설정하는 부분이 있음)      일반 컨테이너와의 차이점\n 사전 정의한 작업이 완료되면 종료되어야 함. 생명주기 옵션 사용을 사용할 수 없음. 여러 개를 정의한 경우에는 순차적으로 시작되며, 정상 종료되었을 때 다음 Init Container 가 시작됨.  구동이 실패한 경우 재시작하지만, restartPolicy가 Never로 명시되었을 때는 파드 자체의 status가 Failure로 기록되고 종료됨.      Init Container 생성 방법 : initContainers  initContainers 로 정의함. Pod가 생성될 때 initContainer 부터 먼저 실행 되고, 내부 정의된 프로세스가 완료될 때까지 실행됨. 복수개로 정의해도 됨. 위에서부터 아래로 순서대로 실행 됨.  # example apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: initContainers: - name: init-myservice image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] - name: init-mydb image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] containers: - name: myapp-container image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] Reference  Multi-Container Pod Design Patterns in Kubernetes  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-08-k8s-multiple-container-pod/","summary":"Multi-Container Pod  (처음 쿠버네티스와 Pod의 개념에 대해 설명할 때) 파드는 최소의 배포단위이기에 여러 개의 어플리케이션 컨테이너를 묶어 하나의 파드로 배포하는 것은 적절한 사용법이 아님. 그러나, 하나의 파드에 복수의 컨테이너를 함께 구성하여 배포하는 방식이 충분히 고려될 수 있음.  다른 컨테이너를 통해 실행되는 서비스가 메인 어플리케이션을 보조하는 경우 동일 스토리지 또는 네트워크의 공유하는 서비스가 필요한 경우 동일한 라이프사이클을 이루는 경우  생성, 배포, 삭제, 스케일 업다운이 동일하게 적용되어야 하는 경우.   예) 웹서버와 별도로 로깅 서비스가 함께 페어를 이룬 경우   따라서 굳이 별도의 파드에 분리할 필요가 없음.","title":"[KR] Kubernetes - Multi-Container Pod"},{"content":"Namespace  Namespace 는 쿠버네티스 내 존재하는 가상의 공간 클러스터 내에서 오브젝트들과 리소스 그룹을 \u0026ldquo;논리적\u0026quot;으로 분리함. (물리적 분리가 아님) 작은 규모의 클러스터에서는 그냥 default에서 작업하면 되지만, 엔터프라이즈나 프로덕션 환경에서는 Namespace를 사용하는 것이 좋음  적용 예시  사용자에 따라 Namespace 접근 권한을 다르게 부여할 수 있음. Namespace마다 다른 정책을 부여할 수 있음. Namespace 별로 리소스 할당량(resource quota)을 지정/정의할 수 있음  Namespace의 유형 Default Namespace  Cluster가 처음 생성될 때 K8s가 default로 생성함 프로덕션 클러스터의 경우 Default Namespace를 사용하지 않고, 다른 Namespace를 만드는 것이 권장됨.  kube-system  쿠버네티스가 네트워킹 DNS서비스 등 내부적인 용도로 생성함 사용자가 사용, 편집, 삭제 등을 하지 못 하도록 격리함  kube-public  유저에게 제공되는 리소스들이 위치한 Namespace  사용 예시 다른 Namespace에 위치한 Pod 접근하기  다른 Namespace 의 자원에 접근하려고 하면, 해당 Namespace를 명시해야 함  ex)  msql.connect(\u0026quot;db-service\u0026quot;) - 동일 Namespace 내 msql.connect(\u0026quot;db-service.dev.svc.cluster.local\u0026quot;) - 다른 Namespace에 위치한 자원   DNS 형식  {SERVICE}.{NAMESPACE}.{sub domain of SERVICE}.{default domain for CLUSTER}      Namespace 내 Pod 조회하기 # Namespace 이름으로 필터링 kubectl get pods --namespace=kube-system 특정 Namespace 내부에 Pod 생성하기 kubectl create -f pod-definition.yml --namespace=dev # 또는 yaml file 내부에 metadata 아래 namespace 를 명시해도 됨 Namespace 생성하기 apiVersion: v1 kind: Namespace metadata: name: dev kubectl create -f namespace-dev.yaml\nOR\nkubectl create namespace dev\n특정 Namespace를 기본으로 설정하기 kubectl config set-context $(kubectl config current-context) --namespace=dev 리소스 할당 정의하기 apiVersion: v1 kind: ResourceQuota metadata: name: compute-quota namespace: dev spec: hard: pods: \u0026#34;10\u0026#34; requests.cpu: \u0026#34;4\u0026#34; requests.memory: 5Gi limits.cpu: \u0026#34;10\u0026#34; limits.memory: 10Gi kubectl create -f compute-quota.yaml\nReference  Kubernetes Documentation Kodekloud  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-05-k8s-namespace/","summary":"Namespace  Namespace 는 쿠버네티스 내 존재하는 가상의 공간 클러스터 내에서 오브젝트들과 리소스 그룹을 \u0026ldquo;논리적\u0026quot;으로 분리함. (물리적 분리가 아님) 작은 규모의 클러스터에서는 그냥 default에서 작업하면 되지만, 엔터프라이즈나 프로덕션 환경에서는 Namespace를 사용하는 것이 좋음  적용 예시  사용자에 따라 Namespace 접근 권한을 다르게 부여할 수 있음. Namespace마다 다른 정책을 부여할 수 있음. Namespace 별로 리소스 할당량(resource quota)을 지정/정의할 수 있음  Namespace의 유형 Default Namespace  Cluster가 처음 생성될 때 K8s가 default로 생성함 프로덕션 클러스터의 경우 Default Namespace를 사용하지 않고, 다른 Namespace를 만드는 것이 권장됨.","title":"[KR] Kubernetes - NameSpace"},{"content":" 데이터를 영구적으로 저장하는 매커니즘 Persistent Storage\n PV / PVC 의 주요 개념 PV (Persistent Volume)  K8s 어드민이 설정한 클러스터 레벨의 스토리지 볼륨군이며, 클러스터 리소스의 일ㅈ오. 관리자가 프로비저닝하거나 Storage Class를 통해 동적으로 프로비저닝 됨 일반 볼륨과의 차이점  일반 볼륨은 Pod와 같은 라이프사이클을 가짐. 함께 생성되고 함께 내려간다는 뜻 PV는 Pod와 별개의 라이프사이클을 가지므로, Pod가 종료되어도 PV에 기록된 데이터는 삭제되지 않음    PV 생성하기 # ex) pv-definition.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-vol-1 spec: accessModes: # 스토리지 볼륨 접근 방식 - ReadWriteOnce  capacity: storage: 1Gi # PV로 선점하고자 하는 용량 # 볼륨 타입 정의 hostPath: # hostPath 는 해당 노드의 local directory이므로, 프로덕션에서는 권장되지 않음. path: /tmp/data awsElasticBlockStore: volumeID: \u0026lt;volume-id\u0026gt; fsType: ext4  생성: kubectl create -f pv-definition.yaml 조회: kubectl get persistentvolume  accessModes 유형\n ReadOnlyMany ReadWriteOnce ReadWriteMany  PVC (Persistent Volume Claim)  Persistent Volume (PV) 을 사용할 수 있게끔 유저가 생성하는 오브젝트 PVC가 생성되면 K8s는 이를 기반으로 PVC에서 요청한 기준에 부합하는 (그리고 용량이 여유가 있는) PV를 찾아 PVC 와 바인드(bind) 함 만일 조건에 부합하는 PV가 여러개 존재하는 경우에는 label과 selector를 써서 매칭 함 만일 최적의 매칭 조합을 찾지 못해 PVC에서 정의된 용량보다 더 큰 용량의 PV와 바인드가 되면, 남아도는 용량은 다른 PVC가 사용할 수 없게 됨 (PV와 PVC는 1:1 매칭) 이후 더 이상 가용할 수 있는 PV가 없는 상태에서 PVC가 생성되면, 매칭 가능한 PV가 추가되기 전까지는 바인딩이 pending 됨. 만일 다음과 같이 생성된 PV가 유일한 PV이고, 이어서 PVC가 생성된다면, 용량은 부합하지 않지만, 다른 선택지가 없으므로 매칭됨.  PVC 생성하기 PV 정의 및 생성\n# ex) pv-definition.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-vol-1 spec: accessModes: - ReadWriteOnce  capacity: storage: 1Gi  awsElasticBlockStore: volumeID: \u0026lt;volume-id\u0026gt; fsType: ext4 persistentVolumeReclaimPolicy: Retain # default PV에 매칭되는 PVC 정의\n# ex) pvc-definition.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi  생성: kubectl -f pvc-definition.yaml 조회: kubectl get persistentvolumeclaim  PVC 바인딩 방법  PVC를 사용하고자 할 때, Pod 정의는 아래 yaml 파일과 같음. ReplicaSet과 Deployment에서도 동일하게 적용  # ex) pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: random-number-generator spec: containers: - image: alpine name: alpine command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;shuf -i 0-100 -n 1 \u0026gt;\u0026gt; /opt/number.out;\u0026#34;] volumeMounts: - mountPath: /opt name: data-volume # 볼륨 생성 volumes: - name: random-number-generator persistentVolumeClaim: claimName: myclaim PVC 삭제  삭제: kubectl delete persistentvolumeclaime \u0026lt;pvc-name\u0026gt;  단, PVC가 삭제되더라도 persistentVolumeReclaimPolicy: Retain 으로 설정되어 있기 때문에, 매뉴얼로 삭제하지 않는 한, PV는 삭제되지 않음.\npersistentVolumeReclaimPolicy: Delete 으로 설정된 경우, PVC가 삭제되면 PV도 함께 삭제됨.\nPV / PVC 의 라이프사이클 (Lifecycle of a volume) 2.1. 프로비저닝  정적 프로비저닝  스토리지 기술을 명세해서 PV를 사전에 만드는 방식 리소스가 한정된 온프레미스 환경에서 사용 PV 생성 이후 PV 리소스를 사용하겠다는 PVC를 생성  PVC에서 storageClassName: '' 의 설정 유의 미리 생성한 PV 안에서 가능한 PV를 바인딩하겠다는 뜻   이후 Pod에서 PVC를 참조하면 PV를 사용할 수 있게 됨 Note:  PVC를 설정하지 않고, Pod 생성 시 곧장 PV를 참조할 수도 있음 PVC 활용은 기존 Storage의 기술을 자세히 알지 못 하더라도 PV만 명시하면 K8s가 알아서 자원을 할당시켜주기 때문에 용이함. (따라서 권장사항)     동적 프로비저닝  PV 프로비저너를 배포하고 storage class만 명시하면 동적으로 프로비저닝 함. PVC에서 stroageClassName만 제공하면 프로비저너가 알아서 PV를 생성해주고 PVC와 연결도 해줌. GKE를 비롯한 Cloud Service에서는 PV 프로비저너를 제공함.    2.2. 바인딩  PV와 PVC가 연결되는 동작. 정적 프로비저닝  PVC를 배포하면 쿠버네티스가 가능한 PV안에서 PVC를 바인딩해줌. 동적 프로비저닝의 경우는 Storage Class의 Provisioner가 PV를 동적으로 생성하고 PVC와 바인딩해줌.    2.3. 사용, 사용 중  바인딩 이후로 PV는 Pod에 속해 사용 됨.  2.4. 반환 (Reclaiming)  볼륨을 다 사용한 뒤, 리소스 반환 API를 사용해 PVC 오브젝트를 삭제할 수 있음. PVC의 반환  볼륨에서 클레임 해제 볼륨에 수행할 작업을 클러스터에 알려줌   볼륨의 반환 정책  Retain  리소스를 수동으로 반환하는 정책 PVC삭제되어도 PV는 존재하기에, 관리자가 수동으로 볼륨을 반환해야 함   Delete  PV와 외부 인프라 등, 관련된 스토리지 자산을 모두 삭제함. 동적 프로비저닝 된 볼륨의 경우에는 Storage Class의 반환 정책을 상속받음.   Recycle (deprecated)    Reference Persistent Volumes\n쿠버네티스(k8s) Persistent Storage란?\n","permalink":"https://wonyoungseo.github.io/posts/2024-02-29-k8s-pv-pvc/","summary":"데이터를 영구적으로 저장하는 매커니즘 Persistent Storage\n PV / PVC 의 주요 개념 PV (Persistent Volume)  K8s 어드민이 설정한 클러스터 레벨의 스토리지 볼륨군이며, 클러스터 리소스의 일ㅈ오. 관리자가 프로비저닝하거나 Storage Class를 통해 동적으로 프로비저닝 됨 일반 볼륨과의 차이점  일반 볼륨은 Pod와 같은 라이프사이클을 가짐. 함께 생성되고 함께 내려간다는 뜻 PV는 Pod와 별개의 라이프사이클을 가지므로, Pod가 종료되어도 PV에 기록된 데이터는 삭제되지 않음    PV 생성하기 # ex) pv-definition.","title":"[KR] Kubernetes - PV \u0026 PVC"},{"content":"KubeFlow  쿠버네티스 기반 ML 워크플로우를 구축, 실행, 관리하기 위한 오픈소스 플랫폼 모델 개발과 배포를 위한 E2E 솔루션 제공 확장성과 유연성을 가지고, 다양한 머신러닝 프레임워크 통합 가능  기능적 특징\n Distributed Training Pipeline Model Serving Model Management Jupyter Notebook Integration Metadata Store  활용 예시\n 분산모델 트레이닝  대규모 데이터셋을 다루는 복잡한 머신러닝 모델을 분산환경에서 트레이닝   모델 서빙 및 배포  학습된 모델을 쿠버네티스 클러스터에 배포 효율적 리소스 관리 스케일링 통해 모델 서빙 최적화    1. Kubeflow Pipeline  ML 워크플로우를 파이프라인으로 구축 반복적이고 일관된 ML 실험을 가능케 함  주요 기능\n 파이프라인 구축  여러 ML 작업을 연결하여 복잡한 워크플로우 생성   재사용 가능한 컴포넌트  공통적인 ML 작업을 위한 컴포넌트 재사용   실험 추적 및 관리  실험 결과와 메트릭 추적 버전 관리를 통해 실험을 체계적으로 관리   자동화 및 스케줄링  모델 훈련과 평가를 자동화 정해진 스케줄에 따라 파이프라인 실행    세부사항\n 독립적 컨테이너를 기반으로 실행해서 환경의 일관성과 격리를 보장 쿠버네티스 기반. 큰 규모의 데이터셋과 복잡한 모델에 대해서 확장성을 보장 파이프라인 구성과 실행을 시각적으로 모니터링할 수 있는 웹 기반 인터페이스 제공  2. Kubeflow Serving  훈련된 머신러닝을 배포할 수 있는 서비스 프로덕션 환경에서 사용 실시간 추론을 위한 환경을 제공 스케일링 가능  주요 기능\n 텐서플로, 파이토치, XGBoost, Scikit Learn 등 다양한 ML 프레임워크 지원 대규모 트래픽과 데이터에 대응할 수 있는 확장성 있는 모델 서빙 아키텍처 제공 버전 관리 및 A/B 테스팅 트래픽의 변화에 따라 자동으로 스케일링하여 리소스를 효율적으로 관리 쿠버네티스 기반으로 REST 및 gRPC 지원  3. Katib “하이퍼 파라미터 튜닝”\n 자동화된 머신러닝 하이퍼파라미터튜닝과 신경망 아키텍처 최적화를 제공 다양한 알고리즘으로 ML 모델의 성능을 최적화 하는데 필요한 파라미터를 찾는데 중점을 둠  주요 기능\n 튜닝 알고리즘 지원 실험 관리 자동화된 모델 최적화 사용자가 통제할 수 있음 대규모 데이터셋과 복잡한 모델에 대해서도 효과적으로 튜닝 수행  4. Metadata Store  머신러닝 워크플로우의 다양한 측면을 추적하고 저장하는 구성요소 실험, 모델, 데이터셋 등의 메타데이터를 중앙화된 방식으로 관리함  주요 기능\n 메타데이터 저장 추적 및 관리 버전 관리 데이터셋 추적 유연한 저장구조 시각화 도구와 통합되어 있음  5. Jupyter Notebook Integration  주피터 노트북 환경에서 개발하고 실험할 수 있게 하는 기능 손쉽게 코드 작성, 실행, 결과 시각화 가능 Kubeflow 환경 내에서 주피터 노트북 쉽게 생성 및 관리 코드 개발 및 실험 리소스 확장성 R 과 같은 다양한 커널 지원 클라우드 저장소 연동해서 결과 저장 가능  Reference  Kubeflow Documentation  ","permalink":"https://wonyoungseo.github.io/posts/2024-02-08-kubeflow-concept/","summary":"KubeFlow  쿠버네티스 기반 ML 워크플로우를 구축, 실행, 관리하기 위한 오픈소스 플랫폼 모델 개발과 배포를 위한 E2E 솔루션 제공 확장성과 유연성을 가지고, 다양한 머신러닝 프레임워크 통합 가능  기능적 특징\n Distributed Training Pipeline Model Serving Model Management Jupyter Notebook Integration Metadata Store  활용 예시\n 분산모델 트레이닝  대규모 데이터셋을 다루는 복잡한 머신러닝 모델을 분산환경에서 트레이닝   모델 서빙 및 배포  학습된 모델을 쿠버네티스 클러스터에 배포 효율적 리소스 관리 스케일링 통해 모델 서빙 최적화    1.","title":"[KR] Kubeflow - 개념 정리"},{"content":"Network Single Node 상황  쿠버네티스에서는 내부 IP 주소가 Pod에 할당됨. 쿠버네티스 클러스터는 처음 설정될 때, internal private network 를 생성함  모든 pods는 이 네트워크의 레인지 내에서 IP를 부여받음   Pod가 재생성될 때는 또 새로운 IP를 부여받음  따라서, Pod에 부여된 IP로 접근하는 것은 적절한 방법이 아님    Multiple Nodes in a Cluster 상황  각 Node 의 IP는 다르지만, 각 노드 내 Pod의 네트워크가 같을 수 있음 하지만,  모든 Container / Pod는 NAT 없이도 서로 networking 가능 모든 Node는 NAT 없이도 서로 networking 가능   따라서 IP conflict 을 피하기 위한 조치가 필요함 pre-built solution 존재하기도 함  cisco big cloud fabric flannel vmware nsx calico cilium … etc     Service   helps connecting applications together with other applications OR userss ex)  backend server frontend application external datasource etc …    Service 개념  Service는 Kubernetes를 기반으로 하는 어플리케이션 내외의 통신과 접근을 위한 obejct Service 유형  Node Port service  node의 port 에서부터 listen(응답대기) → request 를 pod로 전달하는 역할   Cluster IP  클러스터 내 virtual IP를 생성하여 제각기 다른 service들 간의 통신을 가능케 함. ex) frontend server 와 backend server가 서로 접근할 수 있도록 사용   LoadBalance  클라우드 서비스의 로드밸런서 사용을 위한 service      Node Port Service Service 관점에서 \u0026hellip;\n TargetPort : 액세스 하고자 하는 POD의 Port Port : TargetPort와 매핑되는 Service의 Port (Cluster IP) NodePort : 외부 클라이언트에서 액세스할 때 사용하는 노드의 Port (노드는 30000 ~ 32767 사이의 포트만 가질 수 있도록 설정 됨)  Node Port 생성하기 # ex) service-definition.yml apiVersion: v1 kind: Service metadata: name: myapp-service spec: # Service를 정의하는 가장 중요한 부분 type: NodePort port: # 하나의 Service에 여러개의 포트 매핑이 가능함. 하나의 매핑 묶음은 array로 표현 - targetPort: 80 port: 80 nodePort: 30004 # port는 필수 # nodePort를 명시하지 않으면 30000~32767 사이에서 놀고 있는 포트 아무거나 쓰게 됨 # targetPort를 명시하지 않으면 port와 같은 것으로 간주함 # 해당 service를 Pod에 연결하기 selector: # 기존에 pod를 정의하면서 사용한 label를 동일하게 기입하면 해당 pod에 매핑됨 # ex) app: myapp type: front-end  kubectl create -f service-definition.yml kubectl get services  kubectl get svc   외부에서 액세스 할 수 있는 url 표기 (아래 minikube일 경우) : minikube service [SERVICE_NAME] --url  Node Port - Single / Multi 상황 Multiple Pods in Single Node 상황\n Service가 매핑된 여러개 Pod에 알아서 잘 뿌려줌 (ㄷ ㄷ ㄷ) 빌트인 로드밸런서라고 생각하면 됨 이 때 로드밸런싱 알고리즘: 랜덤 알고리즘  Multiple Nodes in Cluster 상황\n 서비스를 생성하게 되면, 클러스터 내 노드를 관통하는 서비스를 자동으로 생성함 각 노드의 포트를 묶어 하나의 노드 포트로 매핑하는 서비스  이렇게 하면 어느 노드 IP로 해도 포트가 같기 때문에 동일하게 액세스 가능    ClusterIP  백엔드, 프론트엔드, 데이터베이스, 레디스 등으로 “티어”를 구분 짓는다면 … 각 티어를 서로 연결시켜주는 service 가 필요해짐 Pod는 내리고 올리고 하면서 새로운 IP가 재부여 되기 때문에, 여기에 기댈 수 없음.  ClusterIP를 사용하게 되면\n 각 티어를 분리시켜 따로 개발 / 수정 / 배포 / 스케일링이 가능함. 그리고 서로 연결은 스무스해짐  ClusterIP 생성 # service-definition.yml apiVersion: v1 kind: Service metadata: name: back-end spec: type: ClsuterIP  # service 정의할 때 명시하지 않으면 default로 ClusterIP로 정의 됨. ports: - targetPort: 80 # 티어의 port port: 80 # service 의 포트 selector: # 마찬가지로 pod의 select에 매핑 app: myapp  type: back-end  kubectl create -f service-definition.yml kubectl get services  LoadBalancer  개별 인프라라면 nginx와 같은 로드밸런서를 세팅하겠지만, 클라우드 환경이라면, K8s에서 자체적으로 클라우드 플랫폼의 네이티브 로드밸랜서에 통합될 수 있도록 지원함  AWS, GCP, Azure 또는 그 외 supported cloud provider 일 때만 유효한 기능임    LoadBalancer 적용 # service-definition.yml apiVersion: v1 kind: Service metadata: name: myapp-service spec: type: LoadBalancer port: - targetPort: 80 port: 80 nodePort: 30008 Reference  kubernetes official document Kodekloud Course  ","permalink":"https://wonyoungseo.github.io/posts/2024-01-25-k8s-trial-network-service/","summary":"Network Single Node 상황  쿠버네티스에서는 내부 IP 주소가 Pod에 할당됨. 쿠버네티스 클러스터는 처음 설정될 때, internal private network 를 생성함  모든 pods는 이 네트워크의 레인지 내에서 IP를 부여받음   Pod가 재생성될 때는 또 새로운 IP를 부여받음  따라서, Pod에 부여된 IP로 접근하는 것은 적절한 방법이 아님    Multiple Nodes in a Cluster 상황  각 Node 의 IP는 다르지만, 각 노드 내 Pod의 네트워크가 같을 수 있음 하지만,  모든 Container / Pod는 NAT 없이도 서로 networking 가능 모든 Node는 NAT 없이도 서로 networking 가능   따라서 IP conflict 을 피하기 위한 조치가 필요함 pre-built solution 존재하기도 함  cisco big cloud fabric flannel vmware nsx calico cilium … etc     Service   helps connecting applications together with other applications OR userss ex)  backend server frontend application external datasource etc …    Service 개념  Service는 Kubernetes를 기반으로 하는 어플리케이션 내외의 통신과 접근을 위한 obejct Service 유형  Node Port service  node의 port 에서부터 listen(응답대기) → request 를 pod로 전달하는 역할   Cluster IP  클러스터 내 virtual IP를 생성하여 제각기 다른 service들 간의 통신을 가능케 함.","title":"[KR] Kubernetes - Network, Service"},{"content":"Pod 쿠버네티스에서 Pod를 정의하는 정의하는 yaml의 형태는 다음과 같다.\n(Pod 뿐만 아니라 다른 오브젝트도 동일)\n# pod-definition.yml apiVersion: kind: metadata: spec: containers: - name: # - before the name indicates, its first item in the list image: Pod 관련 명령어\nkubectl create -f [FILE NAME].yml\nkubectl create -f [FILE NAME].yml --record 또는 kubectl apply -f [FILE NAME].yml\nkubectl get pods\nkubectl describe pod [POD NAME]\nLabels, Selectors 개념  Labels and Selectors act as a filter, filtering pods for ReplicsSet Labels와 Selectors는 필터로서의 기능을 함. ReplicaSet 뿐만 아니라 추후 다룰 Deployment, Service 등에서도 사용됨.  필터링 또는 파드를 선택하기 위한 용도로도 많이 사용 됨\n 파드에 label을 부여하고 selector를 통해 매치. 추후 파드가 수십 수백개가 되는 상태에서도 매니징을 가능케 함 type 유형 별 (파드, 레플리카셋, 디플로이먼트, 서비스 등등) app 애플리케이션 별 function 기능 별  ReplicaSet Replication Controllers (OLD)  High Availability  레플리카 Pod instance를 생성하고 관리함 Pod가 하나인 상황에서도, 만약 죽으면 새로 다시 올리는 역할을 함   Load Balancing \u0026amp; Scaling  multiple pod , multiple node 상황에서도 관리    Replication controller 예시\n# rc-definition.yml apiVersion: v1 # v1에서는 여전히 replication controller 지원함 kind: ReplicationController metadata: name: myapp-rc labels: app: myapp type: front-end spec: template: # Pod 생성 때 사용했던 정보를 다 가져옴  metadata: name: labels: app: type: spec: containers: - name: image: # template 과 같은 레벨에서 정의함 replicas: 3 kubectl create -f [RC FILE NAME].yml\nkubectl get replicationcontroller\nkubectl get pods\nReplicaSet 정의  Replicaiton Controller 는 deprecated 되어 대부분 ReplicaSet 으로 대체됨 생성된 복수개의 Pod 를 모니터하며, fail이 생길 경우 새 Pod를 생성함.  ReplicaSet 예시\n# replicaset-definition.yml apiVersion: apps/v1 # apps/v1 에서부터 지원 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: # 갯수 replicas: 3 # ReplicaSet에서는 어떤 Pod를 쓸지 정의  selector: matchLabels: # Pod에서 명시한 label과 같은 label에서만 레플리카를 만든다 type: front-end\t # replicas와 같은 레벨에서 정의 template: # Pod 생성 때 사용했던 정보를 그대로 다 가져옴  metadata: name: myapp-pod labels: app: myapp  # labels가 위와 매칭하도록 주의!!!!!! type: front-end spec: containers: - name: nginx-container image: nginx kubectl create -f replicaset-definitnion.yml\nkubectl get replicaset\nkubectl get pods\nReplicaSet 스케일링 방식  레플리카 갯수 업데이트  kubectl replace -f replicaset-definition.yml   이미 정의된 replica의 파라미터를 수정하기  kubectl scale --replicas=6 -f replicaset-definition.yml kubectl scale —-kubekreplicas=6 replicaset(TYPE) myapp-replicaset(NAME)    주의: 정의된 내용을 수정한다고 바로 스케일링이 되는 건 아님\n변경사항을 바로 적용하고 싶다면\n kubectl edit replicaset myapp-relicaset 으로 수정(k8s가 자체로 생선한 설정파일이므로, 여기는 수정을 주의해야 함) kubectl scale replicaset myapp-replicaset —-replicas=2 바로 얄짤 없이 적용  가장 적절한 방식은 definition yaml 파일을 수정한 후, kubectl apply를 통해 적용하는 것. (정의 파일과 현상태가 sync 된 상태를 유지할 수 있음)\nDeployment  상황  여러 개의 인스턴스를 배포한 상황일 때, 인스턴스를 업그레이드 해야 한다면, 한꺼번에 업데이트 하는 것은 바람직하지 않음 → 다운 타임 등으로 유저가 영향을 받을 수 있음   Deployment 사용  Deployment는 ReplicaSet보다 더 상위의 객체이며 배포된 여러 개의 Pod 들을 seamlessly 하게 업데이트하고 끄고 닫을 수 있게 함    Deployment 정의 # deployment-definition.yml apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deployment lables: apps: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-2container image: nginx replicas: 3 selector: matchLabels: type: front-end strategy: # strategy를 명시하지 않으면 default는 RollingUpdate 이다. type: RollingUpdate rollingUpdate: maxUnavailable: 1  kubectl create -f deployment-definition.yml kubectl get deployment kubectl explain deployment  Rollout Deployment \u0026amp; Rollback Rollout Rollout and Versioning\n 새로운 버전의 컨테이너가 생성되면 rollout 을 시행 deployment 생성 → rollout 실행 → 새로운 revision 생성 애플리케이션이 새 컨테이너로 업데이트되면 → 새로운 rollout 실행 → 새로운 revision 생성  Rollout history tracking 가능 Rollback도 가능    Rollout commands\n kubectl rollout status deploymnet/myapp-deployment kubectl rollout history deployment/myapp-deployment kubectl rollout history deployment \u0026lt;deployment-name\u0026gt; --revision=\u0026lt;revision-no\u0026gt;  특정 revision number를 통해 히스토리를 선택해서 조회할 수 있음.    Rollback 하는 방법 롤아웃을 취소하는 방식으로 롤백함.\n kubectl rollout undo deployment/\u0026lt;deployment-name\u0026gt; kubectl rollout undo deployment/\u0026lt;deployment-name\u0026gt; --to-revision=\u0026lt;revision-no\u0026gt;  특정 revision으로 롤백 -    Deployment Strategy - Recreate \u0026amp; RollingUpdate   Recreate Strategy\n 실행되는 인스턴스 다 내림 새 인스턴스 올림    One-by-one (Rolling update) - default strategy\n 하나씩 내리고 하나씩 올림 디폴트 설정    Recreate, RollingUpdate 업데이트하는 법\n kubectl edit -f deployment-definition.yml --record kubectl apply -f deployment-definition.yml  Blue-Green  우선 배포 하고 나서, 라우팅 트래픽을 한번에 새 버전으로 옮기는 방법 현행 deployment (blue) 가 띄워져 있는 상태로 업데이트 된 deployment (green)을 생성함. 이후 로드밸런서가 Green deployment를 가리키게 하여 업데이트를 완료함. 리소스가 Blue deployment만큼 배로 필요함.  적용하는 방법\n예시)\n service 가 blue에 연결되어 있는 상태. version: v1 이라는 label을 통해 서비스와 deployment가 연결되어 있음. version: v2 라는 label이 지정된 green을 배포함 green 이 모두 배포된 이후에는 service의 label을 version: v2 으로 변경시켜서 service가 green 과 연결되게끔 함  Canary 일정한 작은 비율을 새 버전으로 배포하고, 일정 테스트를 거치고 괜찮다 판단되면 나머지를 배포함\n적용 방법\n예시)\n service 가 blue에 연결되어 있는 상태. version: v1 이라는 label을 통해 서비스와 deployment가 연결되어 있음. old와 new 에게 공동퇴는 label을 부여하여 서비스가 new에도 연결되도록 함.  이 때, new에는 replica를 최소한의 수로 지정함. 이렇게 되면 old의 배포는 유지하며, new 는 작은 비율을 차지하게 됨 (canary)   new로 업그레이드 해도 되겠다 판단되면, primary를 업데이트하고, canary는 지움.  또는 primary를 scale down to 0 하고, canary의 replica를 기존 갯수대로 scale up 함 → 그리고 primary를 삭제    Reference  kubernetes official document Kodekloud Course  ","permalink":"https://wonyoungseo.github.io/posts/2024-01-18-k8s-trial-pod-replica-deployment/","summary":"Pod 쿠버네티스에서 Pod를 정의하는 정의하는 yaml의 형태는 다음과 같다.\n(Pod 뿐만 아니라 다른 오브젝트도 동일)\n# pod-definition.yml apiVersion: kind: metadata: spec: containers: - name: # - before the name indicates, its first item in the list image: Pod 관련 명령어\nkubectl create -f [FILE NAME].yml\nkubectl create -f [FILE NAME].yml --record 또는 kubectl apply -f [FILE NAME].yml\nkubectl get pods\nkubectl describe pod [POD NAME]\nLabels, Selectors 개념  Labels and Selectors act as a filter, filtering pods for ReplicsSet Labels와 Selectors는 필터로서의 기능을 함.","title":"[KR] Kubernetes - Pod / ReplicaSet / Deployment"},{"content":"1 쿠버네티스 개념 아키텍처 1.1. 개념  컨테이너화 된 어플리케이션의 배포, 확장, 운영을 자동화하기 위한 오픈소스 시스템 구글에 의해 개발됨. CNCF에 기반을 둠  1.2. 주요 특징  자동화된 롤아웃 및 롤백  어플리케이션 업데이트 시 롤아웃을 자동으로 관리 문제 발생 시 이전 버전으로 롤백   서비스 접근 및 로드 밸런싱  클러스터 내의 어플리케이션에 쉬벡 접근 트래픽을 자동으로 분산   스케일링  리소스의 사용에 따라 자동 또는 수동으로 스케일링   자체 회복  실패한 컨테이너 재시작. 건강하지 않은 컨테이너는 교체 준비되지 않은 노드로부터 어플리케이션 이전    2. 쿠버네티스 아키텍쳐와 주요 구성 요소    2.1. 아키텍처  Master ← → Node 구조로 이루어진 클러스터를 사용  마스터 컴포넌트  API 서버 (kube-apiserver): 쿠버네티스 API를 제공하며, 사용자와 내부 컴포넌트 간의 중재자 역할을 함 스케줄러 (kube-scheduler): 새로 생성된 파드를 어떤 노드에 할당할 지 결정 컨트롤러 매니저 (kube-controller-manager): 여러 컨트롤러를 실행. 노드 컨트롤러, 레플리케이션 컨트롤러 등이 존재함 etcd: 모든 클러스터 데이터를 저장하는 경량의 분산 키-값 저장소  노드 컴포넌트 Pod가 할당되어 있는 공간\n kubelet : 각 노드에서 실행되며, 파드 스펙(Spec)에 설명된 대로 컨테이너가 실행되고 있는지 확인 kube-proxy: 각 노드의 네트워크 규칙을 관리하여 네트워크 통신을 가능케 함 컨테이너 런타임: 컨테이너 실행을 담당하는 소프트웨어 (Docker, containerd, CRI-O 등)  2.2. 기능  클러스터 구성  먼저 쿠버네티스 클러스터를 구성. 클러스터는 여러 노드 (물리적 또는 가상머신)와 이들을 관리하는 마스터 노드로 구성됨 etcd 는 클러스터 관리에 사용되는 모든 데이터를 분산된 Key-Value 형태로 저장함. 마스터 간 충돌이 없도록 클러스터 내에서 잠금을 구현함.   API 서버와의 통신  사용자는 kubectl CLI 또는 API를 통해 API 서버와 통신함 이를 통해 파드의 생성, 업데이트, 삭제 등의 작업을 요청함   스케줄링과 실행  스케줄러는 새로운 파드에 대해 가장 적합한 노드를 선택함. kubelet은 해당 노드에서 파드의 컨테이너가 예상대로 실행되도록 관리함.   서비스 관리  kube-proxy는 서비스를 통한 네트워크 트래픽을 관리. 서비스는 파드 그룹에 안정적인 접근성을 제공함.   자동화된 롤아웃 및 롤백  Deployment를 통해 어플리케이션의 업데이트, 롤아웃 및 롤백을 자동으로 관리함   스케일링과 자체 치유  어플리케이션의 수요에 따라 자동으로 스케일링함. 실패한 파드를 재시작 하는 등의 자체 치유 기능을 제공함.    2.3. 주요 구성 요소 2.3.1. Pod 개념\n 쿠버네티스에서 배포할 수 있는 가장 작은 작업 단위 하나 이상의 컨테이너를 포함할 수 있으며, 이들은 스토리지와 네트워크를 공유 공유해야한다면 하나의 파드에서 구동시키는 것이 적합함  특징\n 컨테이너 그루핑:  하나 이상의 밀접하게 관련된 컨테이너를 그룹화 이 컨테이너들은 같은 컴퓨팅 리소스를 공유함   공유 리소스  파드 내 컨테이너는 같은 IP 주소와 포트공간을 공유함 서로 localhost를 통해 통신   일시적인 성격:  파드는 일시적. 파드가 삭제되면 그 안의 컨테이너도 함께 삭제됨 따라서 파드는 변경 가능한 리소스로 간주됨.   생명주기:  파드는 생성되고, 실행되고, 종료될 때 까지의 생명주기를 가짐. 파드가 종료되면, 쿠버네티스 클러스터에서 제거됨    사용 예\n 단일 컨테이너 파드: 대부분의 파드는 하나의 컨테이너만을 실행함 멀티 컨테이너 파드: 로깅, 데이터 백업, 데이터 처리와 같은 보조 기능을 수행하는 사이드카(sidecar) 컨테이너를 함께 포함하는 경우  2.3.2. Service 개념\n 서비스는 파드의 집합에 대한 안정적인 네트워크 주소를 제공함 서비스를 통해 파드 집합에 대한 접근을 관리하고, 로드 밸런싱 및 서비스 발견이 가능함 (service discovery) → 그냥 서비스 디스커버리라고 하면 되지, 자꾸 서비스발견 서비스발견 거려서 뭔말인가 했네  특징\n 안정적인 주소 제공: 서비스는 파드 집합에 지속적으로 접근할 수 있는 안정적인 IP주소와 포트를 제공함 로드 밸런싱: 서비스는 요청을 여러 파드에 분산시켜 로드 밸런싱을 수행함 서비스 발견: Service Discovery 서비스 타입: 다양한 서비스타입을 통해 다양한 네트워크 요구사항을 충족함  ClusterIP NodePort LoadBalancer ExternalName    사용 예\n ClusterIP: 클러스터 내부에서만 접근 가능한 서비스를 만들 때 사용 LoadBalancer: 클라우드 제공 업체의 로드밸런서를 사용하여 서비스에 대한 외부 접근을 관리할 때 사용  2.3.3. Deployment 개념\n 쿠버네티스에서 파드와 레플리카셋의 상태를 선언적으로 관리하는 API 오브젝트 어플리케이션의 배포, 업데이트, 스케일링 등을 자동화 하고 관리함  특징\n 자동화된 롤아웃과 롤백: 새로운 버전을 롤아웃하고 필요한 경우 이전 버전으로 롤백하는 프로세스를 자동화 상태 관리: 원하는 상태 (Desired State)를 정의하고, 쿠버네티스가 현상태(Current State)를 앞서 정의한 상태대로 유지함 선언적 업데이트: YAML 파일이나 JSON 형식을 사용하여 애플리케이션을 업데이트하는 방식을 선언함 스케일링: 파드의 수를 수동 또는 자동으로 조절하여 애플리케이션을 스케일링함  사용 예\n 새 버전 배포 스케일링: kubectl scale deployment 명령어를 사용하여 확장하거나 축소  2.3.4. ReplicaSet 개념\n 레플리카셋은 파드의 복제본을 유지 관리하는 쿠버네티스 오브젝트 파드의 원하는 복제본 수를 지정함 지정된 수의 파드 복제본이 항상 실행되고 있도록 보장함  특징\n 복제본 수 관리: 지정된 수의 파드 복제본을 유지함 자체 치유: 실패한 파드를 자동으로 대체하여 복제본 수를 유지함 유연한 파드 선택: 레이블 선택기(Label Selector)를 사용하여 관리할 파드를 결정함  사용 예\n 어플리케이션 가용성 보장: 레플리카셋은 어플리케이션의 가용성을 높이기 위해 여러 파드의 복제본을 두고 실행함. 부하 분산: 덕분에 트래픽이 분선되고 부하가 복제본에 균등하게 분배됨.  2.3.5. StatefulSet 개념\n Stateful한 pod를 관리하기 위한 controller.  특징\n pod들의 고유성과 순서를 보장함.  사용 예\n 마스터 노드가 가동된 후 순차적으로 워커 노드가 가동되어야 하는 경우를 보이는 database. Persistent Volume(PV)을 개별 포드로 생성하여 연결함. Pod가 비정상적으로 종료된 경우에도, 새로운 Pod가 PV를 담당함.  Reference  kubernetes official document Kodekloud Blog Post  ","permalink":"https://wonyoungseo.github.io/posts/2024-01-15-k8s-concept/","summary":"1 쿠버네티스 개념 아키텍처 1.1. 개념  컨테이너화 된 어플리케이션의 배포, 확장, 운영을 자동화하기 위한 오픈소스 시스템 구글에 의해 개발됨. CNCF에 기반을 둠  1.2. 주요 특징  자동화된 롤아웃 및 롤백  어플리케이션 업데이트 시 롤아웃을 자동으로 관리 문제 발생 시 이전 버전으로 롤백   서비스 접근 및 로드 밸런싱  클러스터 내의 어플리케이션에 쉬벡 접근 트래픽을 자동으로 분산   스케일링  리소스의 사용에 따라 자동 또는 수동으로 스케일링   자체 회복  실패한 컨테이너 재시작.","title":"[KR] Kubernetes - 개념 정리"},{"content":"문서 작성 \u0026hellip; 애써 외면해본다 개발 자체만큼이나 중요한 부분이 문서 작성이라는 것을 부정하는 개발자는 없다. 하지만 문서를 보고 문서를 작성한 개발자를 탓하고, 나의 동료를 탓하고, 더 나아가 과거에 그 문서를 작성했던 사람이 나 자신이었음을 깨닫고 소위 말하는 현자타임에 빠지는 경우를 자주 경험해봤을 것이다(나만 그런 건 아니지?). 성숙한 개발 조직이 아닐 수록, 이런 경우를 많이 맞닥뜨릴 때가 있다. 소수의머리로는 아는 사실을 마음은 애써 외면하는 걸까?\n(1) 가이드라인 없거나, (2) 어떻게 작성해야할 지 막막하거나, (3) 사내에서 잘 작성된 문서라는 기준이 정해져있지 않거나, (4) 중요하다고 인식되지 않아 충분한 시간이 주어지지 않거나 시간을 들여도 인정 받기 어려워서 \u0026hellip; 일 것이다. 이 모든 것이 전부 다 해당될 수도 있다.\nDocs for Developers 한빛미디어에서 출간된 Docs for Developers 기술 문서 작성 완벽 가이드 는 제목 그대로 기술문서를 \u0026ldquo;제대로\u0026rdquo; 작성하기 위한 가이드를 담고 있다. 개인적으로 이 책의 처음 목차를 보았을 때, 내용이 탄탄할 것이라는 기대가 들었고, 실제로 책을 통해 기술 문서를 작성할 때 막막했던 부분들을에 대한 조언을 많이 받을 수 있었다.\n   Docs for Developer 기술문서 작성 완벽 가이드 (저자: 자레드 바티 외 4인)\n  내용 훑어보기 기술 문서를 작성하기 위한 첫 발자국부터, 문서의 유지보수와 더 이상 유효하지 않은 문서를 종료하는 단계까지 전체적인 내용을 훑고 있다. 특히 가상의 서비스를 사례로 들어 실제로 문서를 기획하고 구축해나가는 과정을 통해서 내용을 효과적으로 전달하고 있는 것이 이 책의 장점이다.\n 독자 이해하기  기술 문서의 사용자를 먼저 파악해봄  사용자 니즈 사용자 패르소나 사용자 여정     문서화 계획하기  문서의 유형  코드 README 문서 시작하기 문서 개념 문서 절차 문서 참조 문서     문서 초안 만들기 문서 편집하기  편집 접근법  기술적 정확성 -\u0026gt; 정확한 명명? 혼동을 일으킬만한 용어 은어? 독자가 기대한 결과 완전성 -\u0026gt; Todo, TBD가 채워졌는지? 성공적인 사용을 위한 정보가 다 들어가있는지? 구조 -\u0026gt; 흐름과 분류 명확성/간결성 -\u0026gt; 표현 / 불필요한 부분 / 편향된 언어   편집 프로세스  예) 작성자 검토 -\u0026gt; 동료 검토 -\u0026gt; 기술적 검토     샘플코드 통합하기  샘플코드의 복잡성을 단계별로 명확하게 나누기   시각적 콘텐츠 추가하기  스크린샷 다이어그램 (플로우차트, 스윔레인) 비디오 콘텐츠 유지하기   문서 배포하기  릴리스 프로세스 구축 타임라인 만들기 전달 방식 (매체) 문서가 배포됨을 알리기   피드백 수집하고 통합하기  피드백 채널  직접 수집 고객지원 업무 중 문제 모니터링 문서의 고객 평가 수집 사용자 설문조사 / 사용자 위원회   후속 조치   문서 품질 측정하기  기능적 품질 : 이루고자 하는 목표를 달성하는지 (정확한 내용 전달 등등) 구조적 품질 : 얼마나 \u0026ldquo;잘\u0026rdquo; 작성되었는지 (명확, 간결 등등)   문서 구조화 하기  문서는 양이 커짐 \u0026hellip; 문서 콘텐츠를 올바르게 구조화 하기  사이트 탐색 구조 랜딩 페이지 탐색 신호 (검색)     문서 유지 관리 및 지원 중단하기  문서 최신 유지하기 담당자 선정 유지 관리 자동화 문서 삭제    아쉬운 부분 Docs for Developer 에서 약간 아쉬웠던 부분은 번역이었다. 많은 개발 용어들이 영문 표현에 기반을 두고 있는데, 개인적으로는 영어 표기를 어느 정도 그대로 따라가는 것이 추후 내가 책을 통해 얻은 정보와 현업에서 사용되는 용어를 연관시키고 역량을 확장시키는데 도움이 된다고 생각한다. 하지만 번역으로는 쉽게 와닿지 않는 부분이 있어 \u0026ldquo;원서나 실제 영미권에서는 이 용어를 어떻게 쓰는지\u0026rdquo; 의문이 들어 중간 중간 추가적으로 찾아봐야 했던 상황이 종종 있었다. (예: 마찰 로그 -\u0026gt; Conflict logging) 출판사 정책 등으로 굳이 개발 용어를 번역 해야한다면, 괄호 또는 주석으로 영문을 함께 표기하는 것이 개발자인 대다수의 독자에게 내용을 전달하는게 더 편리하지 않을까 하는 인상이 들었다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2023-05-27-book-review-docs-for-developers/","summary":"문서 작성 \u0026hellip; 애써 외면해본다 개발 자체만큼이나 중요한 부분이 문서 작성이라는 것을 부정하는 개발자는 없다. 하지만 문서를 보고 문서를 작성한 개발자를 탓하고, 나의 동료를 탓하고, 더 나아가 과거에 그 문서를 작성했던 사람이 나 자신이었음을 깨닫고 소위 말하는 현자타임에 빠지는 경우를 자주 경험해봤을 것이다(나만 그런 건 아니지?). 성숙한 개발 조직이 아닐 수록, 이런 경우를 많이 맞닥뜨릴 때가 있다. 소수의머리로는 아는 사실을 마음은 애써 외면하는 걸까?\n(1) 가이드라인 없거나, (2) 어떻게 작성해야할 지 막막하거나, (3) 사내에서 잘 작성된 문서라는 기준이 정해져있지 않거나, (4) 중요하다고 인식되지 않아 충분한 시간이 주어지지 않거나 시간을 들여도 인정 받기 어려워서 \u0026hellip; 일 것이다.","title":"[KR] 책 리뷰 :  Docs for Developers 기술 문서 작성 완벽 가이드"},{"content":"   FastAPI를 사용한 파이썬 웹개발 (저자: 압둘라지즈 압둘라지즈 아데시나)\n  파이썬 개발자를 위한 FastAPI 입문서 업무를 하던 중 사내 대상으로 모델을 서빙하는 API를 만들어야 했던 시기가 있었는데, FastAPI를 기반으로 만들게 되었다. FastAPI라는 도구도 처음이었거니와, API를 만든다는 것 자체가 처음이어서 상당한 기간 헤맸던 기억이 난다.\n이번에 한빛미디어에서 출간한 \u0026ldquo;FastAPI를 사용한 파이썬 웹개발\u0026rdquo; 에서 밝힌 대상 독자는 \u0026ldquo;웹 API 구축에 관심 있는 파이썬 개발자\u0026rdquo; 이다. 책의 내용은 대부분 익숙하지만, FastAPI를 몰랐던 과거의 내가 읽었다면 얼마나 도움이 되었을지에 초점을 맞추어보았다.\nFastAPI 한바퀴 책은 아래와 같이 FastAPI를 구성하는 기본적인 요소들을 다루고 있으며, 예제를 따라가면서 실습을 할 수 있는 구조로 되어 있다.\n 라우팅 방법 Pydantic을 적용한 검증 Response, Error, Status Template 어플리케이션 설계 Database 보안, 인증 테스트 배포  가장 간단한 To-do 어플리케이션을 개발하는 과정에서, 위의 요소들을 어떻게 활용하는지에 대한 예제가 짜임새 있게 구성되어 있다. 가장 참고하기 좋았던 부분은 FastAPI 가 비교적 개발하기 쉽고 유연한 도구지만, 프로젝트가 더 복잡해질 경우를 고려했을 때 어플리케이션을 어떻게 \u0026ldquo;구조화\u0026quot;하는 것이 좋은 방향인지, 라우트와 모델을 활용하여 설명한 챕터였다.\n아쉬운 부분 이 책의 한계점 또는 아쉬운 부분을 짚어보면 여러가지가 있다.\n  이 책은 웹개발에 대한 개념을 깊이 있게 설명하지는 않는다. 따라서 파이썬 개발자이지만 다른 직군으로서 웹개발에 대한 지식이 없다면, 예제를 보아도 도움이 되지 않거나 이해가 되지 않을 것이다.\n  웹 어플리케이션을 만드는 것에 있어 빠질 수 없는 로깅에 대한 부분이 빠져있다. 개인적으로 FastAPI를 처음 사용할 때, 로깅의 best practice에 대한 정보를 찾기가 어려워서 시간을 많이 소요한 적이 있었는데, 이 책에서는 어떻게 다루었을지 궁금했었다. 로깅에 대한 내용이 없어 아쉬웠던 부분.\n  배포가 도커 이미지를 빌드하고 로컬 환경에서 실행하는 단계까지만 다루어져 있다. 가벼운 어플리케이션 예제라면 클라우드 무료 인스턴스를 활용한 배포까지도 다루었으면 어땠을까 하는 생각이 있다. (물론 필수인 부분은 아니지만)\n  이러한 점들을 놓고 봤을 때, 나 같은 데이터 직군의 파이썬 개발자가 처음 접하고 내용을 숙지하기에는 약간은 아쉬운 부분이 있다. 오히려 “웹 API개발 경험이 있지만 FastAPI는 처음 사용해보는 개발자”에게 적합하지 않을까?\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2023-04-19-book-review-web-with-fastapi/","summary":"FastAPI를 사용한 파이썬 웹개발 (저자: 압둘라지즈 압둘라지즈 아데시나)\n  파이썬 개발자를 위한 FastAPI 입문서 업무를 하던 중 사내 대상으로 모델을 서빙하는 API를 만들어야 했던 시기가 있었는데, FastAPI를 기반으로 만들게 되었다. FastAPI라는 도구도 처음이었거니와, API를 만든다는 것 자체가 처음이어서 상당한 기간 헤맸던 기억이 난다.\n이번에 한빛미디어에서 출간한 \u0026ldquo;FastAPI를 사용한 파이썬 웹개발\u0026rdquo; 에서 밝힌 대상 독자는 \u0026ldquo;웹 API 구축에 관심 있는 파이썬 개발자\u0026rdquo; 이다. 책의 내용은 대부분 익숙하지만, FastAPI를 몰랐던 과거의 내가 읽었다면 얼마나 도움이 되었을지에 초점을 맞추어보았다.","title":"[KR] 책 리뷰 : FastAPI를 사용한 파이썬 웹개발"},{"content":"MLOps 시스템에서 Feature Store는 빠지지 않는 구성 요소 중 하나이다. 하지만 여타 다른 구성요소들과는 달리 한번에 이해가 잘 되지 않아, 이번 기회에 개념을 정리해본다.\nFeature Store는 무엇인가? Feature Store는 머신러닝 모델 또는 데이터사이언스 프로젝트에 사용되는 Feature를 저장, 관리, 제공하는 중앙저장소 역할을 수행하기 위해 등장한 개념이다. Feature Store가 있음으로서 여러가지 모델에 사용되는 반복적인 Feature Engineering 작업을 줄이고, Feature들을 관리하고 추적할 수 있다.\nFeature Store는 왜 필요한가? 개인적으로 처음에는 잘 와닿지 않았다. 아마도 실무를 하던 각각의 ML엔지니어, 데이터사이언티스트들이 각자 담당하던 모델의 실험과 서비스의 PoC 단계의 업무만 해와서 그럴 수도 있었다. 마침 이러한 예시가 있어 Feature Store를 잘 이해할 수 있게 되었다.\nE-commerce 기업의 Feature Store. 기업 \u0026ldquo;가나다\u0026quot;는 제품에 대한 고객 사용 후기 데이터를 기반으로 ML모델을 개발하고자 한다. ML조직의 각 팀이 맡은 프로젝트는 다음과 같다.\n 팀A - 제품 사용 후기의 [긍정/부정] 분류 모델 팀B - 텍스트 기반 개인화 추천시스템 팀C - 유해발언 탐지 모델  각 팀은 모델링 과정에서 여러가지 실험을 하고 있으며, 다양한 feature representation을 사용하고 있다.\n W2V 기반 embedding BERT 기반 embedding Sentence-BERT 기반 embedding  각 팀들은 Feature Store가 있기에 매 실험마다 Embedding 을 생성할 필요가 없게 되었고, Feature Store에 저장된 각기 다른 Embedding을 가져와 사용할 수 있게 되었다. 또한 각 Embedding은 각기 다른 데이터 정제, tokenizer 등이 사용되었는데, 이 또한 Feature Store에 기록되어 있어 효율적인 모델링을 진행할 수 있게 되었다.\n이를 가지고 다양하게 모델을 학습할 수 있음.\n각 팀은 다양한 목적을 위해 다양한 모델 구조를 적용하고 실험할 수 있음\nFeature Store의 효과 위의 예시를 통해서 Feature Store의 도입은 다음과 같은 효과가 있다\n 한번 생성한 Feature를 여러 팀이 여러번 재사용 가능 (\u0026ldquo;compute once, used by many\u0026rdquo;) 각기 다른 팀 또는 조직에서 사용하는 Feature가 중앙집중식으로 관리되기 때문에 각 팀의 도메인에 특화된 Feature Engineering 관점의 노하우나 인사이트가 공유될 수 있다. 중앙집중식으로 관리되기에 버전이 기록되고, 품질과 일관성이 보장된다.  모델에 사용된 Raw Data 생성된 Feature Feature 를 생성하는데 사용된 전처리 기법 / Feature Engineering 기법    Feature Store는 어떻게 만들어야 할까? Feature Store 는 현재 서비스 또는 구축된 인프라에 기반하여 구축하는 것이 적합하지만, 크게는 3가지 부분을 중점적으로 염두에 둬야 한다.\n Ingestion // Feature Store // Consumption\n Ingestion Feature를 생성하기 위한 Raw Data를 적재되는 부분이다. Data는 online / offline에 따라 접근하는 방식이 다르다.\n online  streaming data 의 성격을 띄며, log 또는 api call 가 online 데이터의 예시. streaming 환경에서는 Raw Data가 적재되는 즉시 Feature로 생성되어야 하므로 online process에 맞는 접근법이 필요함. streaming process를 위한 Kafka 같은 도구를 사용할 수 있음. Feature Store에는 Data ID 또는 Entity ID를 통해 Feature에 바로 접근할 수 있도록 Key-Value 형태로 저장되는 것이 적합함.   offline  batch data의 성격을 띈다. 따라서 데이터베이스에서부터 AWS S3와 같은 다양한 저장소가 사용될 수 있다 Scheduler 등을 사용해 처리하고 저장하는 것이 적합하다.    Consumption Feature Store에 저장된 Feature를 추출하여 사용하는 부분이다. 크게 Training 과 Model Serving으로 용도가 나뉠 수 있다.\n Training:  모델링 과정에서의 실험 또는 서비스를 위한 모델 학습을 위해 Feature Store로부터 Feature를 불러와 사용할 수 있다.   Model Serving  새로운 데이터에 대한 Inference Request가 발생했을 때, 이를 Feature Store를 통해 Feature를 생성하고 Feature Store에 적재하거나, 이미 있는 Entity의 경우 Feature Store에서 Feature를 바로 사용할 수 있다. 단, 이 과정에서 유의해야할 점은 서비스 단계에서 사용하는 것인만큼 Latency가 최소화 되고 빠르게 Feautre를 사용할 수 있어야 한다.    Feature Store 실제로 생성된 Feature를 저장하는 부분이다.\n 저장 및 Feature 추출  Bulk Data에 대한 Feature  train / experimentation을 위해서 Dataset에 대한 Feature를 한번에 효율적으로 추출할 수 있어야 한다. 이에 적합한 저장소는 HDFS / S3 / Database / Redshift 등이 있다.   개별 Entity 에 대한 Feature  Inference 를 위한 개별 Entity의 Feature를 빠르게 추출할 수 있어야 한다. key(Entity ID)-value(Feature) 의 dictionary 형태가 적합하며, Redis 등의 적합한 저장소로 사용된다.     Feature Registry  생성된 Feature에 대한 버저닝, 정보 추적 등 Metadata가 기록된 부분이다. 이 정보는 api를 통해서 제공도 가능하다.   Monitoring  failure / alert / logging 등 Feature Store 또한 독립된 시스템으로, 모니터링이 동반되어야 한다. 또한 저장된 Data와 Feature에 대하여 사용자의 편의를 위해 조회할 수 있도록 data availability / feature availability 가 조회될 수 있다.    마무리하며 Feature Store에 대해 알아보면서, 관리해야하는 모델이 하나라면 당장 필요한 부분은 아니지만, Data 조직의 규모가 커지고 복수의 ML기반 서비스와 모델을 관리해야 한다면 효율성과 조직의 지속가능성을 위해 필수로 갖춰야 하는 요소라는 것과, 각 조직의 니즈에 맞게 Feature Store를 구축하는 것이 가장 큰 고민이 필요한 부분이라는 점을 확실히 체감하게 되었다.\n다음 포스팅에서는 가장 널리 알려진 오픈소스 기반 Feature Store를 간단하게 사용해보면서 실제로 어떻게 적용해볼 수 있는지 알아볼 계획이다.\nReference  featurestore.org Architectures for Data Scientists and Big Data Professionals  ","permalink":"https://wonyoungseo.github.io/posts/2023-03-26-introduction-feature-store/","summary":"MLOps 시스템에서 Feature Store는 빠지지 않는 구성 요소 중 하나이다. 하지만 여타 다른 구성요소들과는 달리 한번에 이해가 잘 되지 않아, 이번 기회에 개념을 정리해본다.\nFeature Store는 무엇인가? Feature Store는 머신러닝 모델 또는 데이터사이언스 프로젝트에 사용되는 Feature를 저장, 관리, 제공하는 중앙저장소 역할을 수행하기 위해 등장한 개념이다. Feature Store가 있음으로서 여러가지 모델에 사용되는 반복적인 Feature Engineering 작업을 줄이고, Feature들을 관리하고 추적할 수 있다.\nFeature Store는 왜 필요한가? 개인적으로 처음에는 잘 와닿지 않았다. 아마도 실무를 하던 각각의 ML엔지니어, 데이터사이언티스트들이 각자 담당하던 모델의 실험과 서비스의 PoC 단계의 업무만 해와서 그럴 수도 있었다.","title":"[KR] Feature Store 의 개념"},{"content":"요즘 구인공고를 보다보면 클라우드서비스에 능숙한 부분을 필수로 요구하는 곳들이 많아지는 것을 보면서, 현재 온프레미스 업무 환경에 안주하지 않고 클라우드를 따로 공부해야겠다는 생각이 들고 있다. AWS EC2만 조금 알고 있는 상태였는데, 좋은 기회에 한빛미디어에서 입문서인 업무에 바로 쓰는 AWS 입문 를 지원 받았다.\n   업무에 바로 쓰는 AWS 입문 (저자: 김성민)\n  업무에 바로 쓰는 AWS 입문 이 책은 AWS 클라우드 서비스를 처음 다루는 사람들에게 적합한 입문서이다. IAM, EC2, S3, Lambda, RDS, DynamoDB, ECR 같은 서비스를 소개하면서, 기본적인 AWS 아키텍처와 사용법을 설명해준다. 이를 통해 AWS에 대해 처음 접하는 사람들도 쉽게 이해할 수 있도록 구성되어 있다.\n기본적으로 어떤 상황을 시나리오로서 들고, 실습이 단계별로 이루어져 있다. AWS의 각종 서비스들을 어떻게 사용해야하는지 쉽게 이해할 수 있으며, 예시 코드와 함께 실습해보면서 직접 적용해볼 수도 있는 점에서 입문자에게 친절하다. 이를 통해 어떤 서비스를 사용해야 하는지, 어떻게 구성해야 하는지에 대한 이해를 돕는다. 책의 분량이 적당하다. 입문서이기 때문에 지나치게 상세하거나 어려운 내용이 없고, 가볍게 읽을 수 있다.\n아쉬운 점은 도표가 너무 간단하다는 점을 들 수 있다. 도표가 전문성이 떨어져 보이기도 하다. 이 부분은 출판사 측에서 좀 다듬었으면 책의 완성도가 더 높아지지 않았을까 하는 생각이 든다. 분량이 적당하다는 것 또한 정말 입문서의 역할에 충실하기 때문에 어느정도 AWS와 친숙한 독자라면 내용이 부족할 수 있다.\n개인적으로 이 책을 읽기 전에는 그저 EC2, S3, ECR 정도만 아주 조금씩 사용해보았던 상태였고, 이 책을 훑어본 이후에는 어떻게 잘 활용할 수 있을지에 대한 고민이 생겼다. AWS에서 제공하는 생태계와 그 안에서 구성할 수 있는 어플리케이션은 정말 무궁무진한 것 같다. 다음 개인 프로젝트는 AWS에 기반해서 해보면 어떨까 생각이 들고, 비용을 최소화 하는 케이스로 찾아보는 게 첫 단계이지 싶다.\n총평하자면, 이 책은 AWS에 대해 처음 다루는 사람들에게 좋은 입문서이다. 쉽게 이해할 수 있도록 구성되어 있고, AWS에 들어가면 수많은 서비스들 안에서 압도될 수 있지만, 어떻게 보면 차근차근 시작할 수 있는 시작점을 제공한다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2023-02-28-book-review-intro-to-aws/","summary":"요즘 구인공고를 보다보면 클라우드서비스에 능숙한 부분을 필수로 요구하는 곳들이 많아지는 것을 보면서, 현재 온프레미스 업무 환경에 안주하지 않고 클라우드를 따로 공부해야겠다는 생각이 들고 있다. AWS EC2만 조금 알고 있는 상태였는데, 좋은 기회에 한빛미디어에서 입문서인 업무에 바로 쓰는 AWS 입문 를 지원 받았다.\n   업무에 바로 쓰는 AWS 입문 (저자: 김성민)\n  업무에 바로 쓰는 AWS 입문 이 책은 AWS 클라우드 서비스를 처음 다루는 사람들에게 적합한 입문서이다. IAM, EC2, S3, Lambda, RDS, DynamoDB, ECR 같은 서비스를 소개하면서, 기본적인 AWS 아키텍처와 사용법을 설명해준다.","title":"[KR] 책 리뷰 : 업무에 바로 쓰는 AWS 입문"},{"content":"매번 반복되는 작업 지겨우셨죠? (홈쇼핑 쇼호스트 톤으로) 업무를 하다보면 프로토타입을 만들어 공유하는 일이 종종 있다. 개발하는 하는 과정도 시간이 소요되지만, 그에 못지 않게 시간이 소요되는 부분이 초기 설정이나 프로젝트 구조를 구성하는 일이다. 이 작업이 불가피하다는 것은 알고 있지만 꽤 아깝고 또 반복작업이라 답답하게 느껴지기도 했다.\n이번 포스트에서는 이런 고민을 상당 부분 해결하는 Cookiecutter에 대해서 다뤄보려고 한다. 특히 반복적인 프로토타입, 머신러닝 프로젝트, 모델 서빙 API 측면에서 아주 잘 사용할 수 있을 것 같다고 판단되어 내용을 정리해보았다.\nCookiecutter 를 소개합니다 ! (사족: Cookiecutter를 작년 Udacity의 Machine learning DevOps Engineer nanodegree 강의를 들을 때 실습 코드를 통해서 처음 접해보았던 기억이 난다. 그 당시에는 Cookiecutter 에 대한 별다른 설명 없이 커맨드를 따라 치라고만 가이드 되어 있어서 어버버하고 그냥 지나가기만 했었던 것 같다.)\n거두절미하고 Cookiecutter는 프로젝트 / 보일러플레이트 (여러가지 용어로 불리울 듯 하다) 생성을 자동화 해주는 커맨드라인 툴이다. Cookiecutter가 요구하는 요건에 맞추어서 어떠한 템플릿을 만들어두면, 나중에 손쉽게 가져다 쓸 수 있다.\n그냥 디렉토리에 템플릿 용도로 프로젝트 하나 만들어놓고 쓰면 복사해서 쓰면 안 돼? \u0026hellip; 나도 처음에는 이런 의문이 들었다.\n하지만 Cookiecutter는 Jinja2 템플릿을 통해 미리 템플릿을 만드는 과정에서 커스터마이징과 템플릿을 활용하는 사람의 선호도에 따른 전후처리 기능을 제공한다는 점이 강력하다. 이게 Cookie를 사용하는 가장 큰 이유이다.\n간단하게 알아보는 Cookiecutter 시작하기에 앞서 Cookiecutter는 기본적으로 모든 언어의 템플릿 생성을 지원한다.\n Project templates can be in any programming language or markup format: Python, JavaScript, Ruby, CoffeeScript, RST, Markdown, CSS, HTML, you name it. You can use multiple languages in the same project template.\n 단, Cookiecutter 자체는 파이썬 기반이기에 사용하기 위해서는 OS에 파이썬이 설치되어 있어야 한다.\n설치는 여느 파이썬 패키지처럼 pip으로 설치했다. brew, conda 등 다른 방법도 있으므로 공식문서를 확인해보자.\n$ pip install cookiecutter Cookiecutter 동작 원리 Cookiecutter는 기본적으로 템플릿의 역할을 한다. 동작 원리는 다음과 같다.\n 프로젝트 템플릿 내부에 내부에 다음과 같이 Jinja2 형태의 템플릿 변수를 위치한다. {{ cookiecutter.your_key }} 입력하고자 하는 값 value는 cookiecutter.json 파일에 {\u0026quot;your_key\u0026quot;: value} 형태로 정의해놓으면 추후 Cookiecutter를 통해 프로젝트를 생성했을 때 템플릿에 value가 입력된다.  아래의 예제를 살펴보자\n예제 1) - 가장 기본적인 템플릿 아래와 같이 디렉토리를 구성해보자\n. └── cookiecutter_example ├── cookiecutter.json └── {{cookiecutter.project_name}} ├── main.py ├── .env └── README.md [main.py](http://main.py) 에서는 아주 단순하게 프로젝트명, 작성자, 프로젝트 설명, 라이선스 유형을 출력하는 템플릿을 정의해두었다. 마찬가지로 .env 에는 프로젝트명, 초기 버전, 라이선스 유형이 입력되도록 했고, [README.md](http://README.md) 파일에서도 프로젝트에 대한 정보가 자동으로 기입되도록 작성했다.\n# ./cookiecutter_example/{{cookiecutter.project_name}}/main.py if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#34;Project: {{ cookiecutter.project_name }}\u0026#34;) print(\u0026#34;Author: {{ cookiecutter.author }}\u0026#34;) print(\u0026#34;Description: {{ cookiecutter.description }}\u0026#34;) print(\u0026#34;Description: {{ cookiecutter.license }}\u0026#34;) # ./cookiecutter_example/{{cookiecutter.project_name}}/.env PROJECT_NAME={{ cookiecutter.project_name }} VERSION={{ cookiecutter.initial_version }} LICENSE={{ cookiecutter.license }} \u0026lt;!-- ./cookiecutter_example/{{cookiecutter.project_name}}/README.md --\u0026gt; # Project: {{ cookiecutter.project_name }} \u0026gt; Author: {{ cookiecutter.author }} \u0026gt; Description: {{ cookiecutter.description }} \u0026gt; License: {{ cookiecutter.license }} 그리고 Jinja2 템플릿 변수에 들어갈 내용은 cookiecutter.json 에 정의한다. key는 Jinja2 템플릿 변수와 매핑되며, value값은 default값으로 나중에 cookiecutter를 사용할 때 변경 가능하다. 하나의 변수값을 정의할 수도 있고, license 부분을 보면 알 수 있듯이 몇가지 옵션을 선택지로 정의할 수도 있다.\n// ./cookiecutter_example/cookiecutter.json { \u0026#34;project_name\u0026#34;: \u0026#34;new_project\u0026#34;, \u0026#34;license\u0026#34;: [\u0026#34;MIT License\u0026#34;, \u0026#34;GNU General Public License v3\u0026#34;, \u0026#34;Apache Software License 2.0\u0026#34;], \u0026#34;initial_version\u0026#34;: \u0026#34;0.0.1\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;hello world\u0026#34; } Cookiecutter를 활용한 프로젝트 생성하는 방법은 템플릿의 경로를 파라미터로 하여 실행하면, 커맨드라인에서 앞서 cookiecutter.json에서 정의한 템플릿 변수들의 value값을 입력하거나 선택할 수 있다. 아무것도 입력하지 않고 그대로 ENTER 를 하면 [ __ ] 에 입력된 앞서 정의한 디폴트 값이 선택된다.\n$ cookiecutter ./cookiecutter_example project_name [new_project]: my_example_project Select license: 1 - MIT License 2 - GNU General Public License v3 3 - Apache Software License 2.0 Choose from 1, 2, 3 [1]: 1 initial_version [0.0.1]: author [John Doe]: WY Seo description [hello world]: Example case for cookiecutter usage 결과를 보면 my_example_project 라는 프로젝트 경로가 생성되었고 하위 파일 내 템플릿 변수들이 의도한 대로 반영된 것을 확인할 수 있다.\n. └── my_example_project ├── main.py ├── .env └── README.md # ./cookiecutter_example/my_example_project/main.py if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#34;Project: my_example_project\u0026#34;) print(\u0026#34;Author: WY Seo\u0026#34;) print(\u0026#34;Description: Example case for cookiecutter usage\u0026#34;) print(\u0026#34;Description: MIT License\u0026#34;) # ./cookiecutter_example/my_example_project/.env PROJECT_NAME=my_example_project VERSION=0.0.1 LICENSE=MIT License \u0026lt;!-- ./cookiecutter_example/my_example_project/README.md --\u0026gt; # Project: my_example_project \u0026gt; Author: WY Seo \u0026gt; Description: Example case for cookiecutter usage \u0026gt; License: MIT License 템플릿 생성 전/후 처리를 지원해주는 Hooks Cookiecutter 에는 cookiecutter.json 에서 정의한 변수에 따라 템플릿에 어떠한 처리 동작을 수행할 수 있는 Hooks 기능이 지원된다. Hooks의 전후처리 로직은 파이썬 또는 쉘 스크립트로 정의할 수 있으며 hooks 폴더 아래 위치해야 한다. 파일 네이밍은 아래와 같이 고정되어야 한다. hooks 폴더는 cookiecutter.json 과 같은 디렉토리에 위치시키면 된다.\n 전처리: 프로젝트 생성 전 실행  pre_gen_project.py pre_gen_project.sh   후처리: 프로젝트 생성 후 실행  post_gen_project.py post_gen_project.sh    . └── cookiecutter_example ├── cookiecutter.json ├── hooks # \u0026lt;---- hooks 정의 │ ├── pre_gen_project.py │ └── post_gen_project.py └── {{cookiecutter.project_name}} ├── main.py ├── .env └── README.md 예제 2) - 전/후처리 과정이 추가된 템플릿 이번 예제에서는 파이썬 프로젝트에 따라 setup.py 파일의 유무를 선택할 수 있는 Hooks를 적용한다.\n우선 프로젝트 템플릿 구조에 setup.py 와 post_gen_project.py 를 작성해둔다.\n. └── cookiecutter_example ├── cookiecutter.json ├── hooks │ └── post_gen_project.py # \u0026lt;---- hooks 설정 └── {{cookiecutter.project_name}} ├── main.py ├── requirements.txt └── setup.py # ./cookiecutter_example/{{cookiecutter.project_name}}/setup.py import io from setuptools import find_packages, setup def long_description(): with io.open(\u0026#39;README.md\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: readme = f.read() return readme def requirements(): with io.open(\u0026#39;requirements.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: requirements = f.read() return requirements setup( name=\u0026#39;{{ cookiecutter.project_name }}\u0026#39;, version=\u0026#39;{{ cookiecutter.initial_version }}\u0026#39;, description=\u0026#39;{{ cookiecutter.description }}\u0026#39;, long_description=long_description(), author=\u0026#39;{{ cookiecutter.author }}\u0026#39;, author_email=\u0026#39;{{ cookiecutter.author_email }}\u0026#39;, license=\u0026#39;{{ cookiecutter.license }}\u0026#39;, packages=find_packages(include=[\u0026#39;{{ cookiecutter.project_name }}\u0026#39;, \u0026#39;{{ cookiecutter.project_name }}.*\u0026#39;]), install_requires=requirements(), classifiers=[ \u0026#39;Programming Language :: Python :: 3.7\u0026#39;, \u0026#39;Programming Language :: Python :: 3.8\u0026#39; \u0026#39;Programming Language :: Python :: 3.9\u0026#39; ], ) 이어서 post_gen_project.py 에는, 프로젝트 생성 이후 cookiecutter.json 에서 {\u0026quot;need_setup\u0026quot; : false} 인 경우에 setup.py 파일을 삭제하는 후처리 과정이 정의되어 있다.\n// ./cookiecutter_example/cookiecutter.json { \u0026#34;project_name\u0026#34;: \u0026#34;new_project\u0026#34;, \u0026#34;license\u0026#34;: [\u0026#34;MIT License\u0026#34;, \u0026#34;GNU General Public License v3\u0026#34;, \u0026#34;Apache Software License 2.0\u0026#34;], \u0026#34;initial_version\u0026#34;: \u0026#34;0.0.1\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;hello world\u0026#34;, \u0026#34;need_setup\u0026#34;: [true, false] } # ./cookiecutter_example/hooks/post_gen_project.py import os if \u0026#34;{{ cookiecutter.need_setup }}\u0026#34; == False: os.remove(os.path.join(os.getcwd(), \u0026#34;setup.py\u0026#34;)) 실행하면 아래와 같이 프로젝트가 생성된다.\nproject_name [new_project]: my_example_project_with_hooks Select license: 1 - MIT License 2 - GNU General Public License v3 3 - Apache Software License 2.0 Choose from 1, 2, 3 [1]: 1 initial_version [0.0.1]: author [John Doe]: WY Seo description [hello world]: Example case for cookiecutter usage with hooks Select need_setup: 1 - true 2 - false Choose from 1, 2 [1]: 2 . └── my_example_project_with_hooks ├── main.py └── requirements.txt # \u0026lt;--- setup.py 파일은 삭제됨. 실제로 바로 사용해보았다 이번에 Cookiecutter에 대해 알아보고 나서, 유용함을 느껴 평소에 자주 쓰는 Streamlit과 최근에 업무에 사용하게 된 FastAPI에 대한 템플릿을 만들어보았다.\n Cookiecutter FastAPI 템플릿 Cookiecutter Streamlit 템플릿  README.md 에 서술했듯이, Github repository로 작성된 Cookiecutter 템플릿을 바로 사용하는 방법은 두가지 방식 모두 간편하다.\n# repository URL을 사용 $ cookiecutter https://github.com/wonyoungseo/cookiecutter-fastapi.git # Github 계정명과 repository 이름을 사용 $ cookiecutter gh:wonyoungseo/cookiecutter-fastapi 마무리하며 Cookiecutter는 기존에 만들어져 있는, Github 별이 아주 많은 템플릿을 그대로 가져다 써도 되겠다는 생각이 들었다. 마치 Github Actions와 같다. 하지만 종종 나의 니즈와는 다르게 과하게 복잡하거나, 꼭 필요한 무언가가 빠져있는 경우도 많다. 따라서 일단은 필요에 맞게 직접 만들어보는 것을 추천한다. Cookiecutter 가 익숙해지는 계기가 될 수도! 아무튼 이제 프로젝트 뼈대는 뚝딱 생성해서 시간을 많이 아낄 수 있을 것 같다 !\nReference  Project templates and cookiecutter 공식 문서 Github Repository  ","permalink":"https://wonyoungseo.github.io/posts/2023-02-15-intro-cookiecutter/","summary":"매번 반복되는 작업 지겨우셨죠? (홈쇼핑 쇼호스트 톤으로) 업무를 하다보면 프로토타입을 만들어 공유하는 일이 종종 있다. 개발하는 하는 과정도 시간이 소요되지만, 그에 못지 않게 시간이 소요되는 부분이 초기 설정이나 프로젝트 구조를 구성하는 일이다. 이 작업이 불가피하다는 것은 알고 있지만 꽤 아깝고 또 반복작업이라 답답하게 느껴지기도 했다.\n이번 포스트에서는 이런 고민을 상당 부분 해결하는 Cookiecutter에 대해서 다뤄보려고 한다. 특히 반복적인 프로토타입, 머신러닝 프로젝트, 모델 서빙 API 측면에서 아주 잘 사용할 수 있을 것 같다고 판단되어 내용을 정리해보았다.","title":"[KR] Cookiecutter로 개발 프로젝트 템플릿 미리 만들어놓기"},{"content":"지인의 선물로 받은 영화추천 웹서비스로 배우는 풀스택 을 훑어보던 중, Gitlab 배포를 자동화하는 부분을 보고, 이번 기회에 동일한 과정을 내가 익숙한 Github으로 따라해보고 Github Actions에 대해서도 찍먹 해보기로 했다.\n 0. 준비 사항   서버\n API 를 배포할 AWS EC2 인스턴스 EC2 인스턴스에 연결할 때 사용될 키페어 (.pem 파일) - RSA 유형으로 생성하여 다운로드 받자 (참고링크)    GitHub 계정과 프로젝트를 관리할 Github Repository\n 기본적으로 생성되는 main 브랜치와 별개로 deploy 브랜치를 생성해놓는다. deploy 브랜치에는 서버에 배포할 코드(deploy-ready)만 올라가게 된다.    API \u0026amp; Docker\n 해당 실습에서는 FastAPI 기반의 API 서버를 작성하였다. Docker로 컨테이너화 하여 실행한다. FastAPI 기반의 API 어플리케이션 AWS EC2 인스턴스에는 Docker가 설치해놓는다.     1. Workflow 파일 작성하기  Actions에서 자동화하고자 하는 workflow는 아래와 같은 경로에 yaml 형식으로 작성한다.  .github/workflow/{파일 이름}.yaml  변수 등록하기 ({{ secrets.변수명 }})   직접 보안과 관련된 정보 또는 hard-coding으로 간주되는 변수는 {{secrets.변수명}} 으로 입력한다.\n  repository 우측 상단 Settings -\u0026gt; Secrets and variables -\u0026gt; Actions 탭 에서 등록하면, 추후 Github Actions workflow가 실행될 때 등록된 변수에 접근이 가능하다.\n     이번 실습에서는 다음과 같은 변수들을 등록했다. (스크린샷 참조)\n DEPLOY_HOST : 배포할 원격 서버의 주소 DEPLOY_USERNAME : 원격 서버 계정 DEPLOY_SSH_KEY : 원격 서버에 접속할 때 사용할 SSH 인증키의 값 DEPLOY_SOURCE_DIR : repository의 소스코드를 동기화할 원격 서버의 경로 DEPLOY_IMAGE_NAME : 원격 서버에서 빌드할 이미지 이름 DEPLOY_CONTAINER_NAME : 원격 서버에서 실행할 컨테이너 이름       1.1. 작성된 Workflows 파일 # .github/workflows/deploy.yaml name: Push to EC2  # workflow의 이름 on: # workflow 가 실행되게 하는 event push: # deploy 브랜치가 push (또는 merge) 될 경우 아래의 job이 실행되게 한다. branches: deploy jobs: deploy: # job의 이름 runs-on: ubuntu-latest # job이 실행되는 runner 명시 steps: # step - repository의 소스코드 내려 받기 - uses: actions/checkout@v2 # 아래 actions 설명 참조 # step - 내려 받은 소스코드를 원격 서버에 동기화 시키기 - name: rsync deployments  uses: burnett01/rsync-deployments@5.2.1 # 아래 actions 설명 참조 with: switches: -avzr --delete path: ./ remote_host: ${{ secrets.DEPLOY_HOST }} remote_user: ${{ secrets.DEPLOY_USERNAME }} remote_key: ${{ secrets.DEPLOY_SSH_KEY }} remote_path: ${{ secrets.DEPLOY_SOURCE_DIR }} # 동기화 시키고자 하는 원격 서버 내 경로 # step - 동기화된 소스코드를 가지고  # [도커 이미지 빌드 -\u0026gt; 기존 도커 컨테이너 중단 -\u0026gt; 새 도커 컨테이너 실행]을 수행  - name: Build, run uploaded Docker container uses: appleboy/ssh-action@master # 아래 actions 설명 참조 with: host: ${{ secrets.DEPLOY_HOST }} username: ${{ secrets.DEPLOY_USERNAME }} key: ${{ secrets.DEPLOY_SSH_KEY }} script: |cd ~ cd ${{ secrets.SOURCE_DIR }} docker build . -t ${{ secrets.DEPLOY_IMAGE_NAME }} if [ $(docker inspect -f \u0026#39;{{.State.Running}}\u0026#39; ${{ secrets.DEPLOY_CONTAINER_NAME }}) = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Stopping current running docker container ...\u0026#34; \u0026amp;\u0026amp; docker stop ${{ secrets.DEPLOY_CONTAINER_NAME }}; fi echo \u0026#34;Running docker container based on newly built image\u0026#34; docker run --name ${{ secrets.DEPLOY_CONTAINER_NAME }} --rm -d -p 8000:8000 ${{ secrets.DEPLOY_IMAGE_NAME }}  1.2. 사용된 Actions 1.2.1. checkout actions  Repository의 소스파일을 Github에서 제공하는 CI 서버 내 GITHUB_WORKSPACE에 내려받을 수 있게 하는 actions.  https://github.com/marketplace/actions/checkout    1.2.2. rsync actions  checkout을 통해 내려받은 소스파일을 ssh로 연결한 원격서버의 특정 경로에 배포할 수 있게 하는 actions.  https://github.com/marketplace/actions/rsync-deployments-action    1.2.3. ssh-action actions  ssh로 연결한 원격 서버에서 명령어를 실행할 수 있게 하는 actions.  https://github.com/marketplace/actions/ssh-remote-commands     2. Actions 실행 2.1. 변경 사항 Push 하기  소스코드의 변경사항을 main 브랜치에 커밋해준다. git add . git commit -m \u0026quot;{커밋 메세지}\u0026quot; git push origin main   2.2. main --\u0026gt; deploy merge 하는 PR  코드를 배포하기 위해 변경된 코드를 deploy 브랜치에 머지하는 PR을 작성하고 merge를 수행한다.    main 브랜치에서 deploy 브랜치로 merge하는 PR 작성\n     merge 실행하기\n    2.3. Actions 실행 확인하기   deploy 브랜치로의 merge가 수행되면 동시에 workflow 파일에서 정의한 push trigger가 동작되어 아래와 같이 Actions의 실행으로 이어진다. (Repository 상단의 \u0026ldquo;Actions\u0026rdquo; 탭에서 확인해보자)    (workflow가 실행되었다!)\n     (workflow가 실행되는 중이다!)\n    앞서 작성한 workflows 파일(deploy.yaml)에서 정의한 actions들이 사용되는 것을 확인할 수 있다.      workflow 파일에서 정의한 job의 step이 순차적으로 실행되었고, 해당 Actions workflow가 성공적으로 수행되었음을 확인할 수 있다.         배포된 코드 기반으로 실행 중인 API도 확인해보았다.         마무리하며 업무에서는 Github Actions를 활용한 CI/CD를 하고 있지 않기 때문에, 점심시간에 따로 경험이 있는 동료에게 물어보기도 하고 적용해서 삽질을 하기도 하면서 이런 생각이 들었다.\n 이번 찍먹 에서는 배포에만 사용해봤지만, 각 브랜치에 push 하기 이전에 진행하는 unit-test를 workflows에 포함시키면 되겠다. (이미 그렇게들 많이 쓰는 것 같다)  사실 Actions 의 예시가 여러가지가 있었지만, 배포하는 과정의 자동화에 적용하는게 내 입장에서 가장 실용적으로 다가와서 이렇게 해보긴 했다.   [새로운 이미지를 빌드 -\u0026gt; 실행 중인 컨테이너 중단 -\u0026gt; 새로운 이미지로 컨테이너 실행]의 프로세스가 이게 맞나...? 하는 생각이 들었다.  이 방식의 단점이 있다면 어떤 점이 있을까? 매번 이렇게 이미지를 빌드해야 하나 \u0026hellip; ? 싶기도 하고 그렇다. Dockerfile을 보고 이미지를 빌드하는 Actions가 있지는 않을까? 바로 컨테이너만 실행할 수는 없을까? (다 하고나니 가지처럼 뻗어나가는 의문점 \u0026hellip; ;;; ) 비슷한 다른 예시를 찾아보면 AWS S3와 AWS Code Deploy를 엮어서 사용하는 예시도 있던데, 비용이 살짝 부담되었다. 어느 방식이 \u0026ldquo;Best practice\u0026quot;일까?   적절한 Actions를 marketplace 에서 찾고 적용하는 것이 핵심이겠다는 생각이 든다.   Reference  AWS EC2 생성하기 AWS EC2 키페어 파일 생성하기 GitHub Actions의 체크아웃(Checkout) 액션으로 코드 내려받기 Github Actions 사용하는 법, SSH 연결  ","permalink":"https://wonyoungseo.github.io/posts/2023-02-07-aws-ec2-deploy-github-actions/","summary":"지인의 선물로 받은 영화추천 웹서비스로 배우는 풀스택 을 훑어보던 중, Gitlab 배포를 자동화하는 부분을 보고, 이번 기회에 동일한 과정을 내가 익숙한 Github으로 따라해보고 Github Actions에 대해서도 찍먹 해보기로 했다.\n 0. 준비 사항   서버\n API 를 배포할 AWS EC2 인스턴스 EC2 인스턴스에 연결할 때 사용될 키페어 (.pem 파일) - RSA 유형으로 생성하여 다운로드 받자 (참고링크)    GitHub 계정과 프로젝트를 관리할 Github Repository\n 기본적으로 생성되는 main 브랜치와 별개로 deploy 브랜치를 생성해놓는다.","title":"[KR] Github Actions로  AWS EC2 배포 자동화 찍먹해보기"},{"content":"2020년 2월 4기부터 참여하기 시작한 글또가 벌써 8기가 되었다. 2020년부터\u0026hellip;라고 하니 \u0026lsquo;어찌됐건 꾸역꾸역 해왔구나\u0026hellip;\u0026rsquo; 확 체감이 된다.\n글또는 어쩌다, 왜 계속해서 참여 하고 있는지 스스로에게 물어보는 시간이 가졌다.\n글또는 왜 계속 하는 걸까 처음 글또를 시작했을 때 초점은 \u0026ldquo;나의 성장\u0026quot;에 초점이 맞춰져 있었다. \u0026ldquo;어떠한 역량을 키워서 무엇을 해내겠다. 그것을 통해서 성장을 꼭 해내겠다. 그것을 글또에 제출하는 글을 통해 증명하겠다\u0026rdquo; 하는 마음가짐이었던 것 같다. 일정 부분 해낸 것도 있고, 다짐했을 때와 달리 놓친 부분도 많았다. 놓쳤다기 보다는 의욕만 앞서서 퍼져버렸다는 게 더 맞는 말일지도 모른다.\n지금의 마음가짐은 약간 다르다. 우선 글또라는 커뮤니티를 받아들이는 스탠스가 좀 다르다. 나에게 글또는 다양한 자극과 영감을 받을 수 있고, 현생을 사느라 좁아지는 시야를 틈틈히 트이게 해주는 고마운 플랫폼이다.\n과거에 글또가 나를 채찍질하고 나를 증명해야 자리라고 인식했다면 (그럴 필요가 없었는데 \u0026hellip; 아무튼 그때는 그랬다), 지금 나에게 글또는 커뮤니티 그 자체로서 다가온다. 그 안에서 충실히 글을 써서 제출하고 시스템의 도움을 받아 글 쓰는 것을 놓지 않고 싶다. 그리고 다른 분들이 쓴 글을 통해서 많은 영감을 받았으면 한다. 오버페이스 하지 않고 딱 내 페이스대로 글또 활동을 즐기는 방식이고, 그래서 8기에도 참여를 하게 되었다.\n이번 기수에 쓰고자 하는 글 MLOps 와 연관된 시리즈 글 지난 7기 때는 ML 추론 파이프라인 구축 업무를 하게 되면서 MLOps에 대한 교육을 듣고, 관련 책도 지원 받아 리뷰해보기도 하였고, 여러가지 도구를 찍먹해보는 글도 작성한 바가 있다.\n이번에는 MLOps 토이프로젝트를 하면서 시리즈 글을 몇가지 작성해보고 싶다. 튜토리얼 성격의 글도 좋고, 프로젝트을 진행하면서의 기록이 될 수도 있다.\n토이프로젝트와 별개로 ML application productionizing 의 키워들 발표되는 논문 리뷰도 시리즈로 해보고 싶다. 최소한 3편은 해야 시리즈 물이 되지 않을까 싶기도 한데, 일단 이렇게 글로서 박제를 해본다.\n조직문화와 리더십에 관한 글 지난 7기 다짐글을 다시 읽어보는데, 조직문화와 관련된 글을 쓰겠다고 한 부분이 있었다. 그리고 전혀 쓰지 않았다. 생각은 많이 하는데 정리가 어려웠던 것 같다. 올해에는 팀리더 역할도 정식으로 부여 받으면서 팀 내 새로운 시도와 시스템도 도입해보고 있고, 다양한 리더십 관련 좋은 책들도 글또에서 추천 받아 열심히 읽으면서 내 나름대로의 방식을 만들어가고 있다. 조만간 정리해서 글로 남겨야겠다.\n글 말고 오프라인 모임도 빠질 수 없지 지난 기수 때 너무 아쉬웠던 점이라면 온오프라인 모임을 전혀 참석하지 못 했다는 것이다. 너무나 집돌이 성향이라 쉽지는 않지만, 기회가 될 때마다 다른 분들과 교류할 수 있도록 좀 내 자신을 끄집어 내고 싶다.\n마무리하며 이번에도 정리해보면 \u0026ldquo;너무 무리하면 퍼지니까 가늘고 길게\u0026rdquo; 인 것 같다. 너무 원대한 계획은 틀어지기 쉽다. 내 페이스대로 해보자. 내 페이스대로\n","permalink":"https://wonyoungseo.github.io/posts/2023-02-02-init-geultto-8/","summary":"2020년 2월 4기부터 참여하기 시작한 글또가 벌써 8기가 되었다. 2020년부터\u0026hellip;라고 하니 \u0026lsquo;어찌됐건 꾸역꾸역 해왔구나\u0026hellip;\u0026rsquo; 확 체감이 된다.\n글또는 어쩌다, 왜 계속해서 참여 하고 있는지 스스로에게 물어보는 시간이 가졌다.\n글또는 왜 계속 하는 걸까 처음 글또를 시작했을 때 초점은 \u0026ldquo;나의 성장\u0026quot;에 초점이 맞춰져 있었다. \u0026ldquo;어떠한 역량을 키워서 무엇을 해내겠다. 그것을 통해서 성장을 꼭 해내겠다. 그것을 글또에 제출하는 글을 통해 증명하겠다\u0026rdquo; 하는 마음가짐이었던 것 같다. 일정 부분 해낸 것도 있고, 다짐했을 때와 달리 놓친 부분도 많았다.","title":"[KR] 글또 8기를 시작하며"},{"content":"왜인지 모르겠지만 2022년 12월 말에 벌써부터 2022년을 회고하고 싶지 않아서 회고를 질질 미뤘다. 대신 1월 동안 2023년에 대한 기대감을 구체화 하는 과정에서 2022년도 돌아보는 시간을 가졌다.\n2022년 기억나는 부분들을 키워드를 통해 되돌아보고, KSS(Keep / Stop / Start)을 통해 2023년을 기대해본다.\n올해의 키워드 올해의 총대 연초에 우리 회사의 조직문화에 대해 토론하고 비전, 미션, 밸류를 정하는 세션을 자주 가졌었지만. 5, 6월이 되었을 때 우리가 원했던 문화는 흔적도 없이 원상태로 돌아가있었다. 그래서 작게나마 우리회사의 조직문화에 기여 할 수 있는 방법을 스스로 찾아서 사내 북스터디를 만들었다. (8년 가까운 역사에 처음 생긴 거라고 한다)\n북스터디는 점심시간을 이용해 구글 엔지니어는 이렇게 일한다 의 “문화” 부분(1~7장) 내용에 대해 토론하는 방식으로 기획했다. 참석 대상의 직무를 따로 구분하지 않았는데 무려 6명이나 신청을 했다(!!!) 그렇게 6명이 4개월 동안 격주로 모여 구글의 일하는 방식과 문화에 대해 이야기를 나누고, 우리 회사는 어떤지 비교하는 시간을 가졌다. 매일 얼굴을 마주치지만, 각 팀이 가진 문화, 어려운 점 등 속사정은 몰랐기 때문에 개인적으로 의미가 컸었다. 영화나 미드에서나 보듯이 사람들이 둘러앉아 상처를 공유하거나 넋두리를 하거나 … 하는 그런 모습도 자주 연출 됐었다. 우리는 그래서 구글이 일하는 방식을 배우는 심리치유 세션이 아닌가 하는 우스갯 소리도 하곤 했다. (ㅋㅋ) 중간중간 뭐하는 건가 궁금해서 참관하는 분들도 한두명 있었다.\n      열정으로 만들어버린 테린이 대회 캘린더\n  이번 스터디를 통해서 책을 읽고 공유하고자 하는 동료들이 생각보다 많았다는 것을 깨달았다. 이후 후속으로 진행되는 북스터디는 아직 없지만, 이때 참여했던 멤버들과 더불어 다른 동료들도 각자 조직문화나 리더십, 자기계발 책을 읽고 서로에게 추천하고 후기를 공유하는 모습이 종종 보여서 약간의 뿌듯함을 느끼고 있다.\n올해의 감투 우리 팀에도 기존에 팀리더가 있었다. 하지만 여러 부분에서 리더십이 발휘되는 경우는 보기 드물었다. 정신 차려보니 오히려 2, 3분기에는 이 역할을 내가 하고 있었다 (…) 그래서 4분 즈음에는 본부장, 인사담당자와 여러 차례 이 점에 대해서 면담을 했고, 결국 팀 리더 역할을 맡게 되었다. 기존에 팀리더를 맡던 분은 나보다 기술적 역량이 뛰어나기 때문에 IC(Individual Contributor)의 역할을 하게 되었고 관리의 스트레스에서 벗어나서 더 연구에 집중할 수 있게 되었다. (IC도 북스터디에서 처음 접해서 적용하게 된 개념이다!!)\n팀리더가 된 이후로는 팀 매니징 관점에서 기존에 없었던 부분들을 아래와 같이 시도하고 있다.\n 팀원들의 업무 파악 팀원 역량에 따른 업무 지정 일정 관리, 데드라인에 임박해서 일을 미루지 않도록 관리 Asana를 통해 팀원 스스로 일정 관리 할 수 있도록 체크하고 수시로 조언 우리 팀에 요청된 업무에 대한 맥락을 파악하고 팀원들에게 투명하게 공유  요즘 들어 자연스럽게 팀장, 조직문화 이런 키워드와 관련된 책을 찾게 된다. 공개된 공간에서 감히 밝힐 수 없는 얘기도 많지만, 잘 해보고 싶다.\n올해의 몰입 올해는 드디어 관심 있었던 MLOps 업무를 조금씩 적용하고 있다. 뉴스 기사를 분석하는 후즈굿의 프로덕트인 IA는 여러가지 NLP모델이 연결되어 최종 결과를 산출하는데, 3/4분기에는 이 프로덕트의 추론 파이프라인을 설계하고 구축하는 프로젝트를 진행했다.\n미리 본부장과 팀원들에게 적극적으로 어필해서 온전히 이 프로젝트에 집중할 수 있는 기간을 확보한 덕분에 몰입할 수 있었다. 회사 업무와 평소에 관심 있는 분야, 그리고 내 성장을 같은 방향으로 설정했을 때 생기는 시너지를 처음 느껴본 것 같다. 모델링을 하는 팀원과의 협업에서 어려움이 없었던 것은 아니지만, 각자 조언해주는 존재 없이 처음 하는 업무였기 때문에 시행착오 덕분에 경험치를 쌓았다고 생각한다.\n앞으로 Model artifact 등을 한번에 관리하는 ML Registry, Human-in-the-loop 개념을 적용한 model update training pipeline 등 MLOps를 구축하는 과정은 아직 한참 남았지만, 올해에도 차근차근 격파해나갈 수 있을 거라는 희망을 가져본다.\n올해의 휴식 올해 하이라이트를 꼽자면 3, 4년동안 못 봤던 누나 가족을 만나러 떠난 3주 휴가였다. 회사를 다니며 이렇게 오랫동안 휴가를 가본 적이 없었는데 (매년 연차를 다 쓰지도 못 하고 남았었는데) 이번에는 정말 제대로 쉬다 왔다. 주변 도시도 방문하고 무엇보다 조카들과의 시간이 정말 힐링이었다.\n올해는 테니스도 열심히 쳤다. 평소 앉아있는 시간이 많았던 것에 비해 운동을 하지 못 했었는데 테니스를 열심히 하면서 다시 활력을 얻고 스트레스 해소에도 많은 도움을 받게 되었다. 테린이 커뮤니티 운영진으로 합류해서 테니스 모임이나 대회를 기획하기도 하고, 2030 동호인들이 참가할 수 있는 대회 정보도 정리해서 공유하는 역할을 하고 있다.\n2022년은 여러모로 열심히 일하는 만큼 열심히 쉬어야 한다는 걸 몸소 깨달았던 게 정말 크다 .. ! 2023년에도 치열하게 쉴 계획을 세워야겠다.\n   열정으로 만들어버린 테린이 대회 캘린더\n  2023년을 맞이하며 Keep  독서 + 독서 교류: 책 읽는 습관 유지 휴식: 꾸준한 운동, 국내 가까운 곳에라도 가끔씩 여행하기 리더십 탐구: 리더십, 조직문화, 팀 매니징 관련 책 꾸준하게 읽기  Stop  적당히 휴식하더라도 적당한 선에서 멈추기. 과몰입 자제.  Start 현재 내 위치에서 조금 더 열심히 할 수 있는 것들  구직자 입장에서 오고싶은 팀 문화 갖추기  인터뷰 개인 성장 팀 내부 공유   주기적인 팀원 원온원 팀 성과 측정 기준 만들기  개인의 성장  Industry standard로 자주 언급되는 스택으로 토이 프로젝트하기  CI/CD, cloud service providers, containerization, model registry, etc   생각 나는 아이디어가 있다면 간단하게 구현할 수 있는 앱 개발 역량 갖추기 (Flutter 찍먹하기 !) 개발자 오프라인 모임, meetup 참석하기 대학원 과목 수료하고 논문 쓸 준비 하기  마무리하며 가볍게 정리해보려 했던 회고글이 생각보다 길어졌다. 아무튼 2023년에는 너무 욕심내지 말고, 퍼지지 말고 꾸준하게 잘 보내보자 !!\n","permalink":"https://wonyoungseo.github.io/posts/2023-01-24-review-2022/","summary":"왜인지 모르겠지만 2022년 12월 말에 벌써부터 2022년을 회고하고 싶지 않아서 회고를 질질 미뤘다. 대신 1월 동안 2023년에 대한 기대감을 구체화 하는 과정에서 2022년도 돌아보는 시간을 가졌다.\n2022년 기억나는 부분들을 키워드를 통해 되돌아보고, KSS(Keep / Stop / Start)을 통해 2023년을 기대해본다.\n올해의 키워드 올해의 총대 연초에 우리 회사의 조직문화에 대해 토론하고 비전, 미션, 밸류를 정하는 세션을 자주 가졌었지만. 5, 6월이 되었을 때 우리가 원했던 문화는 흔적도 없이 원상태로 돌아가있었다. 그래서 작게나마 우리회사의 조직문화에 기여 할 수 있는 방법을 스스로 찾아서 사내 북스터디를 만들었다.","title":"[KR] 2022년이 지났다"},{"content":"GitHub Actions는 CI (Continuous Integration: 지속적인 통합) 와 CD(Continouos Deployment: 지속적인 배포) 의 자동화를 지원하는 도구이다. CI/CD를 자동화 하는 일련의 작업들을 묶어 재사용하거나 공유할 수도 있다.\n해당 포스트에서는 GitHub Actions의 구성요소와 형태를 간략하게 정리한다.\n1. GitHub Actions 구성요소 GitHub Actions는 5가지 요소로 이루어져 있다.\n Event (1)  깃헙에서 발생할 수 있는 대부분의 이벤트   Workflows (2)  이벤트가 발생했을 때 무엇이 어떤 순서로 수행될 지를 정의 Jobs (3)  Workflow를 구성하는 요소 각각의 Job 내부에서도 어떤 step이 수행되어야 하는지 순서를 설정할 수 있음 step은 shell script로 명시 (또는 Action을 사용할 수 있음) Action (4)  Job의 step을 이미 만들어진 Action을 사용하여 수행할 수 있음 Github 또는 3rd party에서 제공되는 것을 사용하면 됨   Runner (5)  VM Machine 또는 Docker container Job은 각각의 개별 Runner라는 컨테이너에서 실행됨        2. 기본적인 형태 자동화 하고자 하는 workflow는 repository 내 아래와 같이 yaml파일에 정의한다.\n.github/workflow/{원하는 workflow}.yaml\nname: workflow-actions # 1 on: [push] # 2 jobs: # 3 check-version: # 4 runs-on: ubuntu-latest # 5 strategy: # 6 matrix: node-version: [12.x, 14.x, 16.x] steps: # 7 - uses: actions/checkout@v3 # 8 - uses: actions/setup-node@v3 # 9 with: # 10 node-version: ${{ matrix.node-version }} - run: npm install -g bats \u0026amp;\u0026amp; bats -v # 11  workflow 이름 workflow가 실행되게 하는 event event에 따라 수행되어야 하는 job들 수행되는 job의 이름 runner 명시 다른 실행이 되길 원한다면 strategy에 명시된 matrix의 변수를 받아 실행 가능 steps checkout이라는 action node를 setup하는 action (node-version은 14) action에 따라 다르게 적용할 수 있는 문법 shell 명령어를 수행  ","permalink":"https://wonyoungseo.github.io/posts/2023-01-22-github-actions-concept/","summary":"GitHub Actions는 CI (Continuous Integration: 지속적인 통합) 와 CD(Continouos Deployment: 지속적인 배포) 의 자동화를 지원하는 도구이다. CI/CD를 자동화 하는 일련의 작업들을 묶어 재사용하거나 공유할 수도 있다.\n해당 포스트에서는 GitHub Actions의 구성요소와 형태를 간략하게 정리한다.\n1. GitHub Actions 구성요소 GitHub Actions는 5가지 요소로 이루어져 있다.\n Event (1)  깃헙에서 발생할 수 있는 대부분의 이벤트   Workflows (2)  이벤트가 발생했을 때 무엇이 어떤 순서로 수행될 지를 정의 Jobs (3)  Workflow를 구성하는 요소 각각의 Job 내부에서도 어떤 step이 수행되어야 하는지 순서를 설정할 수 있음 step은 shell script로 명시 (또는 Action을 사용할 수 있음) Action (4)  Job의 step을 이미 만들어진 Action을 사용하여 수행할 수 있음 Github 또는 3rd party에서 제공되는 것을 사용하면 됨   Runner (5)  VM Machine 또는 Docker container Job은 각각의 개별 Runner라는 컨테이너에서 실행됨        2.","title":"[KR] Github Actions 개념"},{"content":"   트래스포머를 활용한 자연어처리 (저자: 루이스 턴스톨, 레안드로 폰 베라, 토마스 울프)\n  추천하는 대상:\n 파이썬과 딥러닝 프레임워크에 익숙한 리서처 \u0026amp; 엔지니어 자연어처리 프로젝트를 진행하는 모든 분 !  Transformer와 Huggingface 어텐션(Attention) 메커니즘의 등장은 트랜스포머(Transformer) 모델 구조로 이어지며 최근 몇년 간 자연어처리 기술 발전의 근간이 되었다. 이와 더불어 허깅페이스(Huggingface)는 트랜스포머 그 자체로 동일한 이름을 가진 라이브러리가 등장시켰고, 모델에 대한 사용성과 접근성을 크게 개선했다.\n트랜스포머를 활용한 자연어처리 는 허깅페이스에 대한 전반적인 소개와 사용법을 다룬다. 앞서 서술한 바와 같이 이 책은 파이썬 프로그래밍과 딥러닝 프레임워크에 익숙하며, 이미 NLP를 적용한 프로젝트에 익숙한 분들이 대상 독자로 적합하다.\n책의 구성 내용의 구성은 아래와 같다.\n 1 ~ 3장 : 트랜스포머와 허깅페이스 라이브러리, 생태계 소개  2장 : 분류 (Text Classification)   4 ~ 7장 : 허깅페이스를 활용한 자연어처리 태스크 별 적용 소개  4장 : 개체명인식 (Named Entity Recognition) 5장 : 생성 (Text Generation) 6장 : 요약 (Text Summarization) 7장 : 질의응답 (Question \u0026amp; Answering)   8장 ~ 10장 : 모델 성능 향상 11장 : 향후 로드맵  가장 핵심적인 장점 허깅페이스의 이름 아래 조성된 생태계는 초창기 트랜스포머 라이브러리와는 달리 매우 거대해졌다. NLP 뿐만 아니라 컴퓨터비전 문제를 해결하기 위한 모델도 허깅페이스를 통해 접할 수 있게 된 세상이다.\n어디서부터 시작해야할 지 막연할 수도 있지만 허깅페이스 소속 엔지니어와 오픈소스 프로젝트에 참여한 개발자가 직접 참여한 트랜스포머를 활용한 자연어처리 는 프로젝트를 시작하거나 어플리케이션을 개발하고자 하는 분께 허깅페이스의 무엇을 어떻게 활용할 지 훌륭한 가이드가 된다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-12-29-book-review-nlp-with-huggingface/","summary":"트래스포머를 활용한 자연어처리 (저자: 루이스 턴스톨, 레안드로 폰 베라, 토마스 울프)\n  추천하는 대상:\n 파이썬과 딥러닝 프레임워크에 익숙한 리서처 \u0026amp; 엔지니어 자연어처리 프로젝트를 진행하는 모든 분 !  Transformer와 Huggingface 어텐션(Attention) 메커니즘의 등장은 트랜스포머(Transformer) 모델 구조로 이어지며 최근 몇년 간 자연어처리 기술 발전의 근간이 되었다. 이와 더불어 허깅페이스(Huggingface)는 트랜스포머 그 자체로 동일한 이름을 가진 라이브러리가 등장시켰고, 모델에 대한 사용성과 접근성을 크게 개선했다.\n트랜스포머를 활용한 자연어처리 는 허깅페이스에 대한 전반적인 소개와 사용법을 다룬다.","title":"[KR] 책 리뷰 : 트랜스포머를 활용한 자연어처리"},{"content":"   온디바이스 AI (저자: 로런스 모로니)\n  추천하는 대상:\n 머신러닝을 적용한 어플리케이션을 개발하고자 하는 모바일 앱개발자  조금은 더 친근해진 모바일 AI 현실에서 사용할 수 있는 AI 지난 10년이 AI 모델을 연구하고 구현하는 쪽으로 강조가 되었다면, 최근에는 AI 모델을 실제 제품으로서 사용할 수 있도록 생산 제작 하는 쪽으로 접근이 이루어지고 있다. 그렇게 많은 사람들이 모델을 \u0026lsquo;배포\u0026rsquo;하고 API 형태로 \u0026lsquo;서빙\u0026rsquo;하며, 더 가벼운 형태로 경량화 하는 방식에 대한 고민을 하기 시작했다.\n우리 삶에서 AI를 제품으로서 사용할 수 있는 가장 쉬운 경로는 우리가 매일 사용하는 스마트폰이다. 특히 머신러닝 엔지니어라면 웹을 넘어 모바일 어플리케이션에서도 사용 가능한 AI 모델을 꿈꿔봤을 것이다.\n온디바이스 AI 는 그에 대한 시작점으로 삼을 수 있는 책이다. 이 책은 구글에서 개발한 ML Kit라는 도구를 통해 모바일 기기에서 모델을 사용할 수 있는 방법을 소개하며, 더 나아가 텐서플로우의 TFLite, iOS의 Core ML 을 활용하는 방식도 다룬다.\n모바일 앱 개발자라면 오히려 취약할 수 있는 머신러닝에 대한 이해도를 돕는 부분이 있으며, 반대로 머신러닝 엔지니어가 취약할 수 있는 앱 개발 부분에서는 이미 잘 만들어진 도구를 활용하여 커스텀 모델을 만드는 방식으로 진입장벽을 낮춘다. 따라서 입문하기에는 더할나위 없이 괜찮은 책이다.\n안드로이드와 iOS 예제를 모두 다 ! 애플 기기를 메인으로 사용하고 있는 입장에서 이 책이 주는 가장 큰 장점은 iOS 에서의 ML 어플리케이션을 만드는 가이드를 제공해준다는 점이다. 반대로 안드로이드의 경우에도 정확히 똑같은 기능을 구현하기 때문에 모든 수요를 아우르고 있다.\n이건 정말이지 큰 장점이다. 머신러닝 또는 딥러닝을 다루는 책을 접해본 사람이라면 공감할 텐데, 동일한 결과물을 만들어내는 텐서플로우와 파이토치 예제가 사실상 없다. 마찬가지로 데이터 분석을 다루는 책에서 파이썬과 R을 가지고 동일한 분석 프로세스를 다룬 튜토리얼을 접하기는 정말 힘들다. 그리 어렵지 않을 것 같으면서도 막상 찾아보면 없는 것을 이 책은 제공한다.\n그와 더불어 ML모델이 클라우드 서버로 구축되어 접근이 가능하다면, 이 또한 방법을 제공하니, 최대한 모든 고민에 대한 답변을 준비했다는 인상을 받았다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-11-26-book-reivew-on-device-ai/","summary":"온디바이스 AI (저자: 로런스 모로니)\n  추천하는 대상:\n 머신러닝을 적용한 어플리케이션을 개발하고자 하는 모바일 앱개발자  조금은 더 친근해진 모바일 AI 현실에서 사용할 수 있는 AI 지난 10년이 AI 모델을 연구하고 구현하는 쪽으로 강조가 되었다면, 최근에는 AI 모델을 실제 제품으로서 사용할 수 있도록 생산 제작 하는 쪽으로 접근이 이루어지고 있다. 그렇게 많은 사람들이 모델을 \u0026lsquo;배포\u0026rsquo;하고 API 형태로 \u0026lsquo;서빙\u0026rsquo;하며, 더 가벼운 형태로 경량화 하는 방식에 대한 고민을 하기 시작했다.","title":"[KR] 책 리뷰 : 온디바이스 AI"},{"content":"   파이썬 기반 금융 인공지능 (저자: 이브 힐피시)\n  추천하는 대상:\n 금융 데이터를 활용한 모델링에 관심 있는 분  한줄평:\n금융 기반 데이터를 활용의 지침서 금융에서 데이터를 활용하는 법 최근 인공지능, 머신러닝, 딥러닝 등 소위 \u0026ldquo;핫\u0026quot;한 토픽이 대두되기에 이전부터 가장 데이터를 활발하게 활용하고 있던 분야가 있다면, 그건 바로 금융 업계일 것이다. 다만 금융을 전공하지 않은 CS 기반의 데이터 직군 종사자가 금융 업종의 전문 또는 배경지식을 갖추고 데이터를 활용하는 업무를 하는 것 또한 쉽지 않은 일이다.\n특히 기존에 금융 업계에서는 알고리즘에 기반한 트레이딩과 시뮬레이션이 존재해왔었는데, 이는 인공지능 기법의 발전과 더불어 앞으로 더욱 발전해나갈 여지가 무궁무진한 분야이며, 현재에도 활발한 연구가 진행되고 있다.\n금융 데이터의 맛 이번에 리뷰하게 된 파이썬 기반 금융 인공지능 에서는 인공신경망, 강화학습 등의 기법들을 금융 데이터에 적용하는 방식 뿐만 아니라, 금융 이론, 금융 데이터의 개념과 특이점, 백테스팅과 같은 알고리즘 트레이딩 기법도 다루고 있어, 금융 도메인의 지식이 없는 사람이라면 꼭 참고해야할 내용들이 담겨져 있다.\n특히 파트 2 \u0026ldquo;금융과 머신러닝\u0026rdquo; 에서 다룬 내용이 인상적이었는데, 금융학의 베이스를 이루는 이론과 모형에 대해 설명하고, 대량의 데이터를 수집하고 연산할 수 있는 기술의 발전과 더불어 머신러닝을 접목한 금융의 미래로 이어지는 흐름이 이 책의 알짜배기라고 생각이 된다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-10-30-book-reivew-ai-in-finance-copy/","summary":"파이썬 기반 금융 인공지능 (저자: 이브 힐피시)\n  추천하는 대상:\n 금융 데이터를 활용한 모델링에 관심 있는 분  한줄평:\n금융 기반 데이터를 활용의 지침서 금융에서 데이터를 활용하는 법 최근 인공지능, 머신러닝, 딥러닝 등 소위 \u0026ldquo;핫\u0026quot;한 토픽이 대두되기에 이전부터 가장 데이터를 활발하게 활용하고 있던 분야가 있다면, 그건 바로 금융 업계일 것이다. 다만 금융을 전공하지 않은 CS 기반의 데이터 직군 종사자가 금융 업종의 전문 또는 배경지식을 갖추고 데이터를 활용하는 업무를 하는 것 또한 쉽지 않은 일이다.","title":"[KR] 책 리뷰 : 파이썬 기반 금융 인공지능"},{"content":"글또 7기를 마무리하며 어느새 5개월이 지나 글또 7기 활동 기간도 끝을 바라보고 있다. 7기를 시작하며 작성한 다짐글을 다시 읽어보니, 핵심은 \u0026ldquo;너무 열심히 하지 않기\u0026rdquo; 였다. 되돌아보니 정말 이번만큼은 잘 수행하는 기간이었다는 생각이 든다. (어찌 이런 다짐은 또 \u0026hellip;) 5개월 간 있었던 일들을 한번 짚어보고자 한다.\n돌아보기 사내 독서 스터디 진행 \u0026ldquo;구글 엔지니어는 이렇게 일한다\u0026rdquo; 책을 가지고 사내 독서 스터디를 시작했다. 다만 개발 직군만을 대상으로 모집하지 않았고, 최대한 다른 직군에 있는 분들도 함께 하자고 물밑작업을 열심히 했다. 현재 인사, 기획, 리서치, 개발, 데이터 직군의 사람들이 모여 한챕터씩 읽고, 우리 회사는 어떠한지 이야기를 나누는 방식으로 진행하고 있다. 스터디 자체로 인해 부담이 되지 않도록 격주로 하고 있고 진행은 더디다. 이런 도서를 가지고 스터디를 하는 것 자체가 정말 큰 의미를 가지고 있다고 생각한다. 회사 DNA에 조금은 바뀌었기를 바라는 마음이다.\n개인적인 공부 Udacity에서 Machine Learning DevOps Engineer Nanodegree를 수강하고 나서, 최근에 Full Stack Deeplearning의 강의 컨텐츠도 수강했다 (개인 일정이 있어 프로젝트는 하지 않았다.) 내용의 특성상 확고한 방법론이 있는 것이 아니고, 강사 또한 각자의 회사에서 관련 일을 하고 있는 엔지니어들이기에, use case를 배워간다는 생각으로 수강했고, 프로젝트에 많은 참고가 되었다.\nML 추론 파이프라인 프로젝트 올해 개발된 ML모델을 통해 뉴스 기사를 분석하는 추론 파이프라인을 구축하는 프로젝트를 진행하고 있다. 비즈니스 상 사람이 추론 결과를 확인하고 수정해야 하는 과정이 불가피 하기 때문에 end-to-end 형태가 아닌 batch inference pipeline 형태를 이루게 되었다. 휴가를 앞두고 진행되었기 때문에 개인적으로는 압박감이 있었지만 불가능한 일정은 아니었다. 하지만 다른 무엇보다 협업이 발목을 잡는 일이 생기는 와중에, 정말 울며 겨자먹기로 1차 마일스톤을 달성하고 나서 휴가를 떠났다. 현재 시점에서 그때를 다시 돌아보면서 무엇이 문제였고 어떻게 했어야 하는지, 어떤 시스템이 갖춰져야 하는지 계속 고민하고 있다.\n3주 간의 휴가 누나와 가족들이 독일에 사는데, 코로나 때문에 약 4년 간 가족들을 보지 못 했다. 어찌저찌 연차와 추석연휴를 이어붙여 3주 간 독일을 다녀왔다. 여행은 짧게 하고 주로 가족들과 시간을 보냈다. 2018년 마지막으로 봤을 때 애기들이었던 조카들은 어느새 어린이가 되어있었다. 영혼까지 끌어모은 휴가지만 3주가 너무 짧았고, 말도 안 통하는 조카들과 함께하는 시간은 너무 행복했다.\n테니스 모임 강서구 테니스 모임인 조동아리의 운영진으로 합류했다. 현재는 각종 이벤트, 정모 진행과 테린이 대회 알림, 참가 독려(?)를 맡고 있다. 200명이 넘는 회원이 가장 큰 자산이기에 앞으로 해볼 수 있는 일들이 어떤 게 있을 지 행복회로를 돌리고 있다.\n성장 못지 않게 중요한 것들 최근 몇 달 동안은 테니스 열심히 치고 처음으로 휴가다운 휴가도 다녀오고, 성장보다는 노는 것(?)도 열심히 챙긴 시간이었던 것 같다. 놀았던 시간을 생각하니 죄책감이 안 생기는 것은 아니지만 \u0026hellip; 여가와 가족과의 시간에 과감히 투자해서 좋은 시간을 보냈고 일과 성장, 공부 못지 않게 중요한 것들이 있다는 걸 느끼게 되어 참 다행이었다. 글또 7기는 이렇게 마무리 되었다!\n","permalink":"https://wonyoungseo.github.io/posts/2022-10-15-reivew-geultto-7/","summary":"글또 7기를 마무리하며 어느새 5개월이 지나 글또 7기 활동 기간도 끝을 바라보고 있다. 7기를 시작하며 작성한 다짐글을 다시 읽어보니, 핵심은 \u0026ldquo;너무 열심히 하지 않기\u0026rdquo; 였다. 되돌아보니 정말 이번만큼은 잘 수행하는 기간이었다는 생각이 든다. (어찌 이런 다짐은 또 \u0026hellip;) 5개월 간 있었던 일들을 한번 짚어보고자 한다.\n돌아보기 사내 독서 스터디 진행 \u0026ldquo;구글 엔지니어는 이렇게 일한다\u0026rdquo; 책을 가지고 사내 독서 스터디를 시작했다. 다만 개발 직군만을 대상으로 모집하지 않았고, 최대한 다른 직군에 있는 분들도 함께 하자고 물밑작업을 열심히 했다.","title":"[KR] 글또 7기를 마무리하며"},{"content":"사내에서 데이터 버전 관리에 대한 내용을 소개하면서 정리한 내용을 포스팅으로 재정리 해보았습니다.\n1. 버전관리 1.1. 소프트웨어 엔지니어링에서 관점에서의 버전 관리 이제는 Git으로 익숙하게 접하는 버전 관리(version control)이 하는 역할을 소프트웨어 엔지니어링 관점에서 다시 한번 정리해보자면 다음과 같다.\n 소프트웨어를 이루는 코드와 패키지, 라이브러리 등에 생긴 변화를 추적하고 관리하기 위한 용도 여러 구성원이 소프트웨어를 개발할 때 발생하는 혼란을 줄이고, 효율적으로 협업할 수 있게 하는 용도 소프트웨어에 큰 문제가 발생했을 때, 빠르게 이전 버전으로 돌아가는 등, 에러나 실수로 인해 발생하는 손해를 최소화 하기 위한 용도  1.2. 머신러닝 프로젝트 관점에서의 버전관리 비슷하게 접근하여, 머신러닝 프로젝트의 관점에서 버전관리를 하는 이유 또는 역할을 살펴보면 다음과 같다.\n 머신러닝 프로젝트를 이루는 코드, 데이터, 모델, 패키지, 라이브러리 등 과정에서 발생하는 모든 변화를 추적하고 관리하기 위한 용도 모델 개발을 한명의 연구자가 혼자 개발하는 것이 아닌, 팀 내 공통으로 개발할 수 있게 하기 위한 용도 실험한 모델을 그대로 재현해내기 위한 용도 가장 최근에 배포한 모델에 이슈가 있을 경우 이전 버전의 모델로 빠르게 돌아가기 위한 용도  2. ML 버전관리의 목표와 버전관리 대상 1.2. 에서 살펴본 용도들을 다시 간단하게 정리해보자.\n  목표:\n 버전 관리, 버전 추적 용이한 버전 롤백 협업과 공유 모델의 결과 재현    버전 관리 대상\n 모델 학습을 위해 사용된 데이터 모델의 구조 학습된 모델의 결과물  가중치 하이퍼 파라미터 최종 학습 파라미터   해당 모델의 성능 해당 모델의 버전    위와 같이 버전관리 대상이 되는 머신러닝 모델의 결과물을 Artifact (가공품) 이라고 한다.\n3. 머신러닝 버전 관리 도구 3.1. Artifact 저장 및 버전 로깅 Artifact를 저장하고 DB에 관련 정보를 기록하여 관리하는 방식이 있으며, 주로 머신러닝 플랫폼이나 솔루션, 오픈소스 프로젝트에서 이런 방식을 택한다.\n 플랫폼 또는 솔루션  Weights \u0026amp; Biases Neptunes.ai etc \u0026hellip;   오픈소스 라이브러리  BentoML MLflow etc \u0026hellip;    3.1.1. 예시) Weights \u0026amp; Biases Weights \u0026amp; Biases (또는 W\u0026amp;B) 는 머신러닝 프로젝트 실험 관리 플랫폼으로, 유저는 각 실험의 결과와 모델의 성능, 결과물(artifact), 심지어 런타임과 리소스 사용 현황 등을 저장할 수 있다. 간단하게 파악해본 장단점은 다음과 같다.\n   각 실험 결과 관리\n     리소스 관리\n     각 실험결과의 artifact 저장 및 관리\n   장점:  거의 모든 요소들을 기록하고 저장할 수 있음 UI가 아주 편리하고 직관적   단점:  유료 플랜이 존재하며, 비즈니스에 따라 부담스러울 수 있음. 개인 프로젝트의 경우 아우 유용! 매우 편리하기 때문에, 경우에 따라 의존도가 너무 커질 수 있다고 생각됨.    3.2. Version Control System의 응용 VCS를 응용하여, 코드를 버전관리 하듯 Artifact의 변화와 정보를 기록하는 방식이다. 대용량 파일을 Git에 저장하는 Git Large File Storage(LFS), 또는 Git을 적용한 오픈소스 라이브러리인 Data Version Control (DVC)가 있다.\n3.2.1. 예시) DVC DVC는 오픈소스 프로젝트이며 Git을 기반으로 하고 있다. 사용자는 DVC를 활용해 데이터셋과 모델 등 artifact들을 버전관리 할 수 있으며, 실험 파이프라인 기능도 지원한다. DVC의 기본 원리는 ML 프로젝트 과정에서 발생하는 artifact들에 대한 모든 정보(경로 등)을 .dvc라는 파일에 기록하고 git 에 기록하되, 실제 파일은 .gitignore에 기록하여 저장되지 않도록 한다. 추후 사용자는 Git에서 원하는 버전의 .dvc 파일만 있으면 해당 artifact를 불러올 수 있다.\n 장점  오픈소스 Git에 익숙하다면 진입장벽이 거의 없음. AWS S3, Google Drive, GCP Stroage, SSH, HDFS 등 수많은 스토리지 형태를 지원함.   단점  복잡한 Git 브랜칭 전략이 가미될 경우, 모니터링 기능이 필요한 경우 등 유저가 직접 커스터마이징을 해야할 필요가 생김. (장점이면서 단점이 될 수 있음)          4. 마무리하며 이번 포스팅에서는 머신러닝 프로젝트에서 버전 관리를 해야하는 필요성과 버전관리에 사용할 수 있는 도구의 종류를 간단하게 살펴보았다. 다음에는 간단한 토이프로젝트를 통해 실제로 W\u0026amp;B와 DVC를 활용하는 예시를 소개해보도록 하겠다.\nReference  https://neptune.ai/blog/version-control-for-ml-models https://dvc.org/doc https://docs.wandb.ai/?_gl=114eseyj_gaMzAxNTk4NzQ3LjE2NTU5OTM0NTc._ga_JH1SJHJQXJ*MTY1OTU0NjQwMC4zLjEuMTY1OTU0NjQwNS41NQ https://realpython.com/python-data-version-control/#practice-the-basic-dvc-workflow  ","permalink":"https://wonyoungseo.github.io/posts/2022-09-04-introduction-data-version-control/","summary":"사내에서 데이터 버전 관리에 대한 내용을 소개하면서 정리한 내용을 포스팅으로 재정리 해보았습니다.\n1. 버전관리 1.1. 소프트웨어 엔지니어링에서 관점에서의 버전 관리 이제는 Git으로 익숙하게 접하는 버전 관리(version control)이 하는 역할을 소프트웨어 엔지니어링 관점에서 다시 한번 정리해보자면 다음과 같다.\n 소프트웨어를 이루는 코드와 패키지, 라이브러리 등에 생긴 변화를 추적하고 관리하기 위한 용도 여러 구성원이 소프트웨어를 개발할 때 발생하는 혼란을 줄이고, 효율적으로 협업할 수 있게 하는 용도 소프트웨어에 큰 문제가 발생했을 때, 빠르게 이전 버전으로 돌아가는 등, 에러나 실수로 인해 발생하는 손해를 최소화 하기 위한 용도  1.","title":"[KR] ML Version Control 의 개념"},{"content":"Udacity의 MLOps 프로그램    Udacity의 MLOps 관련 강의인 Machine Learning DevOps Engineer Nanodegree를 최근에 수료했다. 수강료 할인과 좋은 기회가 있어 수강하기 시작했는데, 중간에 사정이 있어 잠시 수강을 멈췄다가 최근에야 프로젝트를 완료하면서 수료할 수 있게 되었다. MLOps에 관심이 많은 분들은 해당 강의에 대해 궁금하실 것 같아, 강의 컨텐츠와 후기를 정리해보고자 한다.\n강의 컨텐츠 이 강의는 MLOps에 대한 기초를 다루고 있는 만큼, 파이썬은 물론이고 데이터사이언스와 머신러닝 프로젝트에서 자주 사용하는 라이브러리인 Pandas, Numpy, Scikit-Learn 등이 익숙한 사람들을 대상으로 한다.\n1. Clean Code Principle  핵심 내용  지속가능한 머신러닝 프로젝트와 협업을 고려한 클린 코딩 방법론 로그 남기기 (logging) 테스트 코드 작성   사용하는 라이브러리, 패키지, 툴, 서비스  AutoPEP8, PyLint, Pytest   프로젝트 내용  간단한 Random Forest 모델을 학습하며 logging 패키지를 사용해 로그를 남기고 테스트코드 작성하기    2. Building a Reproducible Model Workflow  핵심 내용  MLflow 를 활용해 각 머신러닝 프로젝트의 각 태스크를 파이프라인으로 연결하기  EDA 데이터 검증 모델 개선을 위한 실험 수행 최종 모델 선택 및 추론 수행   각 태스크 실행 시 Hydra를 활용한 config 관리 Weights \u0026amp; Biases (W\u0026amp;B)를 통해 각 모델링 실험 기록 남기고, 관련 artifact 저장하기   사용하는 라이브러리, 패키지, 툴, 서비스  Conda, MLflow, Weights \u0026amp; Biases, Hydra   프로젝트 내용  Airbnb 데이터셋을 기반으로 렌트 가격 예측하는 ML 프로젝트 머신러닝 파이프라인을 구성하여 효과적으로 실험을 반복수행하고, 각 실험 결과를 기록 및 저장하는 것이 핵심    3. Deploying a Scalable ML Pipeline in Production 개인적으로는 3장 \u0026ldquo;Deploying a Scalable ML Pipeline in Production\u0026quot;의 내용이 가장 유익하고, 프로젝트도 난이도가 있었다. 모델 전처리(+데이터 버저닝) -\u0026gt; 학습(+모델 버저닝) -\u0026gt; API 구축 -\u0026gt; 모델, API 테스트 -\u0026gt; 통합-\u0026gt; 배포 의 큰 사이클을 경험해볼 수 있다.\n 핵심 내용  데이터와 모델 버저닝에 대한 개념 소개 머신러닝 모델 서빙의 개념. RestAPI 소개 모델의 성능과 편향을 확인하는 기법 중 하나인 Data sclicing 소개   사용하는 라이브러리, 패키지, 툴, 서비스  FastAPI, DVC, Github Actions, AWS S3, Heroku   프로젝트 내용  인구통계 데이터셋을 통해 개인의 소득 수준을 예측하는 ML 프로젝트 API 구축 각기 다른 버전의 데이터셋과 모델의 기록하고 저장하기 Heroku에 배포하기 Github Actions를 통해 CI/CD 수행하기    4. ML Model Scoring and Monitoring  핵심 내용  데이터의 변화로 인해 발생하는 모델 드리프트(Model drift)에 대한 개념 소개 지속적인 모델 성능 평가 방법론 모델 성능 변화에 대한 분석 및 대처방법   사용하는 라이브러리, 패키지, 툴, 서비스  Scikit-learn, Seaborn, Crontab, Flask   프로젝트 내용  기존의 모델이 학습하지 못한 새로운 데이터에 대한 모델의 모니터링과 성능 평가, 재학습을 하나의 파이프라인으로 구성 주기적으로 수행할 수 있도록 스케쥴링 설정    장점 MLOps에 대한 best practice가 정립되지 않은 현재 상태에서 Machine Learning DevOps Engineer Nanodegree는 아주 좋은 입문 강의가 될 수 있다.\n특히 여러가지 도구들을 프로젝트를 통해 찍먹해볼 수 있고, 실제 업무에서 어떤 용도로 사용할 수 있는지 힌트를 얻을 수 있다는 점이 긍정적인 부분이다. 특히 클린코드에 대한 중요성을 강의 도입부에서 강조하고, 반복적인 실험 과정들을 자동화 할 수 있는 방법을 소개하는 부분은 \u0026ldquo;혼자 고군분투 하는 데이터사이언티스트, 리서쳐\u0026rdquo; 분들에게 큰 도움이 될 것 같다.\n단점 각 주제들이 각기 다른 강사들에 의해 구성되었기 때문에, 일관성이 다소 (아니 많이) 떨어진다. 예를 들어 3장 \u0026ldquo;머신러닝 모델 배포\u0026rdquo; 주제를 담당하는 강사는 FastAPI를 다루지만, 4장 \u0026ldquo;머신러닝 모델 운영 모니터링\u0026quot;의 주제를 다루는 강사는 Flask를 소개한다.\n각 주제들의 컨텐츠가 하나의 통일된 프레임워크와 도구를 쓰면서 유기적으로 구성되었다면 훨씬 완성도 있는 강의가 되지 않았을까? 각기 다른 회사와 단체에서 섭외한 여러 명의 강사들이 강의를 짜집기했다는 인상을 지울 수 없다(인상이 아니라 실제로 그러하다.)\n마무리하며 앞서 강의 짜집기를 단점으로 꼽았으나, 그럼에도 불구하고 현재 시중에 있는 MLOps 강의와 비교했을 때 \u0026ldquo;툴 사용 위주 짜집기\u0026quot;의 정도가 가장 낮고 프로젝트의 완성도가 있었다고 평가할 수 있겠다.\n강의 소개에서 권장하는 수강 기간은 약 4개월이지만, 수강생의 수준에 따라 더 빠른 수료도 가능할 것 같다. Nanodegree의 수강료(3개월 선불 시 120만원대)는 다소 부담이 될 수 있으나, 짧은 시간 안에 집중해서 수료을 목표를 목표로 한다면 월결제를 하는 것이 더 이득이고, 학생 신분일 경우 큰 폭의 할인도 받을 수 있으니 추천한다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-08-07-udacity-nanodegree-ml-devops-engineer-review/","summary":"Udacity의 MLOps 프로그램    Udacity의 MLOps 관련 강의인 Machine Learning DevOps Engineer Nanodegree를 최근에 수료했다. 수강료 할인과 좋은 기회가 있어 수강하기 시작했는데, 중간에 사정이 있어 잠시 수강을 멈췄다가 최근에야 프로젝트를 완료하면서 수료할 수 있게 되었다. MLOps에 관심이 많은 분들은 해당 강의에 대해 궁금하실 것 같아, 강의 컨텐츠와 후기를 정리해보고자 한다.\n강의 컨텐츠 이 강의는 MLOps에 대한 기초를 다루고 있는 만큼, 파이썬은 물론이고 데이터사이언스와 머신러닝 프로젝트에서 자주 사용하는 라이브러리인 Pandas, Numpy, Scikit-Learn 등이 익숙한 사람들을 대상으로 한다.","title":"[KR] Udacity Nanodegree: Machine Learning DevOps Engineer 후기"},{"content":"0. 머신러닝 추론 시스템이란 최근 머신러닝 모델을 서비스하기 위한 파이프라인을 구축하는 프로젝트를 진행하게 되었다. 경험 없이 밑바닥에서부터 시작하는 입장이었기 때문에 프로젝트를 본격적으로 진행하기에 앞서 여러가지 케이스를 분석하고 참고하는 시간을 가져보았다. 그 과정의 일환으로, 이번 글에서는 머신러닝 파이프라인의 한 부분을 담당하는 추론 시스템(ML inference system)에 대해 간단히 정리해보았다.\n머신러닝 추론 시스템은 학습된 머신러닝 모델을 불러와 실제 데이터에 대한 추론 결과를 사용자에게 제공하는 기능을 한다. 추론시스템은 누군가가 실제로 머신러닝 모델 프로덕트를 사용한다. 따라서, 머신러닝에서 우리가 익숙한 [데이터수집 -\u0026gt; 데이터정제 -\u0026gt; 모델링 -\u0026gt; 학습 -\u0026gt; 성능평가 -\u0026gt; 실험반복]의 패턴과는 확연히 다르며, 다양한 관점에서 고려해야할 부분들이 생긴다.\n추론시스템은 목적과 자원에 따라 여러가지가 있어, 각 케이스와 구현 방식에 따라 정보가 매우 파편적이었지만, 머신러닝 시스템 디자인 패턴(저자: 시부이 유우스케) 에서 포괄적으로 여러가지 케이스에 대해 다루고 있어 아주 좋은 자료가 되었다. 패턴에 대한 네이밍은 다양하게 있을 수 있으나, 해당 포스트에서는 책의 네이밍과 내용을 참고하여 정리했다.\n\u0026amp;nbsp\n 1. 추론 시스템의 베이스 패턴 1.1. 웹 싱글 패턴 개념과 구조:\n 하나의 API 서버에 하나의 추론 모델이 동작함. 사용자는 API에 데이터를 전송하여, 그에 따른 모델 추론 결과를 받음.  목적 또는 사용 예시:\n 모델 구현 등 간단한 프로젝트의 시연 결과물 실제 데이터를 대상으로 모델의 성능을 검증하는 단계에 적합  장점:\n 간단하게 구현 가능.  주의점:\n 2개 이상의 모델이 추가되어야 하거나, 전처리 등의 추가 단계를 거쳐야 한다면 적합한 패턴이 될 수 없음.  \u0026amp;nbsp\n1.2. 전처리 추론 패턴 개념과 구조:\n 대부분의 경우 실제 데이터에 대하여 전처리 이후 추론하는 경우가 많음. 전처리와 추론을 분리하여 코드 관리와 유지보수를 용이하게 함. 전처리와 추론 시스템을 개별 API 서버로 구분하여 개발함.  목적 또는 적용 예시:\n 전처리와 추론에서 필요로 하는 라이브러리, 코드 베이스, 리소스 등이 크게 다를 경우 전처리와 추론 시스템의 개발의 구분해야 하는 경우  장점:\n 전처리와 추론 과정 간 장애의 격리하고, 각 과정의 개발을 구분함. 사용되는 리소스를 효율적으로 관리할 수 있음. 사용할 수 있는 라이브러리와 버전 등을 유연하게 선택할 수 있음.  주의점:\n 추론시스템은 학습된 시점에 사용된 전처리 방식과 모델을 사용해야 하므로, 전처리 시스템과 추론 시스템은 늘 동일한 버전을 유지해야 함.  \u0026amp;nbsp\n1.3. 배치 추론 패턴 개념과 구조:\n 배치 추론 패턴은 일정한 주기마다 다량의 데이터에 대하여 추론을 실시함. 학습된 모델이 API 서버가 아닌, 스케쥴러에 의해 동작함.  목적 또는 적용 예시:\n 모델이 동작하는 방식이 굳이 실시간(real-time) 또는 실시간에 준하는(near-real-time) 추론이 요구되지 않는 경우 일정한 시간동안 데이터를 적재한 뒤 주기적으로 주기적으로 추론하는 경우 과거의 데이터에 대하여 추론하고 싶은 경우 (historical data)  장점:\n 배치 추론이 실행될 때만 자원을 사용하면 되기에, 비용 절감 가능 장애 대응 등 시간적 여유를 가지고 추론이 가능함 (단, 비즈니스가 이러한 여유를 허용할 경우)  주의점:\n 시간의 지남에 따라 데이터의 특성이 변화하는 상황이 발생한 경우 적기에 대응이 쉽지 않으므로, 모니터링 기준이 필요함. 소량의 데이터라도 짧은 주기에 한번씩 추론을 실행하여 결과를 확인하는 과정이 필요할 수 있음.  \u0026amp;nbsp\n 2. 순서에 대한 패턴 2.1. 동기 추론 패턴 개념과 구조:\n 모델이 동작하는 서버에 요청을 하여 추론 결과가 나올 때까지 기다리는 구조를 동기추론 패턴이라 함.  목적 또는 적용 예시:\n 추론 이후의 단계가 추론 결과에 의존 하는 경우. 예시) 불량품 검출 모델 결과를 반환받아, 불량품을 정상 제품으로부터 분리 함.  장점:\n 각 작업이 순차적이고 직관적임. 개발이 다소 간단함.  주의점:\n 프로세스에 병목현상을 야기하는 작업단계가 있을 경우, 사용자는 추론 결과를 받기까지 기다려야 하는 시간이 길어짐. 따라서 작업의 속도를 개선하거나, 모델의 경량화 또는 비동기 추론 패턴을 적용한 프로세스를 고려하는 것이 적합함.  \u0026amp;nbsp\n2.2. 비동기 추론 패턴 개념과 구조:\n 사용자는 추론 요청을 하고 결과가 반환 때까지 기다리지 않고 다른 작업을 수행할 수 있으며, 추론이 완료되었을 때 개별적으로 결과를 확인할 수 있음. (비동기적인 처리) 사용자의 요청을 큐 또는 캐시로 저장하여 순차적으로 추론한 뒤, 추론이 완료되었을 결과를 DB에 저장하고 사용자에게 추론 결과가 반환되었으며 확인이 가능함을 알림.  목적 또는 적용 예시:\n 비즈니스 로직에 따라, 모델의 결과를 즉시 반환하지 않아도 되는 경우. 모델의 추론 시간이 오래 걸리는 경우.  장점:\n 추론 과정에서 발생할 수 있는 병목현상을 해결할 수 있음. 어떻게 설계했는지에 따라 사용자 경험을 개선할 수 있음.  주의점:\n 사용자의 요청을 저장하고 추론하는 순서를 경우에 따라 선택해야하며, 시스템 장애가 발생하는 경우 대응 방법을 고려해야 함.  \u0026amp;nbsp\n 3. 2개 이상의 모델에 대한 추론 패턴 3.1. 직렬 마이크로서비스 패턴 개념과 구조:\n 2개 이상의 모델 추론이 필요한 어플리케이션에서, 모델 간의 순서의 의존성이 존재하는 경우. 단, 여러 개의 추론 시스템을 마이크로서비스로 구현하여, 각 모델 간의 결합도를 낮춤. 모델이 구동되는 여러 개의 추론 마이크로서비스를 워크플로우 파이프라인을 통해 DAG(directed acyclic graph)의 형태로 연결하여 구현할 수 있음.  목적 또는 적용 예시:\n 여러 개의 모델이 순차적으로 실행되어 최종적으로 하나의 결과를 반환하는 유형에 적합.  장점:\n 모델과 추론 시스템의 사이즈가 커질 수록, 마이크로서비스 패턴은 리소스 비용을 효율적으로 관리할 수 있음. 독립적으로 구성되어 있기 때문에 직전 추론 결과에 따라 분기처리 또한 가능.  주의점:\n 동기 추론 패턴과 마찬가지로 추론에 걸리는 시간에 따라 병목현상이 발생할 수 있음. 마이크로서비스 패턴은 서버간 통신으로 인한 지연도 발생할 수 있음.  \u0026amp;nbsp\n3.2. 병렬 마이크로서비스 패턴 개념과 구조:\n 여러 개의 추론 마이크로서비스가 의존관계 없이 병렬로 실행되어 결과가 저장됨. 필요에 따라서는 각 추론 결과를 마지막에 집계하는 구조도 가능함.  목적 또는 적용 예시:\n 하나의 데이터에 대해 여러가지의 모델이 추론을 해야하는 상황에 적합함.  장점:\n 의존 관계가 없어 유연하게 설계가 가능함. 각 모델의 추론 서버가 구별되어, 리소스 관리나 장애 대응이 용이해짐.  주의점:\n 각 추론 서버들의 결과 반환 속도가 다르므로, 결과를 집계하는 방식에 대한 고려가 필요함.  \u0026amp;nbsp\n 4. 캐싱 패턴 4.1. 추론 캐시 패턴 개념과 구조:\n 추론 결과를 캐시로 저장하고, 추후 동일한 데이터에 대해 추론 요청이 있을 때는 추론할 필요 없이 캐시된 결과를 반환하는 개념. 입력된 데이터가 저장되고 식별 및 검색이 가능해야 함. 데이터에 매핑되어 저장된 추론 결과를 반환해야 하기 때문.  목적 또는 적용 예시:\n 동일한 데이터가 추후에도 입력될 수 있고, 추론 시간을 단축해야 하는 경우.  장점:\n 추론 속도를 단축할 수 있으며, 추론 시스템이 구동되는 리소스의 비용을 줄일 수 있음.  주의점:\n 입력 데이터가 적재되는 저장공간과 추론 결과가 저장되는 캐시로 인한 비용이 발생할 수 있음. 동일한 데이터가 발생되는 유형의 목적에 유효한 패턴이며, 그 외 \u0026ldquo;유사한 데이터\u0026rdquo; 수준인 경우에는 적용되지 않음.  \u0026amp;nbsp\n4.2. 데이터 캐시 패턴 개념과 구조:\n 동일한 데이터에 대해 추론 요청이 있을 경우, 데이터 전처리 결과를 캐시하여 추론하는 경우. 추론 캐시 패턴과 유사하며 함께 사용될 수 있음. 추론 캐싱 패턴과 마찬가지로 입력 데이터가 식별이 가능해야 함.  목적 또는 적용 예시:\n 데이터 전처리, 피쳐 추출 등 데이터 처리와 관련된 시간이나 비용이 클 경우 적용할 수 있음.  장점:\n 빈번하게 발생하는 동일한 데이터 전처리와 피쳐 추출로 인해 발생하는 비용을 줄일 수 있음.  주의점:\n 추론 캐싱 패턴과 마찬가지로, 캐시가 적재되는 용량과 그에 따른 비용이 커질 수 있음.  \u0026amp;nbsp\n 5. 추론 시스템의 안티 패턴 추론 시스템을 구축할 때는 지양해야 하지만 간과하기 쉬운 안티 패턴이 몇 가지 있다.\n5.1. 온라인 빅사이즈 패턴 개념:\n 실시간 처리, 스트리밍 데이터에 대한 추론에 큰 규모의 모델을 적용, 이에 따라 서비스 지연이 발생하는 경우 배치 추론의 구조를 가지고 있지만, 1회 배치 추론의 시간이 배치 추론 주기를 초과하는 경우  접근 방법:\n 리소스 규모를 확대하여 처리 속도를 개선함. 2개 이상의 모델일 경우 가벼운 모델과 무거운 모델의 서비스를 분리함. 캐싱 패턴을 통해 속도를 개선함. 완벽하고 거대한 모델이 아닌 비즈니스 요구에 충족하는 수준의 모델을 개발함.  5.2. 올인원 패턴 개념:\n 하나의 서버에서 전처리, 2개 이상의 모델 등 모든 것을 가동하는 경우 활용하는 리소스의 비용은 절약할 수 있으나, 다음과 같은 측면에서 운영이 용이하지 않음.  장애 대응 시스템 갱신 모델 갱신 및 관리 라이브러리 갱신    접근 방법:\n 전처리, 추론 모델 서버 등을 마이크로서비스로 구현함.  \u0026amp;nbsp\n 7. 마무리하며 추론 시스템을 개발하기 위한 다양한 패턴과 고려해야할 점들을 훑어보았다. 기존의 데이터사이언티스트 관점에서 막막한 영역이었으나, 개념이 정리되어 앞으로의 업무에 도움이 많이 되었다. 현업에서는 훨씬 복잡한 서비스를 위해 위에서 정리한 패턴에 그치지 않고 이를 조합하거나 새로운 패턴을 고안할 것으로 생각된다. 정리한 내용을 토대로 앞으로는 다른 기업의 케이스 등을 분석해보고 업무에 참고할 수 있을 것 같다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-07-22-inference-system-pattern/","summary":"0. 머신러닝 추론 시스템이란 최근 머신러닝 모델을 서비스하기 위한 파이프라인을 구축하는 프로젝트를 진행하게 되었다. 경험 없이 밑바닥에서부터 시작하는 입장이었기 때문에 프로젝트를 본격적으로 진행하기에 앞서 여러가지 케이스를 분석하고 참고하는 시간을 가져보았다. 그 과정의 일환으로, 이번 글에서는 머신러닝 파이프라인의 한 부분을 담당하는 추론 시스템(ML inference system)에 대해 간단히 정리해보았다.\n머신러닝 추론 시스템은 학습된 머신러닝 모델을 불러와 실제 데이터에 대한 추론 결과를 사용자에게 제공하는 기능을 한다. 추론시스템은 누군가가 실제로 머신러닝 모델 프로덕트를 사용한다. 따라서, 머신러닝에서 우리가 익숙한 [데이터수집 -\u0026gt; 데이터정제 -\u0026gt; 모델링 -\u0026gt; 학습 -\u0026gt; 성능평가 -\u0026gt; 실험반복]의 패턴과는 확연히 다르며, 다양한 관점에서 고려해야할 부분들이 생긴다.","title":"[KR] 머신러닝 추론 시스템 패턴"},{"content":"   핸즈온 데이터 시각화 (저자: 잭 도허티, 일리야 일라얀코우)\n  추천하는 대상:\n 시각화를 업무에 자주 사용하는 분  한줄평: 데이터시각화를 위한 교과서적인 책이며, 중요한 개념인만큼 차근차근 짚어나간다.\n친근하면서도 어려운 데이터 시각화 데이터 시각화는 단순히 데이터를 가지고 그래프를 그리는 행위가 아니다. 데이터 시각화는 데이터에 기반한 분석과 주장을 더욱 효과적으로 전달하기 위한 한 방법이다. 텍스트만으로는 전달하기 힘든 통찰은 차트, 지도 등과 결합하여 더욱 강한 설득력을 가진다.\n핸즈온 데이터 시각화 이렇게 단순히 차트를 그리는 것을 넘어 데이터 시각화에 대한 이론부터 차근차근 다루고 있는 핸즈온 데이터 시각화 는 2020년 경부터 이미 저자들이 인터넷 상에 책 내용을 무료로 공개하여 화제가 된 적이 있는 데이터 시각화 강의 교재이다. (독자의 편의성을 위해 실물로도 판매가 되고 있는데, 2022년 판매 금액은 전액 우크라이나 구호 재단에 기부된다고 하여 더욱 의미가 깊은 책이다.)\n전 독자층을 배려한 구성 이 책은 데이터 시각화에 대한 올바른 접근을 위한 이론과, 주로 구글 스프레드시트, 태블로 등의 도구를 활용한 실습 예제로 이루어져 있다. 또한 자바스크립트 기반의 Chart.js, 나 Highcharts 같은 기술의 경우, 코딩이 익숙하지 않은 독자층을 위해 코드를 작성하지 않고도 실습을 진행해볼 수 있도록 한 점이 눈길을 끈다.\n의미있는 스토리를 전달하기 위한 시각화 이 책에서 가장 도움이 많이 되었던 부분을 뽑자면, 책의 후반부에 기술된 시각화를 통해 정보를 왜곡된 시각화의 사례와 편향 등을 다룬 부분이다.\n우리는 시각화를 통해 효과적으로 정보에 기반한 주장을 할 수도 있지만, 의도적으로 왜곡된 정보를 전달할 수도 있다. 시각화를 통해 거짓말을 할 수 있는 방법에는 여러가지가 있다.\n 변화를 과장하거나, 변화를 축소하여 보여준다. 비율을 의도적으로 조작하여 보여준다. 세로축을 여러개 사용하여 혼돈을 준다.  이러한 부분을 인지한다면, 시각화를 통해 생길 수 있는 오해를 방지하고 보다 객관적인 관점을 유지할 수 있을 것이다.\n마무리하며 업무를 직접 진행하며 한 경험에 빗대어 본다면, 데이터 시각화는 사람마다 보는 관점이 달라 생각할 수록 어려운 영역이었다. 저자의 생각도 마찬가지이다. 하지만 시각화의 여러가지 사례와 안티 패턴을 소개하면서 읽는 이로 하여금 효과적인 데이터 시각화란 무엇인지 생각해볼 수 있는 능력을 키워주려고 하는 것이 이 책의 목적이라고 할 수 있다. 데이터 시각화는 절대적인 정답이 없다. 다만 저자는 이렇게 말한다.\n 학습자로서 여러분이 해야할 일은 문제에 대한 단 하나의 정답만을 고수하지 않고, 계속해서 더 좋은 답을 찾는 것입니다.\n  한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-07-19-book-reivew-hands-on-data-visualization/","summary":"핸즈온 데이터 시각화 (저자: 잭 도허티, 일리야 일라얀코우)\n  추천하는 대상:\n 시각화를 업무에 자주 사용하는 분  한줄평: 데이터시각화를 위한 교과서적인 책이며, 중요한 개념인만큼 차근차근 짚어나간다.\n친근하면서도 어려운 데이터 시각화 데이터 시각화는 단순히 데이터를 가지고 그래프를 그리는 행위가 아니다. 데이터 시각화는 데이터에 기반한 분석과 주장을 더욱 효과적으로 전달하기 위한 한 방법이다. 텍스트만으로는 전달하기 힘든 통찰은 차트, 지도 등과 결합하여 더욱 강한 설득력을 가진다.\n핸즈온 데이터 시각화 이렇게 단순히 차트를 그리는 것을 넘어 데이터 시각화에 대한 이론부터 차근차근 다루고 있는 핸즈온 데이터 시각화 는 2020년 경부터 이미 저자들이 인터넷 상에 책 내용을 무료로 공개하여 화제가 된 적이 있는 데이터 시각화 강의 교재이다.","title":"[KR] 책 리뷰 : 핸즈온 데이터 시각화"},{"content":"   기업의 성공을 이끄는 Developer Relations (저자: 메리 셍발)\n  추천하는 대상:\n Developer Relations라는 용어가 궁금하신 분 기술을 사랑하고 커뮤니티 활동에도 관심이 많으신 분 오픈소스 활동에 관심 있으신 분  한줄평: 한국에서 DevRel 에 대한 인식과 활동 무대가 더욱 커질 수 있는 시작점이 될 책\n들어가기에 앞서, Developer Relations 이라는 용어를 처음 접하고, Public Relations이 떠올랐다. 보통 줄여서 PR 이라고 하고, 한국에서는 PR한다 와 같이 콩글리쉬로도 쓰인다. 흔히들 PR을 홍보, 마케팅과 결을 같이 하는 용어라고 이해하는 경우가 많지만, 좀 더 정확히 하자면, PR은 대중과 호의적인 관계를 유지하기 위한 모든 활동을 의미한다. (홍보 또는 마케팅은 그 일부분이다.) 그렇기에 PR은 기업의 상업적인 활동 뿐만 아니라, 정당, 정치단체, 기관, 정부, 연예인과 같은 유명인사들도 대중을 대할 때, 필수로 PR 담당자를 대동하거나, PR 전문 업체의 조언을 받는다.\n이렇듯 relations 라는 단어는 한글로 단순히 \u0026ldquo;관계\u0026quot;를 의미하지만, 그 역할이 커버하는 영역은 정말 광범위 하다.\n그래서인지 인터넷에서 Developer Relations 이라는 용어를 접했을 때, 해당 직군이 개발 업무 그 자체가 아닌 개발자를 대하는 업무를 한다는 것은 직감했다. 하지만 개발자와의 관계를 유지하고 개발자를 대하는 직군이라는 건 어떤 일을 하는 것인지, 감을 잡기 어려웠다. 기업의 성공을 이끄는 Developer Relations 가 눈길을 끌었던 이유다.\nDeveloper Relations? 책에서 정의하는 Developer Relations(DevRel, 이하 데브렐)은 다음과 같다.\n 기업과 개발자의 관계를 만들고 유지하며, 장기적으로 회사가 기술적 우위를 점할 수 있도록 기반과 생태계를 만드는 분야\n 정말 와닿았다. 데브렐이란, 기술을 개발하는 기업이 해당 기술을 사용하는 개발자들과 소통하며 소위 팬층을 구축하고, 이들이 기술 사용에 있어 궁금한 점이나 어려움이 있을 때 서로 도움을 주며 더 좋은 아이디어가 있으면 건의하거나 더 나아가 기여할 수 있는 사용자들만의 커뮤니티를 만들어나가는 일을 하는 것이다.\n늘 많은 도움을 받고 있는 한국 Elasticsearch 사용자 페이스북 그룹이 그러했으며, 최근에 컨퍼런스를 열었던 통합 Databricks, Weighs \u0026amp; Biases, 그리고 꾸준하게 MLOps KR 커뮤니티를 이끌어나가고 있는 SuperbAI의 커뮤니티 그로스 매니저님도 비슷한 사례였음을 뒤늦게 깨닫게 되었다.\n조금 더 곱씹어보면 이들에게 개발자들은 자사의 기술을 함께 발전시켜나갈 동반자이기에 고객 그 이상의 가치를 가진다. 그래서 전통적인 영업의 기술이나 고객 관리의 역학이 단순하게 적용되지 않는다.\n생각보다 매우 sophisticated한 분야임은 틀림없다. (우리말로 표현하기에 적당한 말을 찾을 수 없었다)\n세상 친절한 DevRel 입문서 이 책에서는 막연하게 짐작할 수 있는 데브렐의 영역에 대해 작은 부분까지 짚어가며, 친절하게 설명되어 있어, 독자는 다음과 같은 질문에 대한 답을 얻을 수 있다.\n사용자 커뮤니티의 필요성  왜 현 시점에서 개발자(사용자)들만의 커뮤니티가 필요한가? 필요하다면, 커뮤니티를 통해 이루고자 하는 것은 무엇인가 ? 커뮤니티의 역할은 무엇인가?  새로운 사용자층의 유입을 위한 대외 홍보의 공간 기존 프로덕트를 발전시켜나가기 위한 피드백과 토론의 공간    데브렐 조직의 시작  커뮤니티 구축을 위한 데브렐 팀을 만드려면 어디서부터 시작해야 하는가?  데브렐 조직은 기업의 어떠한 니즈를 충족시킬 수 있는가? 데브렐 팀의 탄생을 위해서 어떤 이해관계자들을 설득해야 하는가?   데브렐 팀의 포지셔닝 데브렐팀의 성과는 어떻게 측정하는가?  커뮤니티 운영을 위한 팁  커뮤니티를 새로 만들어야 할 지, 존재하는 커뮤니티를 공략해야 하는가? 커뮤니티에서 이벤트는 어떻게 기획하는가? 커뮤니티에서 생길 수 있는 문제는 어떤 것들이 있고, 어떻게 대응해야 하는가? 커뮤니티 내에서 데브렐 담당자는 어떻게 포지셔닝해야 하고, 어떤 이미지를 가져야 하는가? 어떻게 이미지(퍼스널 브랜딩)를 구축하는가?  데브렐 전문가의 인터뷰 이 책의 한글 번역 출판본에는 라인, SK텔레콤, 우아한 형제들 등 한국 기업의 데브렐 담당자들의 인터뷰를 추가적으로 담고 있다.\n각 조직들의 현업에 대한 이야기가 담겨있는 한편, 기술 자체로 프로덕트를 만드는 기업이 아니더라도 데브렐 조직은 기술 조직의 브랜딩을 담당하여 치열한 구인시자에서 좋은 인재 채용에 기여하는 등 또 다른 역할과 커뮤니티 타겟층에 대한 내용도 다루고 있다.\n책의 본문에 소개된 다양한 해외 사례와 더불어 이와 같이 한국 데브렐 전문가의 인터뷰까지 수록되었기 때문에, 단순한 번역본보다도 더 완성도가 있는 책이라는 생각이 든다.\n책에서 언급된 것처럼 데브렐은 2017년 이후에나 접하게 된, 매우 새로운 영역임이 틀림 없다. 데브렐 포지션에 관심이 없더라도 한번이라도 커뮤니티에 질문을 올렸거나, 구글링을 통해 커뮤니티에서 문제에 대한 해답을 찾았던 개발자라면(아마 거의 대부분의 개발자가 아닐까\u0026hellip;?), 커뮤니티의 수혜를 한번이라도 입은 개발자라면, 한번은 읽어봄직한 책이라 생각된다. 번역도 매끄럽고 잘 읽힌다. 추천!\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-06-23-book-review-developer-relations/","summary":"기업의 성공을 이끄는 Developer Relations (저자: 메리 셍발)\n  추천하는 대상:\n Developer Relations라는 용어가 궁금하신 분 기술을 사랑하고 커뮤니티 활동에도 관심이 많으신 분 오픈소스 활동에 관심 있으신 분  한줄평: 한국에서 DevRel 에 대한 인식과 활동 무대가 더욱 커질 수 있는 시작점이 될 책\n들어가기에 앞서, Developer Relations 이라는 용어를 처음 접하고, Public Relations이 떠올랐다. 보통 줄여서 PR 이라고 하고, 한국에서는 PR한다 와 같이 콩글리쉬로도 쓰인다.","title":"[KR] 책 리뷰 : 기업의 성공을 이끄는 Developer Relations"},{"content":"어떠한 일련의 작업들을 순서대로 묶어 위험요소나 외부의 개입을 최소화하고 자동으로 실행하고자 할 때 파이프라인을 구축한다.\n파이프라인의 개념이 적용되지 않은 머신러닝 프로젝트는 결과물을 얻기 위한 과정의 자동화와 재사용성이 확보되지 않기 때문에 일회성 PoC에 그칠 가능성이 높다.\n머신러닝 파이프라인이라는 개념은 머신러닝 모델을 학습하고, 재사용하며, 필요한 자원들을 관리하고 배포하는 일련의 과정을 표준화하고 자동으로 동작할 수 있도록 하기 위해 논의 되기 시작했다.\nDAG 파이프라인은 대개 DAG(Directed acyclic graph: 방향성 비순환 그래프)의 형태를 띄고 있다. 이름에서 알 수 있듯이, DAG는 방향을 가지되, 루프는 존재하지 않아, 그래프의 시작과 종료가 이루어지는 시점이 명확하기에 파이프라인을 구성하는데 적절한 구조이다.\n   DAG의 예시\n  DAG의 형태를 띈 머신러닝 파이프라인을 구성하는 요소에는 컴포넌트(component)와 아티팩트(artifact)가 있다.\n머신러닝 파이프라인의 요소 머신러닝 파이프라인에는 각각의 작업을 하는 컴포넌트와, 컴포넌트의 결과물이자 다음 작업에 사용되는 아티팩트가 있다.\n      머신러닝 파이프라인에서의 컴포넌트와 아티팩트\n  컴포넌트 (Component)  컴포넌트는 각각의 독립되고 재사용이 가능한 모듈로서 입력을 받아 어떠한 처리나 연산을 거쳐 결과값을 출력한다. 스크립트, 노트북 또는 다른 실행 가능한 형태의 코드가 될 수 있다. 예) 데이터 검증, 데이터 전처리, 학습, 평가, 추론 등  아티팩트 (Artifact)  아티팩트는 컴포넌트의 산출물로서, 파일이나 경로가 될 수 있다. 아티팩트는 다른 컴포넌트의 입력값의 역할을 하며, 컴포넌트와 컴포넌트 사이를 잇는 역할을 할 수 있다. 아티팩트는 저장된 경로와 그 버전이 꼭 관리가 되어야 한다. 예) 데이터, 모델, 피쳐 등   머신러닝 파이프라인의 단계 머신러닝 파이프라인에도, 구현하고자 하는 기능과 재사용성, 자동화 여부 등에 따라 파이프라인의 성숙도 즉, 수준이 구별된다.\n   Level 0 모델을 위한 갖가지 학습을 반복적으로 진행하고, 특정한 성능 지표의 달성을 최우선으로 한다. 주로 노트북이나 개별로 실행하는 스크립트의 형태를 띄고 있으며, 재사용성은 매우 낮다. 따라서 프로덕션 용도로 사용되지 않는다.\n주로 사이드프로젝트나, 새로운 기술을 익히기 위한 용도로 사용되며, 캐글 대회나 연구, PoC 등을 진행할 때 주로 보인다.\n즉, 파이프라인으로서의 역할과 그 효과가 매우 미미하거나 없는 형태인 수준이라고 할 수 있다.\nLevel 1 데이터 처리와 학습, 성능 평가, 추론 등의 단계가 컴포넌트로 독립되어 있으며 각각의 산출물은 아티팩트로서 저장된다. 따라서 재학습이 용이하며, 각 실험결과 또한 기록된다.\n프로덕션 상태에서도 모니터링이 이루어지며, 추론 속도, 재학습 속도 등의 성능 또한 중요한 지표로 기록된다.\n레벨 1의 파이프라인은 최소한의 작업단위들을 모듈로서 표준화 하고 빠르게 프로덕션 환경을 프로토타이핑 하는데 용이하며, 재학습을 가능케 한다. 또한, 이러한 형태의 파이프라인은 업무의 공유나, 인수인계 상황에서 복잡한 과정을 피할 수 있다.\nLevel 2 레벨 2의 파이프라인은 CI/CD(Continuous Integration and Continuous Deployment)와 더불어 CT(Continuous Training)의 요소까지 접목된 수준을 나타낸다.\nCI/CD/CT가 접목된 파이프라인은 모델의 개선과 프로덕션의 수정을 용이하게 하고, 개선된 파이프라인과 기존 파이프라인의 A/B테스팅이 용이해진다. 또한 공백없는 배포를 자동으로 수행하기 때문에, 결과적으로 협업을 통한 제품의 개선과 반영 속도를 훨씬 빠르고 안전하게 할 수 있다.\n 마무리하며 이번에는 MLOps 에 대해 공부하며, 머신러닝 파이프라인에 대한 개념과 수행 기능, 요건 등을 정리해보았다. 현재 머신러닝 파이프라인을 구축하는데 사용되는 툴이나 라이브러리는 너무나 다양하게 포진되어 있기에, 어떠한 특정 툴을 사용하는 것보다는, 파이프라인을 구축하는데 무엇이 필요하고 어떤 기능이 꼭 구현되어야 하는지를 제대로 파악한 뒤, 필요에 따른 적합한 도구를 사용하는 것이 매우 중요하다고 판단된다.\nReference  Udacity: Machine Learning DevOps Engineer  ","permalink":"https://wonyoungseo.github.io/posts/2022-06-12-intro-ml-pipeline/","summary":"어떠한 일련의 작업들을 순서대로 묶어 위험요소나 외부의 개입을 최소화하고 자동으로 실행하고자 할 때 파이프라인을 구축한다.\n파이프라인의 개념이 적용되지 않은 머신러닝 프로젝트는 결과물을 얻기 위한 과정의 자동화와 재사용성이 확보되지 않기 때문에 일회성 PoC에 그칠 가능성이 높다.\n머신러닝 파이프라인이라는 개념은 머신러닝 모델을 학습하고, 재사용하며, 필요한 자원들을 관리하고 배포하는 일련의 과정을 표준화하고 자동으로 동작할 수 있도록 하기 위해 논의 되기 시작했다.\nDAG 파이프라인은 대개 DAG(Directed acyclic graph: 방향성 비순환 그래프)의 형태를 띄고 있다. 이름에서 알 수 있듯이, DAG는 방향을 가지되, 루프는 존재하지 않아, 그래프의 시작과 종료가 이루어지는 시점이 명확하기에 파이프라인을 구성하는데 적절한 구조이다.","title":"[KR] 머신러닝 파이프라인 개념 정리"},{"content":"   MLOps 도입 가이드 (저자: 데이터이쿠)\n  추천하는 대상:\n MLOps를 도입하기 위한 고민을 하는 분  한줄평: 머신러닝 모델을 배포하는 것은, 잘 돌아가는 걸 확인하는 하나의 단계일 뿐, 그 외 해야할 일은 많다.\n더 이상 선택요소가 아니게 된 MLOps 머신러닝이라는 기술에 대한 고도화가 이루어져 다양한 분야에서 활용되고 있는 현 시점에서, MLOps 라는 방법론 한번 슬쩍 접하는 것이 아닌 필수 사항이 되어가고 있다. MLOps는 현재 명확하게 정립되지 않아 논문이 아닌 수많은 블로그 포스트들로 다루어지고 있고, 또 수많은 도구들이 나타나 서로가 편리하다고 주장을 하고 있는 상황이다. 요즘 들어 나오고 있는 MLOps 관련 강의들 또한 이러한 도구들의 사용법에 대해 다루는 경우를 많이 볼 수가 있다.\n하지만 누구나 현재 가지고 있는 자원과 또는 지원 받을 수 있는 자원이 다르고, 다룰 수 있는 능력의 정도가 다르기에, 더더욱 개념부터 더 확실하게 숙지하고 현재 나의 일에 어떻게 적용시킬 수 있을지 고민해보는 과정이 필요하다고 느끼고 있다.\nMLOps 도입가이드 의 원제는 “Introducing MLOps”으로, 제목 그대로 MLOps에 대한 전반적인 내용을 다루고 있다. 이 책에서는 MLOps에 대한 전반적인 내용을 폭 넓게 다루며, 실제 구현보다는 각 개념과 필요성에 대한 설명에 초점을 맞추고 있다.\n   MLOps 의 수많은 이해관계자들의 역할\n   MLOps의 개념 및 이해관계자들 MLOps의 핵심 기능과 필요성 MLOps 적용 방법  모델 개발 상용화 준비 상용 배포 모니터링 반복 거버넌스   적용 사례  한가지 눈에 띄는 점이라면, 책의 저자가 미국과 프랑스에 오피스를 두고 있는 AI/ML 전문 기업인 데이터이쿠(Dataiku) 라는 점이다. 데이터이쿠에서 일하고 있는 9명의 임원과 스태프들이 이 책의 각 부분을 맡아 집필했는데, 1~3명의 소수가 책을 쓰는 것과는 확연히 차이점을 보이고 있다. 그만큼 이 책은 한 기업에 속한 여러 전문가가 각자의 전문성을 발휘할 수 있는 주제를 맡았다는 점에서, 인상적이다.\nMLOps는 리스크 관리 이 포스트에서 이 책이 다루는 MLOps에 대한 내용을 전부 다 다룰 수는 없고, 또한 시중의 다른 자료들과 겹치는 부분도 많다. 하지만 가장 인상적인 부분을 하나 꼽아보자면, 리스크를 줄이고자 하는 차원에서의 MLOps의 필요성에 대해서 서술한 부분이다.\n   머신러닝 모델의 리스크를 측정하기 위한 리스크 매트릭스\n   \u0026ldquo;결국, 모델을 상용 환경에 배포하는 작업은 머신러닝 모델 생애주기의 최종 단게는 아니다. 단지 성능과 정상 작동 여부를 확인하는 시작점이 될 뿐이다. (중략) 더 많은 머신러닝 모델을 상용 환경에 배포할수록, MLOps는 비즈니스에 치명적일 수 있는 잠재적 리스크를 줄이는데 더 필수적인 요소가 된다.\u0026quot;\n 머신러닝 모델은 운용하는 과정에서 다양한 리스크를 맞이할 수 있다. 앞서 언급했듯이, 어느 조직이나 활용 가능한 리소스가, 리소스를 갖추기 위한 지원의 정도가 다르다. 따라서 이 책에서 제시한 리스크 매트릭스를 활용하여, 머신러닝 모델의 리스크를 정량적으로 판단하고, 이에 따라 MLOps의 적용 범위와 정도를 정할 수 있을 것이라 기대된다. 만일 리스크가 크지 않다면 MLOps 시스템을 구축하는 과정에서 우선순위가 높지 않을 수 있고, 만일 리스크가 크지만, 이에 대비할 수 있는 리소스가 갖춰지지 않았다면 이에 대비하는 작업을 우선적으로 시행할 수 있을 것이다.\n마무리하며 MLOps 도입 가이드 는 책의 두께 두껍지 않아 빠르게 읽고, 필요한 부분을 참고할 수 있다. 다만, 앞서 말한 바와 같이 MLOps에는 많은 이해관계자가 필요하고, 각자 가지고 있는 환경과 리소스가 다르기에, 한번에 모든 것을 갖추기보다, 핵심적으로 필요하거나, 시급한 부분을 취사 선택하는 것은 독자의 몫(= 나의 몫)이다. 가벼운 길잡이로 활용해보자.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-05-28-book-review-introducing-mlops/","summary":"MLOps 도입 가이드 (저자: 데이터이쿠)\n  추천하는 대상:\n MLOps를 도입하기 위한 고민을 하는 분  한줄평: 머신러닝 모델을 배포하는 것은, 잘 돌아가는 걸 확인하는 하나의 단계일 뿐, 그 외 해야할 일은 많다.\n더 이상 선택요소가 아니게 된 MLOps 머신러닝이라는 기술에 대한 고도화가 이루어져 다양한 분야에서 활용되고 있는 현 시점에서, MLOps 라는 방법론 한번 슬쩍 접하는 것이 아닌 필수 사항이 되어가고 있다. MLOps는 현재 명확하게 정립되지 않아 논문이 아닌 수많은 블로그 포스트들로 다루어지고 있고, 또 수많은 도구들이 나타나 서로가 편리하다고 주장을 하고 있는 상황이다.","title":"[KR] 책 리뷰 : MLOps 도입가이드"},{"content":"오래간만에 다시 글또 활동을 시작하며 몇 개월 만에 새로운 글또 기수가 시작이 되었다. 이번 기수에도 크게 고민하지 않고 신청을 했는데, 지난 3번의 글또 활동을 시작할 때와 달리, 이번에는 시작을 앞둔 각오가 조금은 다른 것을 느끼고 있다.\n이번에는 너무 잘 하려고 하지 않으려고 한다. 과거에는 회사에서의 일, 퇴근 후 개인 공부, 글또 등등 성장과 관련된 부분들은 전부 다 잡고 싶고 잘 하려고 했던 것 같다. 의욕만 과다했던 탓일까, 때로는 번아웃이 오기도 했고, 스스로 생각해도 퀄리티가 높지 않았지만 데드라인에 맞춰서 그냥 낸 적도 있다. 회고 시점이 되어 돌아보면 열심히 하지 않은 것 같아서 만족스럽지 않았다. 의욕만 과다했던게 독이 되지 않았나 되돌아본 계기가 되었다.\n의욕과 스트레스만 가득했던 2019-2021년 이 지나 2022년에는 어느 정도 좀 내려놓고, 느린 속도여도 꾸준히 한다는 마음 가짐을 유지 하고 있다. 회사일, 대학원, 글또, 한빛미디어서평단 등 \u0026hellip; 나중에 돌아봤을 때, 그래도 아예 내려놓지 않고 꾸준했다 라고만 느껴도 성공한 한 해가 되지 않을까 싶다.\n이번 기수에서 다루고자 하는 주제 1~2달 이후 본격적으로 머신러닝 파이프라인 구축 업무를 하게 될 예정인데, 여러 적용 케이스를 참고하고 우리 팀의 요구 사항을 정리해서, 어떻게 구축할 것인지 고민한 흔적을 정리할 생각이다. 변성윤님의 발표자료에서 MLOps 춘추전국시대라고 묘사되었 듯 너무나 많은 도구와 각기 다른 상황에 따른 적용 방식이 있기에, 우리에게 맞는 방법을 초기에 고민해서 정립하는 것이 중요할 것 같다. 다양한 툴과 프레임워크에 대한 글, 이러한 고민을 토이프로젝트를 하면서 연습하는 과정을 포스트로 정리할 생각이다.\n다른 쪽으로는 팀 문화, 팀원 관리, 제도 등과 관련된 고민을 적어보고 싶다. 현재 회사에서 공식적인 매니저는 아니지만, 팀이 워낙 작은 관계로 각 멤버가 일정 부분에서는 매니저와 같은 역할을 나눠서 하고 있다. 앞으로 회사가 성장하면서 팀원이 더 생길 것으로 예상하는데, 나는 내가 겪었던 \u0026ldquo;체계가 없음\u0026rdquo;, \u0026ldquo;조언해줄 사람이 없음\u0026rdquo;, \u0026ldquo;참고할 게 없음\u0026rdquo;, \u0026ldquo;내 포지션의 고충을 아무도 이해 못 함\u0026rdquo;, \u0026ldquo;같이 고민할 사람 없음\u0026rdquo; 과 같은 상황을 새로 들어올 팀원에게 똑같이 물려주고 싶지 않다(그런 사람은 누구도 없을 것이다) 그렇다면 무엇이 마련되어 있어야 하는지, 어떤 문화가 정착되어야 하는지, 사례를 보고 책을 읽어서 정리해야 할 것 같다. 다행스럽게도, 작년에는 조직문화 전문가 팀원이 채용되었다. 회사의 기존 DNA도 있기에 그 속도가 드라마틱하지는 않지만, 덕분에 일반적인 기업 문화의 경우에는 긍정적으로 바뀌는 중이라고 생각한다. 나는 기술 조직의 성향이나 문화를 조금 고민해보고 싶다. (딱히 내게 주어진 업무는 아니긴 하다. 어쩌면 대학교 때 전공이 인사관리였기 때문에 그런 성향이 남아있는 것일지도 모르지만)\n어김없이 다짐글을 마무리하며 이번에는 너무 구체적이고, 원대한 계획을 세우지 않았다. 지난 시간을 돌이켜봤을 때, 그게 도움보다는 부담과 스트레스를 줬고 이대로 계속하다간 정신건강의원을 갈지도 모르겠다는 생각이 들었다.(정신건강의원을 가는게 안 좋다는 건 아니지만!!) 그래서 이번에는 내가 퇴근하면 운동도 즐기고, 책도 많이 읽으면서 좀 느슨하게 해보려고 한다. 이렇게 보낸 5개월은 어떨까. 나는 2022년을 어떻게 회고하게 될까.\n","permalink":"https://wonyoungseo.github.io/posts/2022-05-14-init-geultto-7/","summary":"오래간만에 다시 글또 활동을 시작하며 몇 개월 만에 새로운 글또 기수가 시작이 되었다. 이번 기수에도 크게 고민하지 않고 신청을 했는데, 지난 3번의 글또 활동을 시작할 때와 달리, 이번에는 시작을 앞둔 각오가 조금은 다른 것을 느끼고 있다.\n이번에는 너무 잘 하려고 하지 않으려고 한다. 과거에는 회사에서의 일, 퇴근 후 개인 공부, 글또 등등 성장과 관련된 부분들은 전부 다 잡고 싶고 잘 하려고 했던 것 같다. 의욕만 과다했던 탓일까, 때로는 번아웃이 오기도 했고, 스스로 생각해도 퀄리티가 높지 않았지만 데드라인에 맞춰서 그냥 낸 적도 있다.","title":"[KR] 글또 7기를 시작하며"},{"content":"   머신 러닝 파워드 애플리케이션 (저자: 파노스 알렉소풀로스)\n  추천하는 대상:\n 데이터 모델을 개발하거나 체계를 구축하는 업무를 하시는 분  한줄평: 데이터는 금가루가 박혀있는 원석. 시맨틱 데이터 모델링은 원석을 정제해서 금가루를 모아 금괴로 만드는 기술.\n시맨틱 데이터 모델링이란 데이터는 그냥 쌓아둔다고 좋은 것이 아니다. 데이터는 사용이 가능하도록 분류하는 과정을 거쳐 저장되어야 적절히 사용할 수 있다. 지극히 당연한 이야기지만, 데이터를 분류할 때 어떠한 의미를 부여할 것인지에 대해 생각해보자면, 막막할 때가 많다.\n그렇기에 시멘틱 데이터 모형화 방법론을 적용하여 데이터에 의미를 부여하는 것이 하나의 방법이 될 수 있다.\n 시맨틱 데이터 모형화란 인간과 컴퓨터 시스템에서 모두 명료하고 정확하며 일반적으로 이해되는 방식으로 데이터 표현을 개발하는 일이라고 정의할 수 있다.\n 이러한 방식으로 데이터를 표현하게 되었을 때 우리는 전산에서 데이터를 활용하며, 동시에 데이터를 공통된 의미로 정의하여 다른 사람들도 이해할 수 있어 효율적이고 관리가 용이하다.\n따라서 시멘틱 데이터 모형화 기술은 우리가 흔히 접할 수 있는 E-R 모형(entity-relationship model)이나 메타데이터 뿐만 아니라, 어떠한 분야의 지식 체계를 정리하는 온톨로지(ontology), 택소노미(taxonomy) 등에도 모두 적용된다.\n시맨틱 모형과 온톨로지? 텍소노미? 온톨로지와 텍소노미, 분류체계라는 정의는 매우 생소하다. 사전적 정의를 참고하자면 다음과 같다.\n온톨로지?\n 온톨로지(Ontology)란 사람들이 세상에 대하여 보고 듣고 느끼고 생각하는 것에 대하여 서로 간의 토론을 통하여 합의를 이룬 바를, 개념적이고 컴퓨터에서 다룰 수 있는 형태로 표현한 모델로, 개념의 타입이나 사용상의 제약조건들을 명시적으로 정의한 기술이다. 온톨로지는 일종의 지식표현(knowledge representation)으로, 컴퓨터는 온톨로지로 표현된 개념을 이해하고 지식처리를 할 수 있게 된다. 프로그램과 인간이 지식을 공유하는데 도움을 주기 위한 온톨로지는, 정보시스템의 대상이 되는 자원의 개념을 명확하게 정의하고 상세하게 기술하여 보다 정확한 정보를 찾을 수 있도록 하는데 목적이 있다. [위키피디아]\n 택소노미?\n 가나다, … ABC, …와 같이 표준화되고 체계적으로 분류된 전통적인 분류학 기반의 분류 체계. 트리형의 위계적 구조로서 이미 결정된 체계를 가지고 있다는 특징이 있다. 그리스어로 ‘분류하다’라는 ‘tassein’과 ‘법, 과학’이라는 ‘nomos’의 합성어로 사람들에 의해 이해되는 관계를 기준으로 분류되는 폭소노미(folksonomy)에 대비되는 용어이다. [네이버 지식백과]\n 온톨로지는 어떠한 개념이나 타입 등의 지식에 의미를 부여하여 정보시스템에서도 유용하게 활용할 수 있도록 의미를 명확하고 상세하게 부여하는 기법이라면, 택소노미는 표준화된 분류체계이다. 즉, 이러한 기술들의 공통된 점은 데이터의 의미를 명시하는 것이다.\n택소노미의 예시\n2022년 2월 3일 유럽연합은 “그린 택소노미(Green Taxonomy)”의 최종안을 발표했다. 그린 택소노미는 ‘환경적으로 지속가능한 경제 활동’의 범위를 정한 분류체계로, 어떠한 산업, 어떠한 경제활동이 탄소중립에 기여하고 환경적으로 지속가능한 것인지를 명시한다. 산업과 경제활동이란 것이 매우 광범위 하기 때문에, 이를 아주 세세하고 또 어떠한 편법과 잘못된 해석이 발생하지 않도록 치밀하게 정의 및 분류된 것을 알 수 있다.\n시멘틱 데이터 모형화 과정에서 유의해야 할 점 분류체계를 작성하는 일은 우리가 상상하는 것보다 훨씬 더 전문적인 접근 방법이 존재한다. 그저 한 개인이 자신의 배경지식과 경력을 가지고 만들어 나갈 수 있는 것이 아니다. 또한 잘못된 분류체계와 의미는, 시간이 지나면서 오류가 발각되고 지속적인 수정이 필요하게 되기 때문에, 초기 개발과 구축 과정 신중함을 요구한다.\n함정 이 책에서 제시하는 시맨틱 모형 개발 과정에서의 함정에는 어떤 유형이 있을까?\n 나쁜 설명 - 잘못된 이름과 의미 부여, 또는 정의 생략 잘못된 규격과 잘못된 지식 공급원, 획득방법, 지식 나쁜 품질 관리 잘못된 활용 나쁜 전략과 나쁜 조직 \u0026hellip;  딜레마 실무에서 어떠한 지식 체계를 정리할 때 자주 발생할 수 있는 딜레마에 대한 내용도 다루고 있다.\n 표현의 딜레마 표현과 내용 간의 관계에 대한 딜레마 모형 개선 방향, 모형 관리에 대한 딜레마  마무리하며 시멘틱 데이터 모형화 는 데이터를 분류하는 방법론에 있어 생각보다 훨씬 전문적인이고 상세하게 내용을 다루고 있다. 입문서는 아니라고 생각되며, 실제로 데이터의 체계를 잡아가는 일을 하시는 분들이라면, 지침서로 삼기 아주 좋은 책이다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-04-24-book-review-semantic-data-modeling/","summary":"머신 러닝 파워드 애플리케이션 (저자: 파노스 알렉소풀로스)\n  추천하는 대상:\n 데이터 모델을 개발하거나 체계를 구축하는 업무를 하시는 분  한줄평: 데이터는 금가루가 박혀있는 원석. 시맨틱 데이터 모델링은 원석을 정제해서 금가루를 모아 금괴로 만드는 기술.\n시맨틱 데이터 모델링이란 데이터는 그냥 쌓아둔다고 좋은 것이 아니다. 데이터는 사용이 가능하도록 분류하는 과정을 거쳐 저장되어야 적절히 사용할 수 있다. 지극히 당연한 이야기지만, 데이터를 분류할 때 어떠한 의미를 부여할 것인지에 대해 생각해보자면, 막막할 때가 많다.","title":"[KR] 책 리뷰 : 시멘틱 데이터 모형화"},{"content":"   머신 러닝 파워드 애플리케이션 (저자: 에마뉘엘 아메장)\n  추천 대상:\n 머신러닝이 적용된 제품을 만들고자 하는 분 머신러닝 엔지니어로 취업/이직하고자 하는 분  한줄평: 데이터 수집, 전처리, 학습 \u0026hellip; 이제 이걸 가지고 뭘 하지?\n머신러닝 모델은 서비스가 될 때 진정한 가치가 있다 “머신러닝에 대해 배운다” 라고 하면 대부분의 커리큘럼은 갖가지 알고리즘과 그 작동 방식에 대해 배우는 것부터 시작한다. 가장 간단한 형태의 모델부터 최신의 복잡한 구조의 모델까지 훑고, 토이 데이터셋으로 실습까지 진행한다. 머신러닝이라는 기술을 구현하기 위한 대략적인 지식을 얻게 되었다면, 그 다음에는 이 질문이 떠올라야 한다.\n “그럼 이제 이걸 가지고 뭘 하면 되지?”\n 머신러닝, 딥러닝, 인공지능이라는 신박한 기술이 소개되어 사람들을 매료시키던 시대는 이제 지났다고 할 수 있다. 기술은 실제로 사용이 되어야 그 가치가 있는 것과 마찬가지로 머신러닝 모델 또한 사용자에게 서비스가 될 때 진정한 가치가 있다. 우리는 지금 이 기술을 활용한 제대로 된 제품을 만들어 내야 하는 시기에 있다.\n머신러닝 파워드 애플리케이션 은 책 제목 그대로 머신러닝 기술에 기반한 제품(서비스)를 만들기 위한 내용을 담고 있다. 따라서, 현업에서 머신러닝 기술을 연구하는 단계가 아닌, 실제로 적용한 비즈니스 진행시켜야 하거나, 머신러닝 엔지니어로서 취업/이직을 고려하는 사람에게 적합하다. 단, 이 책은 머신러닝에 대한 기본적인 개념이 잡혀 있다는 것을 전제로 하고 있기 때문에, 초심자에게는 권하지 않는다. (머신러닝 개발자와 커뮤니케이션이 자주 발생하는 기획자는 업무에 참고할 만 하다.)\n머신러닝 제품화 길잡이 머신러닝 파워드 애플리케이션 은 머신러닝 기반 제품을 만들기 위한 아주 좋은 가이드북이다. 책에서 참고할 수 있는 내용들은 다음과 같다.\n1. 문제를 해결하기 위한 머신러닝 모색  문제를 해결하고 편의를 제공하기 위한 어플리케이션 정의 문제를 어떻게 정의하고, 적합한 모델을 선정하여 필요한 데이터를 준비해야할 지 파악하는 과정 머신러닝 기반 제품 개발의 전체적인 프로젝트의 일정의 계획하는 과정     다양한 문제에 따른 접근 방법 예시\n  2. 가장 간단하게 만드는 엔드투엔드 파이프라인    간단하지만 핵심 기능을 하는 제품부터 만들어보자\n   가장 필요로 하는 서비스를 제공하기 위한 최소한의 기능을 하는 엔드투엔드(End-To-End) 파이프라인 설계 가장 간단한 파이프라인의 성능을 평가하는 다양한 시각 (모델 성능, 사용성, 비용, 속도 등)     학습과 추론 파이프라인 예시\n  3. 모델 학습과 디버깅    토이프로젝트와 실제 산업에서의 차이\n   모델 학습 과정  토이 프로젝트가 아닌 실제 제품 개발을 위한 데이터 준비 과정 반복적인 과정을 거친 모델 학습 과정   실제 서비스되는 모델의 정상적인 동작을 위한 테스트 과정  4. 모델 배포와 모니터링    스트리밍 배포 방식\n     배치 배포 방식\n   사용자가 실제로 머신러닝 모델을 사용하기 위한 다양한 배포 방식 배포 과정에서 생길 수 있는 다양한 문제점들 모델의 이상 작동을 포착하고, 데이터 분포를 감지하기 위한 모니터링의 중요성     데이터 변화에 따른 재학습 주기 예시\n     이상탐지를 통한 모델의 비정상적인 동작 포착 예시\n  마무리하며 이 책은 기본적으로 다양한 예시를 들어 머신러닝 기반 제품이 만들어지는 과정을 설명한다. 따라서 읽는 사람에 내용의 흐름이 약간은 혼란스러울 수 있다. 일반적인 내용만 파악하더라도 실제로 만들고자 하는 서비스를 구현하는데에는 충분히 참고할 수 있는 책이기 때문에, 예시와 예제 코드에 너무 집중할 필요는 없다고 생각한다.\n또한 위에서 서술한 바와 같이 머신러닝의 기초적인 내용은 다루지 않기 때문에, 어느 정도 개념이 잡힌 상태에서 읽는 것을 권장한다.\n+) 현업 CTO / 머신러닝 엔지니어 / 데이터 사이언티스트 들의 인터뷰는 피가 되고 살이 되는 조언들이 담겨있으니 꼭 읽어보자 !!\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-03-27-book-review-ml-powered-application/","summary":"머신 러닝 파워드 애플리케이션 (저자: 에마뉘엘 아메장)\n  추천 대상:\n 머신러닝이 적용된 제품을 만들고자 하는 분 머신러닝 엔지니어로 취업/이직하고자 하는 분  한줄평: 데이터 수집, 전처리, 학습 \u0026hellip; 이제 이걸 가지고 뭘 하지?\n머신러닝 모델은 서비스가 될 때 진정한 가치가 있다 “머신러닝에 대해 배운다” 라고 하면 대부분의 커리큘럼은 갖가지 알고리즘과 그 작동 방식에 대해 배우는 것부터 시작한다. 가장 간단한 형태의 모델부터 최신의 복잡한 구조의 모델까지 훑고, 토이 데이터셋으로 실습까지 진행한다.","title":"[KR] 책 리뷰 : 머신러닝 파워드 애플리케이션"},{"content":"   금융 전략을 위한 머신러닝 (저자: 하리옴 탓샛, 사힐 푸리, 브래드 루카보)\n   추천 대상: 금융업계에 종사하는 PM / 기획자 / 소프트웨어 개발자 한줄평: 무엇을 좋아할 지 몰라서 다 준비해봤어  금융 서비스를 위한 머신러닝? 2022년 현재 머신러닝과 데이터사이언스는 커머스, 소셜, 스포츠 등 수많은 분야에서 각자 필요한 영역을 찾아 적용되어지고 있다. 머신러닝을 직접적으로 개발에 관여하지 않는 부서의 사람들이 봤을 때는 크게 단순히 사람의 노동을 대체할 수 있는 자동화의 측면이거나, 혹은 더 나아가 인간의 역량으로는 쉽게 할 수 없던 작업을 할 수 있는 알고리즘을 만드는, 이렇게 2가지로 인식된다. 모두가 “인공지능\u0026quot;, “데이터 사이언스\u0026quot;, “머신러닝\u0026quot;, “딥러닝\u0026quot; 이라는 기법을 적용해서 혁신을 이루어내겠다는 열의를 불태우는 가운데, 가장 그 반응이 강한 분야 중 하나는 바로 금융 섹터이다.\n금융이라는 단어는 쉽게 다가올 수 있지만, 사실 그 아래에는 은행, 증권시장, 보험, 자산운용, 신용카드, 신용평가 등 수많은 세부 영역들과 해결하고자 하는 문제들이 존재한다. 더불어 그 어느 영역보다 나라의 규제가 많이 적용되는 분야이기도 하기 때문에, 마음대로 할 수 없는 부분이 많다. 따라서 금융 섹터에서는 머신러닝 전문가의 경우 금융 배경의 복잡한 도메인 지식과 규제에 대한 이해가 부족하고, 반대로 금융 도메인 지식이 풍부한 인력은 머신러닝에 대한 지식이 부족한 경우가 많다. (둘 다 잘 알면 서로 모셔가려는 슈퍼스타고 \u0026hellip; ) 그만큼 다른 분야 비해 머신러닝 전문가와 비전문가가 긴밀한 관계를 유지하며 함께 일해야 할 필요가 있는 분야가 바로 금융이다.\n금융 문제에 접근하기 위한 머신러닝 기법의 \u0026ldquo;간단\u0026quot;하고 최대한 \u0026ldquo;다양한\u0026rdquo; 설명 [금융 전략을 위한 머신러닝]에서는 다음과 같은 내용을 다룬다.\n 금융 분야에서 머신러닝이 적용될 수 있는 영역들  알고리즘 트레이딩, 로보어드바이저, 이상 거래 탐지, 대출 심사, 위험 관리, 돈세탁 방지, 감성 분석 등의 개념을 아주 간략하게 소개한다.   통상적으로 머신러닝 프로젝트를 위해 가장 많이 쓰이는 파이썬과 관련 프레임워크  파이썬과 머신러닝을 개발하는 단계를 간략히 설명한다.   금융 문제에 접근하기 위한 머신러닝 기법의 소개  머신러닝의 큰 줄기와 알고리즘에 대해 설명한다.  지도학습: 회귀와 분류, 시계열 모델 비지도학습: 차원축소, 클러스터링 강화학습: 강화학습 자연어처리(NLP)에 대한 소개      또한 이 책에서는 간단한 데이터셋을 통해 실제 프로젝트와 비슷한 환경에서 어떠한 머신러닝 기법을 적용할 수 있는지 예제를 제공한다.\n머신러닝 관련 금융 프로젝트를 이끌어가야 하는 분들을 위한 입문서 저자는 [금융 전략을 위한 머신러닝]이 관련업종에서 종사하는 데이터 사이언티스트, 데이터 엔지니어, 머신러닝 설계자, 퀀트 연구원 등의 직군에 적합하다고 했지만, 단도직입적으로 머신러닝 연구자나 엔지니어를 위한 책은 아니다. 실제로 데이터 관련 직군 종사자들은 머신러닝 알고리즘의 개념과 프레임워크 사용법, 코드는 이미 숙지하고 있을 것이다. 또한 이 책에서 제공하는 예제가 실제 현업의 프로젝트와 난이도는 분명히 차이가 있으므로, 그대로 활용하는 것은 무리가 있을 것이다.\n반대로 내가 오히려 추천하는 대상은 금융 분야에 종사하는 관리자, PM, 기획자 또는 소프트웨어 엔지니어이다. 머신러닝으로 금융 도메인의 문제에 접근하는 측면에서 훌륭한 입문서로 활용하여, 이 책이 제공하는 기본적인 이해를 바탕으로, 데이터 관련 직군의 동료와 함께 프로젝트를 이끌어나갈 수 있는 기반을 쌓을 수 있을 것이라 생각한다.\n 한빛미디어 \u0026lt;나는 리뷰어다\u0026gt; 활동을 위해서 책을 제공받아 작성된 서평입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-02-20-book-review-ml-ds-blueprint-finance/","summary":"금융 전략을 위한 머신러닝 (저자: 하리옴 탓샛, 사힐 푸리, 브래드 루카보)\n   추천 대상: 금융업계에 종사하는 PM / 기획자 / 소프트웨어 개발자 한줄평: 무엇을 좋아할 지 몰라서 다 준비해봤어  금융 서비스를 위한 머신러닝? 2022년 현재 머신러닝과 데이터사이언스는 커머스, 소셜, 스포츠 등 수많은 분야에서 각자 필요한 영역을 찾아 적용되어지고 있다. 머신러닝을 직접적으로 개발에 관여하지 않는 부서의 사람들이 봤을 때는 크게 단순히 사람의 노동을 대체할 수 있는 자동화의 측면이거나, 혹은 더 나아가 인간의 역량으로는 쉽게 할 수 없던 작업을 할 수 있는 알고리즘을 만드는, 이렇게 2가지로 인식된다.","title":"[KR] 책 리뷰 : 금융 전략을 위한 머신러닝"},{"content":"20년 당시 회고글을 돌아보니, 키워드 별로 정리해놓은 포맷이 나쁘지 않은 것 같아 이번에도 같은 포맷으로 회고를 해보고자 합니다.\n\u0026amp;nbsp\n올해의 키워드 1. 올해의 고난 (자세히 기술하지는 못 합니다) 회사에서 기존에 맡고 있던 업무의 비즈니스 모델(BM)이 올해 중반을 기점으로 전혀 예상하지 못 했던 방향으로 계획을 변경되면서 개인적으로는 하반기가 참 힘이 들었습니다. 모든 팀원들이 한번도 경험해보지 못 한 “처음 해보는 일”을 짧은 시간 내 해내야하는 프로젝트가 시작이 되었습니다. 이로 인해 기존에 계획되었던 것보다 더 급하게 자회사가 설립되고, 짧은 시간동안 많은 외부 인사 영입이 이루어졌습니다. 제 역할도 많이 붕 떠버렸고, 제 분야가 아님에도, 현재 할 수 있는 인력이 없기 때문에 떠앉아버린 업무가 많아졌습니다. 해당 업무를 전문성을 가지고 해나갈 수 있는 인력의 채용 계획이 있는지도 확인되지 않았습니다. 그런 와중에 첫번 째 마일스톤은 2022년 2월, 그 다음 마일스톤은 2022년 5월으로 잡혀 있구요. 체력, 감정 소모에 이어 번아웃이 몇 차례 왔었는데요, 그래서 그런지 다른 사람들처럼 활기차게 2022년! 을 맞이할 수 없는 것 같습니다. 어떻게 하면 슬기롭게 헤쳐나갈 수 있을까요? 2022년 1분기부터 고민이 많아집니다.\n\u0026amp;nbsp\n2. 올해의 가장 큰 변화 그럼에도 불구하고 2021년 가장 긍정적으로 받아들이는 변화는, 바로 같은 방향성을 가진 동료가 생겼다는 것입니다. 작년 하반기에 합류한 동료 S와 올해 중순부터 업무적인 대화를 많이 하기 시작했고, 팀과 회사의 프로덕트에 대한 고민을 현실적으로 의논할 수 있었습니다. 문제가 있다면 그 문제가 실제로 있음을 인정하고 구체적인 해결방안을 논의할 수 있는, 그런 제 상식에 부합하는 동료가 생겼다는 것은 여태까지 없던 아주 긍정적인 변화입니다.\n2021년에는 동료 S와 함께 주도적으로 나서서 우리 팀만의 시스템을 갖추기 시작했습니다. 기존에 문제라고 생각했던 “내용이 중구난방인 미팅”과 “목적 없는 모이는 미팅”을 없애기로 했습니다. 미팅에서 다루는 내용을 세분화하여 서로 업무를 공유하기 위한 스탠드업 미팅, 기술적인 내용을 공유하기 위한 미팅, 비즈니스팀과 커뮤니케이션 할 미팅을 정의했습니다. 주기적인 미팅이 각각의 명확한 목적을 띄면서, 업무 공유가 훨씬 투명하게 이루어지고, 자연스럽게 서로 피드백 해줄 수 있게 되었습니다. 이전보다 일정 관리가 수월해지고, 업무에 대한 토론도 원활해진 것 또한 큰 수확이라고 생각됩니다.\n이렇게 작은 부분부터 함께 맞춰보았고, 이제 2022년에는 팀의 비전과 운영 방식, 인력 구성, 충원, 성장, 외부 프로모션 등을 어떻게 해나갈지 함께 고민하고 있습니다. 정답은 없지만 함께 논의하면서 최선의 방식을 추구하는 이 일련의 과정이 꽤 긍정적이었습니다. 역량도 뛰어나고 경험도 있기 때문에 늘 배우는 점이 많아서, 저도 민폐 끼치지 않는 좋은 동료가 되고 싶다는 동기부여도 되고 있습니다. 그래서 참 감사하고 2022년에 어떤 일을 같이 해나갈지 기대가 되는 부분입니다.\n\u0026amp;nbsp\n3. 올해의 큰 수확 올해 가장 큰 수확은 무엇보다 건강을 챙기는 습관을 들인 것과, 운동을 시작한 것입니다. 2021년 2월 말부터 집 바로 앞에 위치한 필라테스 센터에서 1:1 운동을 시작했습니다. 직군 특성 상, 오래 앉아있으면서 자세가 흐트러지기 쉽고, 체력도 점점 안 좋아지는 것을 느껴서 운동을 꼭 해야겠다고 느끼는 와중에 필라테스에 도전해보기로 했습니다. 필라테스는 그 무엇보다 코어에 많은 포커스를 맞추는 운동입니다. 코어 운동을 많이 하면서 자연스럽게 자세가 덜 흐트러지고, 체력이 많이 늘었습니다. 무엇보다 제대로된 좋은 선생님을 만났다는 것이 정말 다행이기도 하구요. 운동 할 때마다 너무 힘들어서 선생님 참 미운데요 \u0026hellip; 그러니까 좋은 선생님이겠죠 \u0026hellip; ?\n그리고 2021년 10월부터는 테니스를 다시 시작하게 되었습니다. 집 근처 안양천에 테니스장이 개장하면서, 주 3회 아침 7시에 레슨을 받고, 주말에도 종종 다른 분들과도 테니스를 치고 있습니다. 테니스는 중3 때 처음 조금 배우고, 그 뒤로는 근처에 코트가 없어서 또는 같이 칠 사람이 없어서, 할 수 없었는데요, 이번에 기회가 생겨서 다시 열심히 치고 있습니다. 테니스는 신체 접촉이 없어서 코로나 시대에 많은 인기를 얻고 있는 운동이기도 하고, 테니스공을 치면서 스트레스도 해소할 수 있어서 최근에 멘탈 관리에도 큰 도움이 되고 있습니다. 주변에도 테니스 치는 사람들이 많이 생겼으면 좋겠어요. 이번에 테니스를 치는 인스타그램 계정도 새로 팠어요. 함께 테니스 칠 분을 찾습니다. 많관부 !! 꼭 연락 주세요 :)\n2022년에 기대하는 것 MLOps MLOps에 대한 공부를 따로 하고 있습니다. 2021년에는 MLOps에 대한 철학과 다양한 use case 에 대해 알아보았다면, 2022년에는 우리 회사에 어떤 부분에 어떻게 MLOps가 필요할 지에 대한 본격적인 고민과 이를 실제로 적용해보게 될 것입니다. 매 순간 순간이 새로운 의사결정이 될 텐데요, MLOps에 대한 배경지식이 없는 팀원들에게 이를 어떻게 이해시키고 커뮤니케이션 할 지에 대해서도 정리를 해야겠습니다.\n글또 글또의 다음 기수 참여도 벌써부터 기대하고 있습니다. 이번에는 MLOps와 도서 리뷰, 그리고 팀운영에 대한 글을 써보게 될 것 같습니다. 그리고 회사에서 우리 팀 자체적으로도 대외 홍보를 위한 방법 중 하나로 컨퍼런스 참여와 기술 블로그 운영을 고민하고 있는데요, 글또를 통해 글을 작성한 경험과, 글또에서 많은 분들이 보여주시는 훌륭한 포스팅을 참고해서 기술 블로그를 런칭할 계획에 있습니다.\n마무리하며 2021년을 되돌아보면 혼돈 속에 시간만 빨리 갔습니다. 그 와중에 하는 일 속에서 의미를 찾으려고 고군분투 했던 해라고 자평해보게 되네요. 그렇지만 2022년에는 뚜렷한 동기부여가 생겼고, 앞으로 잘 풀릴 거라는 기대가 있습니다. 무엇보다 2022년을 회고할 때는 어떤 분위기일까요, 그때 다시 한번 이 글을 꺼내보아야겠습니다.\n","permalink":"https://wonyoungseo.github.io/posts/2022-01-01-review-2021/","summary":"20년 당시 회고글을 돌아보니, 키워드 별로 정리해놓은 포맷이 나쁘지 않은 것 같아 이번에도 같은 포맷으로 회고를 해보고자 합니다.\n\u0026amp;nbsp\n올해의 키워드 1. 올해의 고난 (자세히 기술하지는 못 합니다) 회사에서 기존에 맡고 있던 업무의 비즈니스 모델(BM)이 올해 중반을 기점으로 전혀 예상하지 못 했던 방향으로 계획을 변경되면서 개인적으로는 하반기가 참 힘이 들었습니다. 모든 팀원들이 한번도 경험해보지 못 한 “처음 해보는 일”을 짧은 시간 내 해내야하는 프로젝트가 시작이 되었습니다. 이로 인해 기존에 계획되었던 것보다 더 급하게 자회사가 설립되고, 짧은 시간동안 많은 외부 인사 영입이 이루어졌습니다.","title":"[KR] 2021년이 지났다."},{"content":"   빅데이터 시대, 성과를 이끌어내는 데이터 문해력 (저자: 카시와기 요시키)\n   추천 대상: \u0026ldquo;데이터를 들여다보면 되지 않을까?\u0026rdquo; 하는 분들 한줄평: 데이터에는 답이 없다고? 데이터에서 답을 찾으려던거 아니었어?  데이터 문해력 Data Literacy 데이터, 2010년대 후반 들어 가장 많이 언급되는 단어가 아닐까 합니다. 이런 트렌드에 힘입어 데이터 문해력(Data literacy) 또한 많은 사람들이 관심을 가지고 있는 키워드라고 할 수 있습니다.\n데이터 문해력은 어떻게 정의 내릴 수 있을까요? \u0026ldquo;데이터가 쏟아져 흘러 넘치는 빅데이터 시대에 데이터를 읽고 무언가를 해석하는 능력\u0026quot;이라고 하면 알맞은 정답이 될 수 있을까요?\n이 책의 저자는 이 책을 통해서 데이터 리터러시란, \u0026ldquo;문제 해결 방법에 대해 스스로 정답을 고민하며, 데이터를 활용해 논리적으로 이를 풀어내는 능력\u0026rdquo; 이라 자신의 생각을 전합니다. 그리고 \u0026ldquo;어딘가에 있을 정답을 찾으러 간다\u0026rdquo; 는 식의 접근 방법은 학교 시험에나 해당되는 말과 함께 말이죠. (아 뼈 아퍼)\n   데이터 보기 전에 생각했나요?\n  저자는 이렇 듯, 온화한 화법으로 쉬지 않고 독자에게 질문을 던집니다. 그 질문 하나하나가 팩폭이라 정신이 번쩍 들게 하는데요, 데이터 직군 뿐만 아니라 데이터를 접하는 마케터, 기획자, 애널리스트 등 많은 분들께 유용한 지침들로 가득하다는 생각이 들었습니다.\n책을 읽으며 개인적으로 와닿았던 몇가지 내용을 아래와 같이 정리해봤습니다. 중간중간 내용이 뜨끔했다면 여러분도 책을 직접 읽어보시는 것을 강력 추천합니다!\n데이터를 다룬 다는 것은 무엇을 의미할까?  \u0026ldquo;점점 편리해지는 기계와 도구들이 많아져 사용법과 조작법만 익히면 전부 알아서 해주고 원하는 것까지 손에 넣을 수 있는가라고 묻는다면 그 대답은 아쉽지만 NO입니다.\u0026quot;\n 최근 들어 데이터 분석, 인공지능이 유망한 분야로 떠오르게 되면서, \u0026ldquo;데이터 분석을 배우고 싶습니다, 파이썬을 해야하나요? R을 해야 하나요? SQL은 어느 수준까지 해야 하나요?\u0026rdquo; 와 비슷한 질문들을 많이 접하게 됩니다. 그리고 많은 사람들이 데이터를 다루는 기술이 곧 데이터를 분석하는 행위라고 생각하는 모습을 보게 돼죠. 물론 기술을 익히는 것도 중요합니다. 하지만 이러한 기술은 \u0026ldquo;컴퓨터가 인간보다 빠르고 실수 없이 잘 하는 작업들을 컴퓨터에게 시키기 위한 것\u0026quot;에 목적을 두고 있으며, 도구에 지나지 않습니다.\n그렇다면 도구를 활용하는 방법만 숙달했을 때 어떤 일들이 벌어지게 될까요? 아마 아래와 같은 단계를 밟게 될 것 입니다.\n1. 데이터를 활용해서 무언가를 해보고 싶다. 데이터로 성과를 내고 싶어 ... 2. 오 눈 앞에 데이터가 있네! 그리고 난 데이터 분석 도구를 사용할 줄 알아. 3. 그럼 일단 통계값부터 내보자. 4. 그리고 시각화를 해보자. 5. 통계값을 낸 결과와 시각화를 한 결과는 이러하다. 6. 난 데이터를 분석했어! 이렇게 당장 눈 앞에 놓인 데이터를 가지고 이리저리 통계값을 내고 그래프를 만들고 ~가 증가했다, ~가 ~보다 크다 와 같은 해석을 하게 되겠죠. 하지만 여기까지가 한계입니다\u0026hellip; \u0026ldquo;그래서 뭐?\u0026quot; 라는 추궁을 받게 되면 남은 건 내거친생각과불안한눈길과그걸지켜보는 흔들리는 동공 뿐입니다.\n   (저렇게 모니터 보면 목이랑 허리 나가요 \u0026hellip; )\n  반면, 저자가 제시하는 방식은 좀 다릅니다.\n1. 내가 알고 싶은 것은 무엇인지 2. 어떤 문제를 해결하고 싶은지, 앞서 알게 된 것을 토대로 무엇을 하고 싶은지 3. 어떤 데이터를 봐야 하는지 4. 어떤 지표를 적용할 지 즉, 저자는 어떠한 문제를 해결하기 위해 어떤 목적을 가지고 어떠한 데이터를 사용할 지 설계해나가고, 결과를 토대로 의사결정을 해나가는 데이터를 기반한 사고력(critical thinking)을 갖추는 것이 핵심이라고 강조합니다.\n데이터를 제대로 활용하지 못 하는 이유? 한편, 데이터를 보유하고 있고 데이터 분석을 진행했음에도 불구하고, 시원한 개선이 없다거나 제대로 활용하지 못 했다는 찜찜함이 남는 경우가 있습니다. 왜 그런 것일까요?\n풀고자 하는 문제가 명확하지 않다 문제를 제대로 정의하지 않은 상태에서 당장 눈 앞에 있는 데이터에 달려들어버리면 결과가 좋을 수 없습니다. 우리는 데이터를 들여다보기에 앞서 문제를 풀고자 하는 목적, 문제, 원인, 해결 방안을 구분해서, 구체적인 언어로 정의 해야 합니다.\n저 또한 문제의 윤곽만 어렴풋이 보이는 상태에서 데이터만 탐색했던 적이 떠올라서 뼈를 맞은 듯한 기분을 지울 수가 없네요. 최근에 들어서야 문제를 명확히 정의한다 라는 것의 의미를 알고 이행하려고 하고 있습니다.\n정의한 문제와 사용하는 데이터가 일치하지 않는다 문제와 목적이 정해지면, 이를 객관적으로 측정하기 위한 올바른 지표가 설정되어야 하고, 우리가 보는 데이터가 이 지표를 통해 설명이 되어야 합니다.\n예를 들어, 우리가 측정하고 하는 것은 시간인데, 우리가 사용하는 데이터는 무게와 거리, 사람의 수 등 엇나간 데이터라면 문제를 활용하기 위한 적절한 도구가 될 수 없겠죠. 다시 말하지만, 데이터가 무게와 거리, 사람의 수가 있기 때문에 지표를 무게, 거리, 사람의 수로 정하는 것이 아닙니다. 우리가 정의한 문제에서 시간을 보고자 한다면, 우리가 사용하는 데이터 또한 시간을 지표로 측정할 수 있어야 합니다. 일의 순서를 혼동하지 맙시다!\n결과가 나왔다. 이제 무엇을 할 것인가?  \u0026ldquo;조직에서 데이터를 잘 활용하고 있다는 것은 그 정보를 통해 문제 방안을 수립하거나 구체적인 행동계획을 세우거나 관계자들이 납득할 만한 합의 또는 판단을 내릴 수 있다는 이야기입니다. 즉, 결론으로 유도하지 못하는 정보는 가치가 별로 없으며, 목적에 이르지 못한 어중간한 상태입니다. 그런데 실제로 이 상태를 끝으로 데이터를 활용했다고 말하는 경우가 압도적으로 많습니다.\u0026quot;\n 혹시 데이터 정리해서 현황만 파악 해놓고 데이터를 활용했다고 하진 않았나요? (네 그게 바로 접니다) 저자가 뼈를 때렸 듯, 데이터를 도구 삼아 문제를 해결하기 위한 계획(action plan)을 수립했을 때 우리는 비로소 데이터를 활용했다 말 할 수 있을 것입니다.\n다만, 현재 상태를 안다고 해서 바로 해결 방안을 찾거나 조치를 취하는 단계로 넘어가는 것을 주의해야 합니다. 저자는 데이터 정리를 통해 파악된 현재 상태에 대한 원인을 집요하게 파고드는 훈련을 잊지 말아야 하며, 만약 해결 방안이 성과를 내지 못 한다면 원인을 제대로 짚어내지 못 한 것은 아닌지를 검토해 볼 것을 강조합니다.\n    \u0026ldquo;원인은 항상 단순한 구조가 아닙니다. 가급적 \u0026lsquo;어째서\u0026rsquo;, \u0026lsquo;왜\u0026rsquo;를 반복해서 더욱 \u0026lsquo;본질적\u0026rsquo;인 원인까지 파고들어야 정확하고 밀도 있는 해결 방안을 도출할 수 있습니다. \u0026hellip; (중략) 원인이 명확히 규명된 후 수립하는 대책과 그런 과정 없이 즉흥적으로 만든 대책은 그 효과와 정확성에 엄청난 차이가 있습니다. 논리적인 흐름과 구조에 대해 생각하는 것에 비하면 구체적인 방법을 고민하는 것이 즐겁고 편하므로 이를 우선하기 쉽습니다. 하지만 \u0026lsquo;해결방안\u0026rsquo;을 고민하는 것은 마지막 단계라는 것을 언제나 염두에 두시기 바랍니다.\u0026quot;\n 마무리하며 막상 책의 내용을 다시 곱씹어보며 정리해보자니, 전부 지극히 당연하면서도 원론적인 이야기가 많았습니다. 그만큼 이전에는 기본에 충실하지 않았다는 뜻이 아닐까 되돌아보게 되는 계기가 되었네요.\n240페이지 밖에 안 되고 심지어 한 페이지에 글자 수도 많지 않은 얇은 도서임에도 불구하고 시종일관 뼈를 때리는 내용으로 가득해서(이미 남아나는 뼈가 없\u0026hellip;), 저는 당분간은 종종 일하면서 이 책을 가이드라인으로 삼으려고 합니다.\n","permalink":"https://wonyoungseo.github.io/posts/2021-11-21-book-review-data-literacy/","summary":"빅데이터 시대, 성과를 이끌어내는 데이터 문해력 (저자: 카시와기 요시키)\n   추천 대상: \u0026ldquo;데이터를 들여다보면 되지 않을까?\u0026rdquo; 하는 분들 한줄평: 데이터에는 답이 없다고? 데이터에서 답을 찾으려던거 아니었어?  데이터 문해력 Data Literacy 데이터, 2010년대 후반 들어 가장 많이 언급되는 단어가 아닐까 합니다. 이런 트렌드에 힘입어 데이터 문해력(Data literacy) 또한 많은 사람들이 관심을 가지고 있는 키워드라고 할 수 있습니다.\n데이터 문해력은 어떻게 정의 내릴 수 있을까요? \u0026ldquo;데이터가 쏟아져 흘러 넘치는 빅데이터 시대에 데이터를 읽고 무언가를 해석하는 능력\u0026quot;이라고 하면 알맞은 정답이 될 수 있을까요?","title":"[KR] 책 리뷰 : 빅데이터 시대, 성과를 이끌어내는 데이터 문해력"},{"content":"데이터 사이언스, 머신러닝 프로젝트를 수행하기 위해서는 다양한 파라미터를 실험하는 과정이 동반됩니다. 이번 글에서는 파라미터와 설정값을 간결하게 관리하고 사용할 수 있게 도와주는 Hydra에 대해 알아보도록 하겠습니다.\n1. Hydra란 Hydra는 페이스북에서 오픈소스로 공개한 프레임워크로, 어플리케이션에서 사용하는 여러가지 설정값을 관리할 수 있는 기능을 제공합니다.\n직접 사용해본 Hydra는 아주 명확한 특징을 가지고 있습니다.\n 모든 설정 및 파라미터 값은 config.yaml로 관리하고 계층적으로 설정 그러한 와중에 command-line을 통해서 오버라이딩(overriding) 가능 한번의 명령어로 각각 다른 값을 대입하여 다중 실행 가능  이렇게 설명해도 와닿지 않을 수가 있겠죠. 그럴 때 저는 이렇게 설명합니다.\n \u0026ldquo;더 이상 실험 한번 돌릴 때마다 파라미터 값을 스프레드시트에 적어두지 않아도 돼\u0026rdquo;\n 2. Hydra 기본적인 사용 방법 2.1. 설치하기 pip install hydra-core --upgrade 2.2. Hydra로 Config 불러오기 2.2.1. Config 저장 형태 기본적으로 Hydra에서는 Yaml형태로 config값들을 저장합니다.\n예를 들어, 아래와 같은 파일 구조를 가지고 있다고 할 때,\n. ├── config │ └── config.yaml └── app.py config.yaml 파일에 파라미터를 정의해보았습니다.\n# ./config/config.yaml train_params: epoch: 5 batch_size: 10 learning_rate: 1e-4 2.2.2. Config 불러오기 실제로 config를 불러올 어플리케이션에서는 경로와 파일명을 전달하여 config를 불러옵니다. 이렇게 불러온 config의 데이터타입은 DictConfig 입니다.\n# ./app.py import hydra @hydra.main(config_path=\u0026#34;config\u0026#34;, config_name=\u0026#34;config\u0026#34;) def func(config): # access elements of the config print(type(config)) print(config) if __name__ == \u0026#34;__main__\u0026#34;: func() $ python app.py \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001}} Hydra는 ./output 디렉토리 아래 각 실행마다 결과 또는 로그의 출력을 저장합니다.\n. ├── app.py ├── config │ └── config.yaml └── outputs └── 2021-11-07 └── 22-11-07 └── app.log 3. Config값 오버라이드 하기 앞서 예시의 config.yaml 파일에서는 epoch를 5, batch_size는 10으로 설정했습니다. 그렇다면 다음 실험은 epoch를 2, batch_size는 10로 설정하고 싶다면 config 파일을 내용을 수정해야 할 까요?\nHydra는 command line에서 config 값에 대한 파라미터 오버라이팅을 지원하기 때문에, 파일의 내용을 수정할 필요가 없습니다.\n아래의 명령어를 보면 이해가 빠를 것 같은데요, command line에서 epoch값을 2로 오버라이드 한 실행 결과입니다.\n$ python app.py train_params.epoch=2 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 2, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001}} 애초에 설정하지 않았던 값을 +를 통해 추가할 수도 있습니다.\n$ python app.py train_params.epoch=2 +train_params.random_state=42 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 2, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;random_state\u0026#39;: 42}} +) Config Group 을 적용하면 더욱 다양하고 복잡한 형태의 config 오버라이딩이 가능합니다.\n4. Hydra를 통해 다중실행하기 Hydra에서는 --multiruns 인자를 통한 다중 수행을 할 수 있다는 것도 아주 유용합니다.\n$ python app.py --multirun train_params.learning_rate=0.0001,0.001,0.01,0.1 [2021-11-07 23:09:31,088][HYDRA] Launching 4 jobs locally [2021-11-07 23:09:31,088][HYDRA] #0 : train_params.learning_rate=0.0001 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.0001}} [2021-11-07 23:09:31,156][HYDRA] #1 : train_params.learning_rate=0.001 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.001}} [2021-11-07 23:09:31,230][HYDRA] #2 : train_params.learning_rate=0.01 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.01}} [2021-11-07 23:09:31,297][HYDRA] #3 : train_params.learning_rate=0.1 \u0026lt;class \u0026#39;omegaconf.dictconfig.DictConfig\u0026#39;\u0026gt; {\u0026#39;train_params\u0026#39;: {\u0026#39;epoch\u0026#39;: 5, \u0026#39;batch_size\u0026#39;: 10, \u0026#39;learning_rate\u0026#39;: 0.1}} +) Hydra는 기본적으로 다중실행을 순차적으로 수행하지만, 병렬 실행을 하고자 한다면 Joblib Launcher가 동반되어야 합니다.\n마무리하며 처음에는 데이터 모델을 실험할 때 config를 쉽게 관리할 수 있는 방법에 대해서 찾아보는 중에 Hydra를 발견하게 되었습니다. 하지만 사용법에 대해 알아보면서, 단순히 데이터사이언스와 머신러닝 뿐만 아니라 그 외 어플리케이션에도 다양하게 접목할 수 있는 유용한 프레임워크라는 생각이 드네요. 더욱 고도화해서 실제 프로젝트에 접목할 방법을 구상해봐야겠습니다.\nReference  https://hydra.cc/docs/intro https://pjt3591oo.github.io/hydra_translate/build/html/index.html  ","permalink":"https://wonyoungseo.github.io/posts/2021-11-07-hydra-for-machine-learning/","summary":"데이터 사이언스, 머신러닝 프로젝트를 수행하기 위해서는 다양한 파라미터를 실험하는 과정이 동반됩니다. 이번 글에서는 파라미터와 설정값을 간결하게 관리하고 사용할 수 있게 도와주는 Hydra에 대해 알아보도록 하겠습니다.\n1. Hydra란 Hydra는 페이스북에서 오픈소스로 공개한 프레임워크로, 어플리케이션에서 사용하는 여러가지 설정값을 관리할 수 있는 기능을 제공합니다.\n직접 사용해본 Hydra는 아주 명확한 특징을 가지고 있습니다.\n 모든 설정 및 파라미터 값은 config.yaml로 관리하고 계층적으로 설정 그러한 와중에 command-line을 통해서 오버라이딩(overriding) 가능 한번의 명령어로 각각 다른 값을 대입하여 다중 실행 가능  이렇게 설명해도 와닿지 않을 수가 있겠죠.","title":"[KR] Hydra를 활용해서 Config 관리 효과적으로 해보자"},{"content":"Streamlit은 파이썬 기반으로 웹어플리케이션을 개발할 수 있는 프레임워크입니다. 이번 포스팅에서는 Streamlit에서 제공하는 서비스를 통해 아주 쉽고 간단하게 내가 개발한 웹을 배포하는 방법에 대해 소개하고자 합니다.\n웹 어플리케이션 개발 \u0026hellip; 그 다음에는? 웹개발과 다소 거리가 먼 데이터분석가, 데이터 사이언티스트 분들에게 Streamlit은 분석 결과를 공유하거나, 학습한 모델을 제품으로 프로토타이핑 할 수 있는 아주 강력한 도구로 자리 잡았습니다.\n하지만 나 혼자만 보고 사용하면 무슨 소용일까요\u0026hellip;? (도발)\n웹어플리케이션 개발과 별개로 배포 또한 하나의 장벽으로 느껴질 수 있는데요, Streamlit Sharing 은 Streamlit 어플리케이션을 배포하기 위해 알아야 할 모든 지식과 과정을 극도로 간소화 했습니다. (이 정도로 간소화가 아닌 삭제라고 봐도 무방합니다.)\nStreamlit Sharing Streamlit Sharing은 Streamlit에서 자체적으로 제공하는 Streamlit 어플리케이션 배포, 관리 및 공유 플랫폼입니다.\n기본적으로 무료 계정의 경우 3개의 어플리케이션까지 배포할 수 있고, public 레포지토리(Github)만 배포가 가능합니다. 또한 하나의 어플리케이션 당 프로비저닝 되는 리소스는 RAM 1GB로 제한됩니다. (그외 소규모 팀단위 플랜과 엔터프라이즈 플랜이 옵션으로 있습니다. 더 자세한 사항은 링크 를 참고해주세요)\nStreamlit Sharing에서 웹어플리케이션을 배포하는 과정은 정말정말 단순하고 간단합니다.\n Streamlit Sharing에 가입하고 Github 계정을 연동 배포하고자 하는 Streamlit 어플리케이션을 설정  Github의 repo를 선택 브랜치를 선택 실행하고자 하는 어플리케이션 파일을 선택(app.py)   어플리케이션을 수정할 경우에는 선택한 브랜치에 git push 만 하면 변경된 사항이 반영  (참쉽죠1)\n하나씩\n1. 가입 및 Github 계정 연동  https://share.streamlit.io/에서 Sign in 합니다.     오른쪽 상단 Settings를 클릭 후 \u0026ldquo;Sign in with Github\u0026quot;을 통해 Github 계정을 연동합니다.     2. Github 레포지토리 연결    New app에서 배포하고자 하는 Streamlit 어플리케이션의 Github 레포지토리를 연결합니다.\n2.1. 레포지토리, 브랜치, 앱파일 선택 이번 예제에서 사용할 레포지토리 streamlit_app_stockprice_downloader의 파일 구조는 아래와 같습니다.(링크)\n. ├── README.md ├── app.py ├── prepare_data.py ├── requirements.txt ├── resource │ ├── stock_list.json │ └── stock_market.json └── utils ├── __init__.py ├── utils.py └── utils_fdr.py 아래와 같이 레포지토리와 브랜치, 그리고 실행하고자 하는 파일을 선택합니다.\n   2.2. (Optional) 파이썬 버전, 환경변수 설정하기 Advanced Settings를 통해 파이썬의 버전을 선택할 수 있고, 인증이나 DB 관련 민감한 정보를 TOML 포맷으로 하여 환경변수로 설정할 수 있습니다.\n   2.3. 배포하기 ! Deploy 버튼을 클릭하면, Streamlit Sharing에서 자동으로 어플리케이션을 위한 리소스를 할당하고, 연동된 Github 레포지토리 내 requirments.txt에서 정의된 라이브러리를 설치한 후, Streamlit 어플리케이션을 실행시킵니다. (참쉽죠2)\n      예제에서 실행된 웹어플리케이션의 링크에서 보실 수 있습니다.\n3. 배포 후 어플리케이션을 수정할 경우에는 별다른 과정 없이, 그저 처음에 설정한 브랜치에 git push 만 하면 Streamlit Sharing에서는 continous deployment를 할 수 있습니다. (참쉽죠3)\n마무리 하며 Streamlit Sharing이라는 플랫폼을 통해 간단히 Streamlit 기반 웹어플리케이션을 배포하는 과정에 대해 알아보았습니다.\n한 가지 아쉬운 점은 어플리케이션의 규모나 ML모델의 사이즈가 다소 큰 상황일 경우에는 실행하고자 하는 앱에 따라 무료 플랜의 1GB RAM의 리소스가 제한적일 수 있다는 것 입니다.(마찬가지로 Heroku도 기본 무료 플랜은 500MB RAM의 리소스로 제한되어 있습니다)\n따라서 다음 포스팅에서는 Streamlit Sharing이나 Heroku 같은 PaaS가 아닌, 메이저 클라우드 플랫폼에서 웹어플리케이션의 배포와 CI/CD를 하는 과정을 다루어보도록 하겠습니다.\nP.S. Streamlit에 대한 다른 포스팅들은 아래를 참고해주세요 !\n  Streamlit 소개 글 Heroku에 Streamlit 웹어플리케이션 배포하기 Streamlit을 사용한 툴 개발기   코드  Stock price info downloader  Reference  Introducing Streamlit Sharing  ","permalink":"https://wonyoungseo.github.io/posts/2021-10-09-intro-streamlit-sharing/","summary":"Streamlit은 파이썬 기반으로 웹어플리케이션을 개발할 수 있는 프레임워크입니다. 이번 포스팅에서는 Streamlit에서 제공하는 서비스를 통해 아주 쉽고 간단하게 내가 개발한 웹을 배포하는 방법에 대해 소개하고자 합니다.\n웹 어플리케이션 개발 \u0026hellip; 그 다음에는? 웹개발과 다소 거리가 먼 데이터분석가, 데이터 사이언티스트 분들에게 Streamlit은 분석 결과를 공유하거나, 학습한 모델을 제품으로 프로토타이핑 할 수 있는 아주 강력한 도구로 자리 잡았습니다.\n하지만 나 혼자만 보고 사용하면 무슨 소용일까요\u0026hellip;? (도발)\n웹어플리케이션 개발과 별개로 배포 또한 하나의 장벽으로 느껴질 수 있는데요, Streamlit Sharing 은 Streamlit 어플리케이션을 배포하기 위해 알아야 할 모든 지식과 과정을 극도로 간소화 했습니다.","title":"[KR] Streamlit 앱 정말 쉽게 배포하기 (ft. Streamlit Sharing)"},{"content":"Machine Learning Operations(MLOps)에 대한 주제로 공부하며 기초를 정리해보았습니다.\n1. ML VS Real-world ML 1.1. 학술, 연구, 대회, 개인 프로젝트에서의 머신러닝 프로젝트 일반적으로 머신러닝 프로젝트를 시작하거나 학술적인 연구를 하는 경우, 머신러닝 프로젝트는 아래와 같은 성향을 띄고 있음.\n   source: Udacity\n   해결하고자 하는 문제의 정의와 데이터셋이 주어져 있음. 프로젝트의 목적은 주어진 데이터셋을 기반으로 평가 메트릭에서 모델의 성능이 가장 높게 나오는 모델링을 실행하는 것 프로젝트 진행 동안 만족할만한 모델 성능이 나올 때까지 일련의 과정을 사이클로 반복함  데이터셋에 대한 탐구 여러가지 피쳐 엔지니어링 테크닉 적용 여러가지 하이퍼파라미터 조절 여러가지 머신러닝 알고리즘의 적용    1.2. 프로덕션까지 이어진 머신러닝 프로젝트    source: Udacity\n   실제 머신러닝 프로젝트에서는 머신러닝 모델이 프로덕션에서 사용되어지는 경우를 염두에 둬야 함. 해결하고자 하는 문제를 정의하고, 머신러닝 기법이 필요한지의 여부를 결정함.  머신러닝 기법이 필요하지 않은 경우도 있으며, 이 때는 굳이 머신러닝을 적용하지 않아도 됨. (~하지만 제품을 마케팅하기 위해, 과제를 따내기 위해, 투자를 받기 위해 등등 갖가지 이유로 기어코 머신러닝을 적용하는 방향으로 가는 경우도 있음~)   여러 경로를 통해 데이터를 직접 확보해야 함. 데이터 정제, 전처리, 모델링을 통해 모델을 구축하고, 실제 프로덕션 시스템에서 동작할 수있도록 배포할 수 있어야 함. 데이터 드리프트(data drift), 퍼포먼스 드리프트(performance drift) 등이 발생하는 경우에 대비해서 지속적인 모니터링과 모델 재학습의 사이클이 자리잡혀 있어야 함. 여러 팀원과 부서에서 협업할 수 있도록, 과정이 명확하고, 재현 가능해야 함.  2. MLOps의 등장 2.1. MLOps의 정의  미국의 정보 기술 연구 \u0026amp; 컨설팅 기업인 Gartner는 2020년 12월을 기준으로 47%의 인공지능 프로젝트가 제품화에 실패한다는 조사 결과를 밝힘.(링크) 이러한 결과는 대부분의 머신러닝 프로젝트가 상단에서 기술한 1.1. 에 기반을 두고 있어서 임을 유추해볼 수 있음. 따라서 실제 프로덕트에서도 머신러닝을 적용하고 유지할 수 있는 파이프라인에 대한 고민을 하게 되었고, MLOps라는 개념이 등장하게 됨. MLOps의 정의: 실제 프로덕션 환경에서 가장 효율적으로 높은 성능, 안정적, 재현가능, 확장가능, 자동화, 재사용성을 갖춘 머신러닝 파이프라인을 end-to-end로 구축하는 방법론  2.2. 성공적인 머신러닝 프로젝트가 되기 위한 조건  (당연한 말이지만 1) 머신러닝으로 풀어야 하는, 풀 수 있는 문제 정의 (당연한 말이지만 2) 목표와 기간 설정 (당연한 말이지만 3) 충분한 데이터셋 프로덕션 환경에서 모델을 유지하기 위한 전략  한번 배포한 모델이 영원히 잘 동착할 거라고 장담할 수 없음.   프로덕션 환경을 늘 고려할 것.  머신러닝 프로젝트를 진행하는 과정에서 내리는 모든 의사결정은 프로덕션 환경에서 동작되는 시나리오를 고려하여 내려야 함.    2.3. MLOps를 적용해야 하는 환경  프로덕션 환경  PoC(Proof of concept)이나 MVP(minimum viable product)를 위한 머신러닝 프로젝트인 경우에는 적용되지 않아도 됨.   모델의 재현, 재사용이 빈번하게 일어나는 경우 프로젝트의 문서화, 협업이 빈번하게 일어나는 경우 프로젝트의 구조가 복잡한 경우 Model Drift가 발생하는 경우  Data drift, Performance drift라고도 함.  Data drift: 시간의 흐름에 따라 데이터의 특성이 달라질 경우 Performance drift: 시간의 흐름에 따라 모델의 성능이 감소할 경우   대부분의 모델은 일정한 시점의 데이터셋을 기반으로 학습되었음. 따라서 새로운 데이터에 대해서는 안정적인 성능을 보이지 않을 수 있음. 이러한 경우를 대비한 시스템이 구축되어 있어야 함.  모델의 성능, 데이터의 특성 등에 대한 모니터링 새로운 데이터에 대한 EDA, 피쳐 엔지니어링, 하이퍼파라미터 튜닝, 모델 재학습과 같은 사이클      Reference  Udacity: Machine Learning DevOps Engineer  ","permalink":"https://wonyoungseo.github.io/posts/2021-09-26-what-is-mlops/","summary":"Machine Learning Operations(MLOps)에 대한 주제로 공부하며 기초를 정리해보았습니다.\n1. ML VS Real-world ML 1.1. 학술, 연구, 대회, 개인 프로젝트에서의 머신러닝 프로젝트 일반적으로 머신러닝 프로젝트를 시작하거나 학술적인 연구를 하는 경우, 머신러닝 프로젝트는 아래와 같은 성향을 띄고 있음.\n   source: Udacity\n   해결하고자 하는 문제의 정의와 데이터셋이 주어져 있음. 프로젝트의 목적은 주어진 데이터셋을 기반으로 평가 메트릭에서 모델의 성능이 가장 높게 나오는 모델링을 실행하는 것 프로젝트 진행 동안 만족할만한 모델 성능이 나올 때까지 일련의 과정을 사이클로 반복함  데이터셋에 대한 탐구 여러가지 피쳐 엔지니어링 테크닉 적용 여러가지 하이퍼파라미터 조절 여러가지 머신러닝 알고리즘의 적용    1.","title":"[KR] MLOps: 무엇인가?"},{"content":"주피터노트북을 벗어나보자 데이터 분석 공부를 시작하는 분들 중 90% 이상은 주피터 노트북을 활용하는 데에서 출발하셨을 것입니다. 주피터 노트북은 데이터 분석 결과를 빠르게 확인할 수 있고 있다는 점에서 아주 간편하고 입문자들에게는 진입장벽이 낮은 도구입니다. 하지만 주피터노트북 형태의 코드는 정리가 되어있지 않으면 코드가 뒤죽박죽 섞일 수 있고, 재사용이 힘들어 유지보수를 하거나 협업을 하는 관점에서는 활용도가 매우 떨어집니다.\n이런 상황에서 기술적인 방법론이 아닌, 클린코드에 대한 감을 잡을 수 있도록 가이드 형식의 내용을 정리해보았습니다.\n 대상 독자\n 주피터 노트북으로 코딩에 입문하고, 현재까지도 사용 중이신 분 연구와 PoC만 해오시다, 실제 프로덕션 코드를 작성할 시점에 맞닥뜨리신 분 여태 홀로 일해야 하는 상황이었다가, 이제 협업을 하셔야 하는 분   1. 프로덕션 코드란\u0026hellip;? 프로덕트에 쓰이는 코드는 간단히 말해, 실제 제품에 쓰이는 코드를 말합니다. 따라서 프로덕션 코드는 아래와 같은 조건들을 갖춰야 합니다.\n  클린 코드\n 읽기 쉽고, 간결하며, 명확한 코드는 유지보수와 협업에 필수    모듈의 형태를 띄는 코드\n 일련의 코드가 논리적인 구조에 따라 함수와 클래스 등으로 구분되어진 형태    모듈\n 파일 형태(ex. .py 확장자의 파일)로 구분되어, 효율적으로 재사용할 수 있도록 잘 정리된 코드 import 하여 쓸 수 있음    2. 클린 코드 작성하기 2.1. 의미 있는 이름의 사용  변수명, 함수명 등은 작업을 묘사하고 타입을 명시합니다.  get, convert, 등 동사를 통해 작업을 묘사 하지만 모든 디테일을 다 넣은 긴 이름은 결코 좋은 이름이 될 수는 없음   비슷한 성향을 가졌다면 일관성을 가진 이름을 부여  ex)  ages, age (x) age_list, age (o)     축약어나 한글자 변수명은 지양하는 것이 좋습니다.  2.2. 적절한 공백의 사용  공백을 사용하여 코드의 영역을 구분시키면, 관리하기도 좋고, 가독성도 향상됩니다. 한 줄에 79자 정도로 코드를 제한 하는 것을 권장합니다.  2.3. 모듈러 코드 작성 모듈러 코드를 작성하는 이유는 같은 코드를 반복해서 작성하지 않고도 재사용하는 것이, 장기적으로 봤을 때 유지보수 관점에서 더 낫기 때문입니다.\n모듈러 코드를 작성할 때 지키면 좋을 가이드를 몇가지 알아보도록 하겠습니다.\n 반복하지 말 것 (Don\u0026rsquo;t repeat yourself - DRY 객체(함수, 클래스, 모듈 등)의 갯수를 최소화. 불필요하게 많은 객체는 오히려 역효과가 있음. 하나의 함수는 한가지 작업만을 수행해야 함.  함수가 하나 이상의 작업을 수행할 경우, 재사용이나 일반화를 하는데 제한적임. 만약 함수 내에서 AND의 요소가 발견된다면, 쪼개서 작성할 것을 추천함.   재사용을 위해 변수명, arguemnt명은 되도록이면 가장 기본적인 의미를 담는 것이 좋습니다.  간단한 변수명일 수록 가독성이 향상 다양한 상황에서의 재사용이 쉬워짐\u001c   하나의 함수당 argument 사용을 3개까지 제한하는 것을 권장합니다. 만약 4개 이상 사용하게 된다면, 함수를 쪼개어 구성하는 방안을 생각해보세요.  3. 리팩토링(Refactor) 리팩토링이란 코드의 기능에 영향을 끼치지 않은 상태를 유지하며 코드의 구조와 호율성을 향상시키는 작업입니다. 주로 초기 의도한 코드를 작성한 이후 이루어집니다.\n리팩토링은 좋은 퀄리티의 코드를 위해서 빠질 수 없는 작업입니다. 초반 기능을 구현하기 바쁠 대에는 불필요해 보일 수 있어도, 장기적으로 봤을 때 반드시 이점을 볼 수 있는 작업이므로 되도록이면 개발 기간을 산정할 때, 리팩토링할 시간을 포함하는 것이 좋습니다.\n4. 문서화(Documentations) 문서화는 개발에 대한 전반적인 내용을 문서로 기록하여 다른 사람들로 하여금 코드에 대한 이해를 돕는 작업인데요, 문서화가 문서를 작성하는 것만 한정하지는 않습니다.\n4.1. 인라인 코멘트 (Inline Comments) 인라인 코드는 주석이라고도 하는데요, 코드 중간 중간 설명을 다는 행위를 뜻합니다. (파이썬에서는 #를 사용합니다) 특정 코드에 대한 설명을 달아 빠른 이해를 도울 수 있습니다.\n주석은 복잡한 로직을 가지고 있는 코드 블럭의 경우, 주석을 통해 자연어로 설명하여 이해를 도울 수 있습니다. 또한 코드 자체가 어떠한 로직의 배경이나 이유, 영향을 받는 외부적인 요인 등을 설명하지 못할 때는 주석으로 설명하는 것이 효과적입니다.\n4.2. 독스트링(Docstrings) 독스트링은 세개의 따옴표로 감싼 주석으로 함수나 모듈에 대한 설명을 작성할 때 사용됩니다.\n  One line docstring\n 함수가 매우 간단할 경우, 아래와 같이 독스트링을 한 줄로 작성할 수 있습니다. 예시) def female_ratio(female_count, total_count): \u0026#34;\u0026#34;\u0026#34; Calculate the ratio of female \u0026#34;\u0026#34;\u0026#34; return female_count / total_count     Multiline docstring\n 다소 설명이 많이 필요한 경우에는 아래의 조건을 만족시키며 docstring의 내용을 확장할 수 있습니다.  argument에 대한 설명 argument의 타입 명시 함수의 목적 함수의 출력에 대한 설명   예시) def female_ratio(female_count, total_count): \u0026#34;\u0026#34;\u0026#34;Calculate the ratio of female. Args: female_count: int. count of female. total_count: int. total count of people. Returns: female_ratio: female_count/total_count. \u0026#34;\u0026#34;\u0026#34; return female_count / total_count     Docstring에는 더욱 다양한 스타일에 대한 내용은 링크 를 참조하세요.\n4.3. 프로젝트 문서 코드에 대한 문서 뿐만 아니라 전체 프로젝트에 대한 문서도 빠질 수 없습니다. 우리가 가장 쉽게 접할 수 있는 프로젝트 문서는 Github에서 종종 볼 수 있는 README.md 파일이죠. 프로젝트 문서는 프로젝트에 대한 개요, 의존성, 사용법, 디버깅 방법 등을 기술합니다.\n효과적인 프로젝트 문서 작성 방식에 대한 내용은 Udacity에서 제공하는 무료 강의를 참조하실 수 있습니다.\n마무리 하며 팀원이 많아지고 프로젝트의 단위가 커질 수록, 클린 코드는 권장사항이 아니라 필수인 것을 체감하고 있습니다. 그리고 클린코드는 습관인 것 같다는 생각이 듭니다. 기존의 습관을 떨쳐내기가 쉽지 않네요. 입사 초창기부터 가이드해줄 수 있는 시니어의 존재가 조직에 없었던 것이 참 아쉽게 다가오기도 합니다. 괜히 세상에 존재하지도 않는 그 분을 탓해보네요. (좋은 시니어나 멘토가 있다면 당신은 정말 축복받은 사람입니다!)\n아무튼 주니어 분석가를 위한 클린코드에 대한 가이드를 나름대로 적어보았는데요, 처음 기획했던 것보다 다뤄야 할 내용이 다소 더 많아진 것 같습니다. 조금 더 공부하고 정리해서 다음에는 2편으로 돌아오겠습니다 :)\nReference  파이썬 클린 코드 Udacity: Machine Learning DevOps Engineer  ","permalink":"https://wonyoungseo.github.io/posts/2021-09-11-intro-clean-code/","summary":"주피터노트북을 벗어나보자 데이터 분석 공부를 시작하는 분들 중 90% 이상은 주피터 노트북을 활용하는 데에서 출발하셨을 것입니다. 주피터 노트북은 데이터 분석 결과를 빠르게 확인할 수 있고 있다는 점에서 아주 간편하고 입문자들에게는 진입장벽이 낮은 도구입니다. 하지만 주피터노트북 형태의 코드는 정리가 되어있지 않으면 코드가 뒤죽박죽 섞일 수 있고, 재사용이 힘들어 유지보수를 하거나 협업을 하는 관점에서는 활용도가 매우 떨어집니다.\n이런 상황에서 기술적인 방법론이 아닌, 클린코드에 대한 감을 잡을 수 있도록 가이드 형식의 내용을 정리해보았습니다.","title":"[KR] 주피터노트북만 써왔던 당신을 위한 클린코드 소개서"},{"content":"Airflow를 처음 알게 된 후, 이것 저것 찾아보는 중, 아래와 이미지를 접한 기억이 납니다.\nSource Airflow: a workflow management platform\n수많은 태스크들이 모여 복잡한 DAG를 이루는 파이프라인이 구축되어 있는데요, 처음에 언뜻 봤을 때는, 정말 그럴 듯하고 멋있어 간지나 보였습니다. 하지만, Airflow를 적극 도입하려고 하는 현재 시점에서 다시 생각해보니, 막상 저런 파이프라인을 전부 다 파악하고 관리할 상상을 하니까 결코 쉽지 않을 것 같다는 인상도 피할 수 없었습니다. 하지만 이런 고민은 Airflow2에서부터 도입된 Task Group의 도움을 받아 개선할 수 있을 것으로 보입니다.\n이번 글에서는 Task Group의 개념과 사용 예제를 정리합니다.\nTask Group 개념 특히 사용자는 Task Group을 통해 복합하게 연결된 태스크를 복수 개의 그룹으로 묶어 Webserver의 Graph View에서 깔끔하게 시각화 할 수 있으며, 특정 프로세스의 그룹은 재사용할 수도 있습니다.\nSubdag와 Task Group의 비교 Airflow 1.x 에서는 유저가 SubDag를 따로 정의하여 이러한 역할을 수행할 수 있게 했습니다. 하지만 Subdag는 하나의 DAG 내부에 또 다른 DAG를 선언하는 형식으로 구성되기 때문에 파라미터와 스케줄이 추가적으로 정의되어야 하고, 만에 하나 엉킨다면 상위 DAG에서 문제를 일으킬 소지가 있는 만큼, 안정적인 방법은 아니었습니다.\n반면, Task Group의 경우, 개별 파라미터와 스케줄 등을 정의할 필요 없이 단순히 태스크를 묶어주는 역할만 하기 때문에, 많은 자원을 차지하지 않습니다.\nTask Group 정의 방법 Task Group 객체는 다음과 같이 사용할 수 있습니다.\n Task Group 정의 방법  # Task Group 객체 불러오기 from airflow.utils.task_group import TaskGroup # 시작 task task_0 = DummyOperator(task_id=\u0026#39;init\u0026#39;) # Task Group 1 with TaskGroup(group_id=\u0026#39;group_1\u0026#39;) as tg_1: task_1 = DummyOperator(task_id=\u0026#39;task_1\u0026#39;) task_2 = DummyOperator(task_id=\u0026#39;task_2\u0026#39;) # Task Group 1 내부에서의 의존성 task_1 \u0026gt;\u0026gt; task_2 # Task Group 2 with TaskGroup(group_id=\u0026#39;group_2\u0026#39;) as tg_2: task_3= DummyOperator(task_id=\u0026#39;task_3\u0026#39;) task_4 = DummyOperator(task_id=\u0026#39;task_4\u0026#39;) [task_3, task_4] \u0026gt;\u0026gt; task_5 # 종료 task task6 = DummyOperator(task_id=\u0026#39;end\u0026#39;)  DAG 내부에서 의존성을 정의하는 방식은 기존 task 간의 의존성 정의 방식과 마찬가지로 \u0026gt;\u0026gt; 연산자를 사용하면 됩니다.  # DAG 내 task 의존성 정의 task_0 \u0026gt;\u0026gt; tg_1 \u0026gt;\u0026gt; tg_2 \u0026gt;\u0026gt; task6 Task 테스트 시 주의 사항 보통 task를 테스트하는 명령어는 다음과 같습니다.\nairflow tasks test [dag_id] [task_id] [date] 하지만 Task Group에 속하는 task의 경우에는 다음과 같이 group_id를 앞에 붙여줘야 합니다.\nairflow tasks test [dag_id] [group_id.task_id] [date] Task Group을 활용한 예제 Task Group을 활용하여 어떠한 가상의 프로세스 파이프라인에서 특성이나 단계에 따라 task들을 묶어 DAG를 형성해보았습니다. Webserver 상의 Graph View도 그룹에 따라 구분되었으며, 그룹을 펼쳐 볼 수도 있게 되었습니다. 자세한 내용은 아래 덧붙인 예제 코드와 Graph View 상의 DAG 시각화를 비교해보시길 바랍니다.\n      import airflow from airflow import DAG from airflow.utils.task_group import TaskGroup from airflow.operators.dummy import DummyOperator default_args = { \u0026#34;owner\u0026#34;: \u0026#34;temp\u0026#34;, \u0026#34;depends_on_past\u0026#34; : False, \u0026#34;start_date\u0026#34; : None } with DAG( dag_id = \u0026#39;dummy_dag\u0026#39;, default_args = default_args, description = \u0026#34;Task Group demo\u0026#34;, schedule_interval = None, start_date = airflow.utils.dates.days_ago(10), ) as dag: task_0 = DummyOperator(task_id=\u0026#34;task_0\u0026#34;) with TaskGroup(group_id=\u0026#39;group_1\u0026#39;) as tg_1: task_1_1 = DummyOperator(task_id=\u0026#34;task_1_1\u0026#34;) task_1_2 = DummyOperator(task_id=\u0026#34;task_1_2\u0026#34;) with TaskGroup(group_id=\u0026#39;group_1_3\u0026#39;) as tg_1_3: task_1_3_1 = DummyOperator(task_id=\u0026#34;task_1_3_1\u0026#34;) task_1_3_2 = DummyOperator(task_id=\u0026#34;task_1_3_2\u0026#34;) task_1_3_3 = DummyOperator(task_id=\u0026#34;task_1_3_3\u0026#34;) task_1_3_4 = DummyOperator(task_id=\u0026#34;task_1_3_4\u0026#34;) [task_1_3_1, task_1_3_2, task_1_3_3] \u0026gt;\u0026gt; task_1_3_4 task_1_1 \u0026gt;\u0026gt; task_1_2 \u0026gt;\u0026gt; tg_1_3 with TaskGroup(group_id=\u0026#39;group_2\u0026#39;) as tg_2: task_2_1 = DummyOperator(task_id=\u0026#34;task_2_1\u0026#34;) with TaskGroup(group_id=\u0026#39;group_2_2\u0026#39;) as group_2_2: task_2_2_1 = DummyOperator(task_id=\u0026#34;task_2_2_1\u0026#34;) task_2_2_2 = DummyOperator(task_id=\u0026#34;task_2_2_2\u0026#34;) task_2_2_1 \u0026gt;\u0026gt; task_2_2_2 with TaskGroup(group_id=\u0026#39;group_2_3\u0026#39;) as group_2_3: task_2_3_1 = DummyOperator(task_id=\u0026#34;task_2_3_1\u0026#34;) task_2_3_2 = DummyOperator(task_id=\u0026#34;task_2_3_2\u0026#34;) task_2_3_3 = DummyOperator(task_id=\u0026#34;task_2_3_3\u0026#34;) [task_2_3_1, task_2_3_2, task_2_3_3] with TaskGroup(group_id=\u0026#39;group_2_4\u0026#39;) as group_2_4: task_2_4_1 = DummyOperator(task_id=\u0026#34;task_2_4_1\u0026#34;) task_2_4_2 = DummyOperator(task_id=\u0026#34;task_2_4_2\u0026#34;) task_2_4_3 = DummyOperator(task_id=\u0026#34;task_2_4_3\u0026#34;) [task_2_4_1, task_2_4_2] \u0026gt;\u0026gt; task_2_4_3 with TaskGroup(group_id=\u0026#39;group_2_5\u0026#39;) as group_2_5: task_2_5_1 = DummyOperator(task_id=\u0026#34;task_2_5_1\u0026#34;) task_2_5_2 = DummyOperator(task_id=\u0026#34;task_2_5_2\u0026#34;) [task_2_5_1, task_2_5_2] task_2_1 \u0026gt;\u0026gt; group_2_2 \u0026gt;\u0026gt; group_2_3 \u0026gt;\u0026gt; group_2_4 \u0026gt;\u0026gt; group_2_5 with TaskGroup(group_id=\u0026#39;group_3\u0026#39;) as tg_3: task_3_1 = DummyOperator(task_id=\u0026#39;task_3_1\u0026#39;) task_3_2 = DummyOperator(task_id=\u0026#39;task_3_2\u0026#39;) task_3_3 = DummyOperator(task_id=\u0026#39;task_3_3\u0026#39;) task_3_4 = DummyOperator(task_id=\u0026#39;task_3_4\u0026#39;) [task_3_1, task_3_2, task_3_3, task_3_4] with TaskGroup(group_id=\u0026#39;group_4\u0026#39;) as tg_4: task_4_1 = DummyOperator(task_id=\u0026#39;task_4_1\u0026#39;) task_4_2 = DummyOperator(task_id=\u0026#39;task_4_2\u0026#39;) task_4_1 \u0026gt;\u0026gt; task_4_2 task_0 \u0026gt;\u0026gt; [tg_1, tg_2] \u0026gt;\u0026gt; tg_3 \u0026gt;\u0026gt; tg_4 Reference  Airflow Documentation - DAGs  ","permalink":"https://wonyoungseo.github.io/posts/2021-08-29-airflow2-task-group/","summary":"Airflow를 처음 알게 된 후, 이것 저것 찾아보는 중, 아래와 이미지를 접한 기억이 납니다.\nSource Airflow: a workflow management platform\n수많은 태스크들이 모여 복잡한 DAG를 이루는 파이프라인이 구축되어 있는데요, 처음에 언뜻 봤을 때는, 정말 그럴 듯하고 멋있어 간지나 보였습니다. 하지만, Airflow를 적극 도입하려고 하는 현재 시점에서 다시 생각해보니, 막상 저런 파이프라인을 전부 다 파악하고 관리할 상상을 하니까 결코 쉽지 않을 것 같다는 인상도 피할 수 없었습니다. 하지만 이런 고민은 Airflow2에서부터 도입된 Task Group의 도움을 받아 개선할 수 있을 것으로 보입니다.","title":"[KR] Airflow 2.x 에서는 Task Group 씁시다"},{"content":"작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1.x에 한정되어 있기 때문에, 이번 글에서는 Airflow의 공식 Docker 이미지를 활용해보도록 하겠습니다.\n0. Docker 설치 Docker가 설치되어 있지 않은 경우 아래의 링크를 통해 Docker를 설치하도록 합니다.\n Docker 설치 (맥, 윈도우, 리눅스)  맥, 윈도우의 경우 Docker 데스크탑을 설치할 시, Docker Compose가 함께 설치 됩니다. 단, 리눅스의 경우에는 별도의 설치가 필요합니다.\n Docker Compose 설치 (리눅스)  1. 아무것도 모르지만 일단 실행해보기 Airflow 공식 문서(Running Airflow in Docker)에서는 기본적인 Docker 및 Docker Compose를 활용한 Airflow 예제를 제공하고 있습니다. 이 파일을 토대로 일단 한번 실행 과정을 따라가보겠습니다.\n1.1. Airflow 2 Docker Compose 파일 공식문서에서의 Docker Compose 파일 기본적으로 CeleryExecutor를 사용하는 환경설정이 정의되어 있지만, 이번 예제에서는 단일 머신에서 구동을 할 것이기 때문에 LocalExecutor를 사용하도록 docker-compose.yml 파일을 아래와 같이 약간만 변경하도록 하겠습니다. (기본적으로 Celery와 Redis와 관련된 설정은 제외하였습니다.)\n./docker-compose.yml\nversion: \u0026#39;3\u0026#39; x-airflow-common: \u0026amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}  # Docker 이미지는 Airflow 2.1.2 를 사용 environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: LocalExecutor  # LocalExecutor를 사용 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # PostgreSQL을 데이터베이스로 사용 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#39;true\u0026#39; # 실행시 DAG가 자동으로 동작되지 않음 AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#39;false\u0026#39; # 기본 예제 DAG는 불러오지 않음 AIRFLOW__API__AUTH_BACKEND: \u0026#39;airflow.api.auth.backend.basic_auth\u0026#39; _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} volumes:\t # DAG, 로그, 플러그인 파일이 저장될 경로의 폴더를 볼륨으로 마운트함 - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins - ./data_files:/opt/airflow/data_files user: \u0026#34;${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\u0026#34; depends_on: postgres: condition: service_healthy services: postgres: image: postgres:13 environment: POSTGRES_USER: airflow POSTGRES_PASSWORD: airflow POSTGRES_DB: airflow volumes: - postgres-db-volume:/var/lib/postgresql/data healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] interval: 10s retries: 5 restart: always airflow-webserver: \u0026lt;\u0026lt;: *airflow-common command: webserver ports: - 8080:8080\t# 실행되는 컨테이너와 localhost의 포트를 8080으로 맞춰, 실행 중인 Webserver에 접근할 수 있도록 함 restart: always airflow-scheduler: \u0026lt;\u0026lt;: *airflow-common command: scheduler restart: always airflow-init: \u0026lt;\u0026lt;: *airflow-common command: version environment: \u0026lt;\u0026lt;: *airflow-common-env _AIRFLOW_DB_UPGRADE: \u0026#39;true\u0026#39; _AIRFLOW_WWW_USER_CREATE: \u0026#39;true\u0026#39; # 기본 유저 계정을 생성함 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}  # 기본 유저 Username _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}  # 기본 유저 Password volumes: postgres-db-volume: 1.2. Docker Compose 파일 실행하여 Airflow 컨테이너 띄우기   우선 DAG, 로그, 플러그인, 데이터 파일을 저장할 경로를 생성합니다.\n mkdir ./dags ./logs ./plugins ./data_files    Docker 컨테이너에서 구동되는 파일과 호스트의 파일이 동일한 user / group permission을 가질 수 있도록 .env파일을 생성합니다.\n echo -e \u0026quot;AIRFLOW_UID=$(id -u)\\nAIRFLOW_GID=0\u0026quot; \u0026gt; .env    Airflow를 실행하기에 앞서, 데이터베이스를 구축하고 유저를 생성하는 명령어를 실행합니다.\n docker-compose up airflow-init 다음과 같은 메세지와 함께 완료되었다면 데이터베이스와 유저 생성이 성공적으로 실행된 것입니다.       3번이 정상적으로 실행되었다면, 다음 명령어를 통해 Airflow를 실행합니다. (실행하는 시간이 걸릴 수 있습니다.)\n docker-compose up    localhost:8080 , 즉 Airflow Webserver UI로 접속해보겠습니다.\n docker-compose.yml 에서 정의한 Username과 Password를 입력해서 로그인합니다.            Security -\u0026gt; List Users 경로에서 추가적으로 유저 계정을 생성할 수 있습니다.           1.3. DAG 작성하기 예제로 사용할 수 있는 간단한 DAG를 작성하고 로컬 경로(./dags/)에 저장해서, 현재 구동 중인 Airflow 컨테이너에서 실행해보겠습니다. 해당 DAG 파일은 Launch Library API에서 우주 발사 관련 데이터를 다운 받고, 이 파일을 변환하여 text파일로 저장하는 프로세스를 수행합니다.\n( Launch Library API는 SpaceDev 사에서 제공하는 API로, 전세계의 우주 로켓 등 모든 우주 발사체의 발사 일정을 데이터로 제공하는 무료 API입니다. )\n./dags/dag_rocket_launch_schedule.py\nimport datetime import requests import airflow from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator def save_launch_schedule_txt(): with open(\u0026#34;/tmp/launches.json\u0026#34;) as f: launches = json.load(f)[\u0026#39;results\u0026#39;] launches_df = pd.DataFrame.from_dict(launches, orient=\u0026#39;records\u0026#39;) cur_datetime = str(datetime.datetime.now()) launches_df.to_csv(\u0026#39;/opt/airflow/data_files/{}_launch_schedule.txt\u0026#39;.format(cur_datetime), encoding=\u0026#39;utf-8\u0026#39;, sep=\u0026#39;\\t\u0026#39;) default_args = { \u0026#34;start_date\u0026#34; : airflow.utils.dates.days_ago(1), \u0026#34;owner\u0026#34; : \u0026#34;airflow_admin\u0026#34; } with DAG( dag_id=\u0026#34;get_upcoming_rocker_launch_schedule\u0026#34;, schedule_interval=@daily, default_args = default_args ) as dag: download_launches = BashOperator( task_id = \u0026#34;download_launches\u0026#34;, bash_command = \u0026#34;curl -o /tmp/launches.json -L \u0026#39;https://ll.thespacedevs.com/2.0.0/launch/upcoming\u0026#39;\u0026#34; ) save_launch_schedule = PythonOperator( task_id = \u0026#34;update_launch_schedule\u0026#34;, python_callable = save_launch_schedule_txt ) download_launches \u0026gt;\u0026gt; save_launch_schedule 작성한 DAG 가 실행되었고, 결과 파일이 성공적으로 저장된 것을 마운트한 로컬 머신의 디렉토리에서 확인할 수 있습니다.\n                 2. 커스텀 이미지 만들기 가장 기본적인 예제를 순서대로 따라가며 Airflow 2.x를 Docker를 활용해서 실행해보았습니다. 그렇다면 이대로 예제를 수정해가며 DAG를 작성해서 실전에 적용시켜도 될까요? 실제 프로젝트에 적용하기에는 한계가 존재합니다. Airflow 공식 Docker 이미지는 말 그대로 reference image로, Airflow가 Docker 컨테이너에서 구동하기 위한 최소한의 요소만 갖추어져 있습니다. 즉, 대부분의 경우에는 실제 프로젝트에서 사용하는 커스텀 패키지나 의존적인 부분이 설치되어 있지 않습니다. 따라서 사용자는 Airflow 이미지를 활용하여 커스텀 이미지를 빌드하는 것이 적절한 사용 예라고 Airflow에서 안내하고 있습니다. (Airflow 공식 문서 - Building Image)\n2.1. 커스텀 Airflow Docker 이미지 만들기 커스텀 Docker 이미지를 만드는 것은 그리 어렵지 않습니다.\n  사용하고자 하는 패키지가 정의된 requirements.txt 파일 작성합니다.\n./requirements.txt(예시)\nCython==0.29.23 numpy==1.19.2 pymssql==2.1.5 pandas==1.1.2 pytz==2020.1   Docker 파일 작성을 작성합니다.\n./Dockerfile\nFROMapache/airflow:2.1.2 # 앞서 다룬 docker-compose.yml 파일에서 사용한 동일한 Airflow 이미지를 사용합니다. USERroot RUN apt-get update \\  \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\  build-essential libopenmpi-dev \\  \u0026amp;\u0026amp; apt-get autoremove -yqq --purge \\  \u0026amp;\u0026amp; apt-get clean \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* USERairflowCOPY requirements.txt requirements.txt # 현재 경로에 작성해둔 requirements.txt 파일으로 복사합니다.RUN pip3 install -r requirements.txt --no-cache-dir # 패키지를 설치합니다. 캐쉬는 저장할 필요가 없음을 파라미터로 넘깁니다.  커스텀 Docker 이미지를 빌드합니다.\n  --tag 로 이미지의 이름과 버전을 전달합니다.\n docker build . --tag \u0026quot;my_custom_image:0.0.1\u0026quot;        Docker 이미지가 빌드된 것을 확인할 수 있습니다.\n          2.2. 커스텀 Airflow Docker 이미지를 통해 컨테이너 띄우기  커스텀 Docker 이미지를 활용하여 Airflow를 실행할 수 있도록 docker-compose.yml를 수정합니다.\n./docker-compose.yml\nversion: '3' x-airflow-common: \u0026amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-my_custom_image:0.0.1} # Airflow 이미지를 커스텀 Docker 이미지로 수정합니다. environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: LocalExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' ...   앞서 docker-compose.yml 파일을 실행하여 컨테이너를 띄웠던 과정을 다시 실행합니다.\ndocker-compose up airflow-init docker-compose up   3. 마무리하며 Airflow 공식 문서를 참고하여, Docker를 활용하여 2.x 버전을 실행시키는 과정을 정리해봤습니다. 확실히 Docker를 쓰면 앞으로의 관리나 지식 전달 등의 과정도 무척 간소화될 것 같아 적극적으로 적용해야겠다는 생각이 드네요.\nReference  Airflow 공식 문서 - Running Airflow in Docker Airflow 공식 문서 - Building Image Datapipelines with Apache Airflow (Bas Harenslak, Julian de Ruiter)  ","permalink":"https://wonyoungseo.github.io/posts/2021-08-15-airflow2-with-docker/","summary":"작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1.","title":"[KR] Docker를 활용하여 Airflow 2.x 실행하기"},{"content":" 한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 2장 - 전자 회로의 조합 논리 2.0. 전자 회로의 조합 논리  컴퓨터는 2진법을 적용한 비트를 내부 언어로 사용함. 컴퓨터는 비트를 기반으로 새로운 비트를 표현하고 연산하는 불리언대수 또는 조합논리(combinatorial logic)을 통해 구현한 기능을 통해 동작함.  2.1. 논리 게이트  1960년대 개발된 논리 연산을 수행하는 회로. 간단하게 게이트라고도 함. 앞서 1장에서 다루었던 기본적인 부울대수의 연산을 수행하며, 이들의 결합으로 더욱 복합적인 기능을 구현할 수 있음.  2.2. 게이트를 조합한 회로  가산기 adder  덧셈연산을 하는 논리회로 종류  반가산기 전가산기 리플 자리올림 가산기 올림 예측 가산기     디코더 decoder  인코딩된 수를 비트의 집합으로 변환하는 회로   디멀티플렉서 demultiplexer (dmux)  하나의 입력을 받아 몇가지의 출력 중 한 곳으로 전달하는 회로   설렉터 selector (multiplexer)  여러 입력 중 하나의 입력을 선택하는 회로    3장. 순차 논리 3.0. 순차 Sequence  앞서 다룬 조합논리는 입력의 현재 시점의 상태만을 다룸. 반면 순차 논리는 시간의 흐름에 따른 값을 다룸. 입력값의 현재 상태 뿐만 아니라 과거의 입력값 또는 입력값의 과거 상태를 함께 고려해야하는 것을 의미함. 순차 논리를 통해 데이터를 저장하는 메모리 등을 구현함.  3.1. 시간의 표현  발진자  발진자 (또는 진동자)가 진동하는 특정 주기를 기준으로 두어 시간을 측정할 수 있음.   클록  발진자에서 제공되는 시간을 셀 수 있게 해주는 신호. 신호를 통해서 발진자에서 표현되는 전기적 신호의 진동 속도를 측정할 수 있음. 컴퓨터 부품이 고장나지 않는 선에서 클록의 속도를 더욱 빨리 하여 단위 시간당 진동횟수를 늘리는 것을 오버클럭이라고 함.   래치: 1비트의 정보를 기억, 보관할 수 있는 회로 플립플롭: 데이터의 전이(에지)를 통해서 시점을 반영하는 래치 레지스터: 클록을 공유하는 복수개의 플립플롭을 묶은 기억장치 또는 메모리  3.2. 메모리 3.2.1. 임의 접근 메모리 RAM  임의 접근, 휘발성의 특성을 가진 메모리  3.2.2. 읽기 전용 메모리 ROM  휘발성인 RAM과 달리 한번 쓰기가 적용된 이후에는 여러번 읽을 수 있는 메모리  3.2.3. 플래시 메모리  EEPROM의 한종류로, 블록으로 나누는 것이 가능함. 읽을 때 임의접근, 쓸 때 블록 접근의 구조를 가진 장치.  3.3. 블록 장치  우리가 흔히 아는 디스크 드라이브 원판 또는 플래터에 비트를 저장함. 플로피 디스크, 광학 디스크(CD, DVD), 자기 테이프, 하드 드라이브 등이 모두 블록 장치에 해당함 SSD로 세대 교체 중  ","permalink":"https://wonyoungseo.github.io/posts/2021-08-08-the-secrete-life-of-programs-revie-chp-0203/","summary":"한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 2장 - 전자 회로의 조합 논리 2.0. 전자 회로의 조합 논리  컴퓨터는 2진법을 적용한 비트를 내부 언어로 사용함. 컴퓨터는 비트를 기반으로 새로운 비트를 표현하고 연산하는 불리언대수 또는 조합논리(combinatorial logic)을 통해 구현한 기능을 통해 동작함.  2.1. 논리 게이트  1960년대 개발된 논리 연산을 수행하는 회로. 간단하게 게이트라고도 함. 앞서 1장에서 다루었던 기본적인 부울대수의 연산을 수행하며, 이들의 결합으로 더욱 복합적인 기능을 구현할 수 있음.","title":"[KR] 한 권으로 읽는 컴퓨터 구조와 프로그래밍 2-3장 : 조합논리,  순차논리"},{"content":"다시 돌아온 글또 6기 지난 5월 글또 5기를 마친 시점에서 여러가지로 굉장히 지쳤었던 생각이 납니다. 잠시 그때로 기억을 거슬로 올라가보면, 코로나 시국으로 인해 심적 피로가 쌓여가는 것을 느끼고 있었고, 회사 일과 병행하고 있는 방송통신대 3번째 학기 과제와 기말고사가 겹쳐있었던 시점이었던 것 같습니다. 글또 활동을 통해서 쓴 포스팅의 퀄리티도 곤두박칠 치지 않았나 생각이 들었습니다. 그래서 다음 기수에는 참여할 수 있을지 스스로 의문이 들기도 했습니다. 그런데 다시 신청했네요. 또 속냐\n글또를 참여하는 이유 글또를 참여하는 분들마다 참여 동기와, 각자가 느끼는 글또의 매력은 다 다를 것입니다. 제 경우에는 다양한 분들이 쓰시는 글들을 통해 제 시야가 넓어지는 것에 가장 큰 매력을 느꼈습니다. 글또에서 공유되는 글들은 분야가 겹치기도 하고, 또 직접 찾아보지 않고는 알 수가 없는 생전 처음 들어보는 분야이기도 합니다. 게다가 일반 정보 요약, 메모 용도의 포스팅과 다르게 정성을 들여서 쓴 글이기에 퀄리티도 보장이 되죠. 이해가 안 되는 부분이나 글에 데한 의견을 슬랙을 통해 저자와 나눌 수 있는 부분도 너무 좋았습니다. 이런 교류를 통해 제가 신선한 자극을 받을 수 있다는 점이 정말 중독적으로 다가왔습니다. 이번에도 글또를 참여하는 가장 큰 이유라고 할 수 있습니다. 이건 제가 글또라는 커뮤니티를 통해서 외적으로 얻는 부분입니다.\n그렇다면 글또를 통해서 내적으로 얻을 수 있는 것은 무엇일까요. 글또를 참여하게 되면, 글감에 대한 고민과 주제를 어떻게 글로 풀어나갈지 계속해서 생각하고 정리하게 됩니다. 처음에는 상당히 피로했는데, 4, 5기를 참여하면서 상당히 자연스러운 과정이 되었습니다. 일을 할 때나 개인적인 공부를 할 때도, \u0026ldquo;이걸 글로 정리를 한다면 어떻게 구성해야 할 지\u0026quot;에 대한 생각을 자연스럽게 하게 됩니다. 글을 쓰는 빈도가 다른 분들처럼 높은 것은 아니지만, 그럼에도 불구하고 무엇이든 \u0026ldquo;그냥 하는 일\u0026quot;이 없어졌습니다. 비시즌에는 그냥 생각만 하고, 글또 시즌에는 이를 실제 글로 작성하면서 실천을 하게 됩니다. 글또는 제가 이런 과정을 지속적으로 이행할 수 있게 도와주는 \u0026ldquo;훈련 시즌\u0026rdquo; 같은 거라고 생각하고 있습니다.\n이번 기수에서 다루고자 하는 주제 지난 글또5기에서는 글의 주제가 굉장히 들쑥날쑥했습니다. 아마도 급하게 글을 땜빵으로 작성해서인 것 같습니다. 그래서 이번 기수에서는 주제를 정해놓지 않되, 작성되는 글의 주제가 들쑥날쑥하게 하는 것이 가장 큰 목표입니다.\n생각해놓은 글의 주제는 다음과 같습니다.\n 파이썬 멀티 프로세싱에 관한 정리 및 예제 글 에어플로우 기초 개념 및 초기 구축에 관한 글 MLOps에 대한 이론과 토이 프로젝트 연재 현재 일하고 있는 회사의 도메인에 관한 글 기술 관련 서적에 관한 리뷰 글 방송통신대 과정에 관한 글  글을 마무리 하며 항상 큰 포부를 가지고 글또 시즌을 시작하곤 했는데요, 이번에는 이상하게 그런 기분이 들지 않는 것 같습니다. 반면에 많은 분들이 참여를 해서 그런지, 기대감은 굉장히 많이 부풀어 있는 것 같기도 하네요. 많은 분들을 통해서 많은 자극을 받고, 시야를 넓히며, 또 반대로 제가 다른 분들께 같은 느낌을 드릴 수 있도록 열심히 글을 작성해보겠습니다.\n","permalink":"https://wonyoungseo.github.io/posts/2021-07-30-init-geultto-6/","summary":"다시 돌아온 글또 6기 지난 5월 글또 5기를 마친 시점에서 여러가지로 굉장히 지쳤었던 생각이 납니다. 잠시 그때로 기억을 거슬로 올라가보면, 코로나 시국으로 인해 심적 피로가 쌓여가는 것을 느끼고 있었고, 회사 일과 병행하고 있는 방송통신대 3번째 학기 과제와 기말고사가 겹쳐있었던 시점이었던 것 같습니다. 글또 활동을 통해서 쓴 포스팅의 퀄리티도 곤두박칠 치지 않았나 생각이 들었습니다. 그래서 다음 기수에는 참여할 수 있을지 스스로 의문이 들기도 했습니다. 그런데 다시 신청했네요. 또 속냐\n글또를 참여하는 이유 글또를 참여하는 분들마다 참여 동기와, 각자가 느끼는 글또의 매력은 다 다를 것입니다.","title":"[KR] 글또 6기의 시작"},{"content":" 한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 1장 - 컴퓨터 언어 체계 1.0. 컴퓨터의 언어  언어는 정보의 소통을 하기 위한 매개체 언어를 사용하기 위해서는 2가지가 필요함  인코딩: 어떠한 기호의 집합으로 변환, 기록되어야 함. 문맥: 의사소통의 당사자들이    1.1. 2진법 체계 1.1.1. 비트  비트(bit) 2진법을 사용하는 체계 참/거짓 (또는 다른 형식의 표현)을 기반으로 하는 비트를 사용하여 논리연산을 할 수 있음  1.1.2. 불리언 대수 (Boolean algebra)  비트에 대한 연산 규칙들의 집합 불리언 연산자 (Boolean operators)  NOT : 논리적 반대 AND : 논리곱. 둘 다 참인 경우에만 참 (또는 둘 다 거짓인 경우에만 거짓) OR : 논리합. 둘 중 하나 이상이 참인 경우에만 참 (둘 중 하나라도 거짓인 경우 거짓) XOR : 배타적논리합. 두개의 bit값이 서로 다를 경우에만 참. 서로 같을 경우에는 거짓   드모르간의 법칙  a AND b = NOT( NOT a OR NOT b) 긍정적 논리(정논리)를 부정적 논리(부논리)로 변환하여 연산할 수 있게 됨.    1.2. 정수의 표현 1.2.1. 10진수의 표현  0~9 의 10가지 숫자를 사용하여 표현 1603의 10진수 표현  \\(1 \\times 10^3 + 6 \\times 10^2 + 0 \\times 10^1 + 3 \\times 10^0\\)    1.2.2. 2진수의 표현  비트를 사용하여 표현. 단, 1과 0 두가지 기호 밖에 사용할 수 없음. 1603의 2진수 표현  \\(1 \\times 2^{10} + 1 \\times 2^9 + 0 \\times 2^8 + 0 \\times 2^7 + 1 \\times 2^6 + 0 \\times 2^5 + 0 \\times 2^4 + 0 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0\\)  \\(1024 + 512 + 64 + 2 + 1\\) 즉, 11001000011 로 표현될 수 있음.      1.2.3. 2진수의 덧셈  2진수의 덧셈에서 사용할 비트의 개수를 벗어날 때를 오버플로(overflow)라고 함.  상태 코드 레지스터(condition code register)에 오버플로 비트를 정보를 담아둠.    1.2.4. 음수의 표현  음수를 표현하기 위한 방법으로 부호를 사용하는 것이 있음.  가장 왼쪽 비트 (MSB)를 부호를 표현하기 위한 비트로 사용하는 것. 하지만, XOR과 AND를 사용한 덧셈 연산을 수행할 수 없음.   음수를 표현하기 위한 방법으로 보수(one\u0026rsquo;s coomplement)가 있음.  양수의 모든 비트를 반대로 뒤집는 방법. 단, 순환올림의 상황에서 하드웨어의 추가가 필요하기에 비용이 발생함.   부호, 보수의 단점을 해결하는 음수 표현의 방식으로 2의 보수 표현법이 있음.  비트를 모두 뒤집고, 1을 추가하는 방법.    1.3. 실수의 표현  실수에는 소수점이 포함됨 고정소수점 표현법  소수점의 위치를 정해놓고 표현하는 방법 특수 목적을 위해서 사용되고 있으나, 범용 컴퓨팅에서는 사용되고 있지 않음.   부동소수점 표현법 (IEEE)  IEEE 표준에서 제안하는 부동소수점 방식  1bit - 부호 8bit - 지수 23bit - 가수      1.4. 텍스트의 표현 1.4.1. 아스키 코드 (ASCII)  키보드에 있는 모든 값에 대해서 7비트의 수를 할당한 체계 ASCII 코드 테이블 알파벳 이외에 장치를 제어하기 위해 사용되는 제어문자가 포함됨. 알파벳 뿐만 아니라 다른 언어를 지원하기 위한 표준들도 생김.  ISO-646, ISO-8859, KSC5601 등등    1.4.2. Unicode  문자에 16비트를 부여한 체계  1.4.3. UTF-8 (유니코드 변환 형식 8비트)  호환성과 효율성을 고려해 8비트만으로 문자를 표현할 수 있도록 Unicode를 인코딩하는 방식 ASCII 문자의 변환 때에도 추가의 공간이 필요로 하지 않음 ASCII기호에 해당하지 않는 기호나 문자의 경우에는 UTF-8 덩어리를 복수개를 사용하여 표현 함.  1.6. 색의 표현  대표적으로 적, 녹, 청을 섞은 RGB가 있으며, 이 조합으로 가지고 컬러큐브를 표현할 수 있음.  색을 표현할 때 #rrggbb의 형식으로 표현하며, 각 색상은 16진법으로 표현함.   투명도를 가미한 색표현을 RGBa가 있음.  ","permalink":"https://wonyoungseo.github.io/posts/2021-07-25-the-secrete-life-of-programs-revie-chp-01/","summary":"한 권으로 읽는 컴퓨터 구조와 프로그래밍 (Jonathan E. Steinhart 저, 책만 사) 를 읽고 정리하는 요약 글입니다.\n 1장 - 컴퓨터 언어 체계 1.0. 컴퓨터의 언어  언어는 정보의 소통을 하기 위한 매개체 언어를 사용하기 위해서는 2가지가 필요함  인코딩: 어떠한 기호의 집합으로 변환, 기록되어야 함. 문맥: 의사소통의 당사자들이    1.1. 2진법 체계 1.1.1. 비트  비트(bit) 2진법을 사용하는 체계 참/거짓 (또는 다른 형식의 표현)을 기반으로 하는 비트를 사용하여 논리연산을 할 수 있음  1.","title":"[KR] 한 권으로 읽는 컴퓨터 구조와 프로그래밍 1장 : 컴퓨터 내부의 언어 체계"},{"content":"글또 5기를 마무리하며 글또 5기의 활동이 곧 마지막이라는 알림을 보고 약간 당황했습니다. 5기를 언제부터 시작했는지도 약간 가물가물하던 차, 다시 한번 찾아보니 작년 11월이더라구요. 언제 부터 시작했는지조차 기억이 나지 않는 것을 보니 이번 기수 동안에는 참 개인적으로 정신이 없었다는 걸 새삼 느끼게 되었습니다. 그래도 희미한 기억을 되짚어가며 글또 5기의 되돌아보고자 합니다.\n돌아보기 5기의 시작 어느 직장이나 바쁜 시즌이 있듯이, 현재 일하고 있는 직장에서는 10~12월에 항상 무언가가 터지는 시즌입니다. 연말을 앞두고 쎄한 느낌이 들어서였을까요, 11월 초 글또 5기를 시작하면서 다짐글을 작성하지 않았습니다. 지키지 못 할 약속은 하지 않는다 대신 글또라는 시스템을 통해 무엇인가라도 꾸준히 쓰자 라는 생각을 했던 것 같습니다.\n아쉬웠던 점 그래서 5기 활동 기간동안 썼던 포스팅을 보니 중구난방도 이런 중구난방이 없습니다. 사실 저만 보려고 쓴 포스팅도 있고요, 좀 부끄럽기도 하고요 \u0026hellip; 그래서 글또 슬랙 채널에 공유하지 않은 포스팅도 보입니다.\n초반에 오디오 데이터를 다루는 글, 그 이후에는 자료구조를 정리했고, 마지막으로는 회사에서의 업무내용을 공유하는 포스팅을 작성했습니다. 가장 아쉬운 점은, 시리즈의 느낌을 띄는 주제를 다뤘음에도 확실하게 끝맺음을 짓지 못 했다는 것이라는 생각이 듭니다. 반대로 생각하면 그만큼 방황하며 정체성을 찾는 기간이었다고도 볼 수 있을 것 같네요. 는 합리화\n그럼에도 좋았던 점 글을 작성하게 하는 시스템과 더불어 글또의 진짜 매력은 아무래도 사람에 있습니다. 비슷한 일을 하는 분들과 함께 고민을 나누고, 좋은 점은 눈팅하고, 배우고, 피드백하고, 서로 응원하며 이름모를 동료애를 키울 수 있는 커뮤니티는 정말 많지 않습니다. 이번 기수에서도 분석가 팀 뿐만 아니라 AI/ML엔지니어, 데이터 엔지니어 채널을 통해서 직간접적으로 다양한 분들과 교류할 수 있었습니다.\n갑분 다짐 여전히 기술적인 내용을 습득해서 내용을 정리하고 글로 풀어내는 것은 어렵게 느껴집니다. 그렇지만 글또는 계속해서 참여할 생각입니다. 항상 깊이 있는 내용의 포스팅만 작성할 수는 없겠죠. 작성하는 글이 간단한 개념 정리일 수도 있고, 기술을 소개하는 글이 될 수도 있다고 생각하며 글 쓰는 부담을 좀 내려놓으려고 합니다. 다만, 다음 글또 기수를 참여하게 된다면, 방향성을 가지고 일관적인 글 주제를 유지하며, 데이터 직군으로 일하는 제 정체성도 한번 챙겨보려고 합니다.\n","permalink":"https://wonyoungseo.github.io/posts/2021-05-02-reivew-geultto-5/","summary":"글또 5기를 마무리하며 글또 5기의 활동이 곧 마지막이라는 알림을 보고 약간 당황했습니다. 5기를 언제부터 시작했는지도 약간 가물가물하던 차, 다시 한번 찾아보니 작년 11월이더라구요. 언제 부터 시작했는지조차 기억이 나지 않는 것을 보니 이번 기수 동안에는 참 개인적으로 정신이 없었다는 걸 새삼 느끼게 되었습니다. 그래도 희미한 기억을 되짚어가며 글또 5기의 되돌아보고자 합니다.\n돌아보기 5기의 시작 어느 직장이나 바쁜 시즌이 있듯이, 현재 일하고 있는 직장에서는 10~12월에 항상 무언가가 터지는 시즌입니다. 연말을 앞두고 쎄한 느낌이 들어서였을까요, 11월 초 글또 5기를 시작하면서 다짐글을 작성하지 않았습니다.","title":"[KR] 글또 5기를 마무리하며"},{"content":"1. 정규표현식이란 Regular Expression (또는 Regex)\n 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어. (Wikipedia)\n 2. 정규표현식의 구조 /PATTERN/FLAG\n / :  정규표현식임을 알리는 기호. 정규표현식의 시작과 끝에 위치함.   PATTERN  정규표현식으로 찾고자하는 문자열의 패턴   FLAG  옵션    3. 정규표현식, 표현의 종류 3.1. 그룹과 범위 group and ranges  |  OR, 또는   ()  그룹 지정 복수의 패턴을 하나의 그룹으로 묶어 찾는 식으로 사용할 수 있음 예시)  gray 또는 grey 를 찾고자 할 때  /gr(a|e)y/gm   URL 패턴을 찾고자 할 때  /(http|https):// ... 이하 생략   한번에 패턴을 검색하되, 다른 별개의 그룹으로 찾고자 할 때  (Hi|Hello)|(AND)   그룹을 지정하고 싶지 않을 때     (?:)  그룹을 지정하지 않음 예시)  grey 또는 gray를 찾되, 그룹으로 지정하지않음  (?:grey|gray)       []  대괄호의 모든 문자열에 대해서, 하나라도 만족하는 문자열 예시)  gr로 시작하는 모든 알파벳 3개로 구성된 문자열  gr[a-zA-Z0-9]   gr과 y사이에 a, e, d 중 하나를 충족하는 문자열  gr[aed]y       [^]  부정 문자열. 괄호 안의 어떤 문자열을 제외한 문자열 예시)  대소문자 알파벳, 숫자, 공백이 아닌 모든 문자열  [^a-zA-Z0-9 ]        3.2. 수량 quantifier  ?  없거나, 있거나 (zero or one)   *  없거나, 있거나, 많거나 (zero or more)   +  하나 또는 많이 (one or more)   {n}  n번 반복   {n,}  최소 n번 반복   {,m}  최대 m번 반복   {n, m}  최소 n번, 최대 m번 반복    3.3. 경계 boundary-type  \\b  단어의 경계 문자열의 앞 또는 뒤에서 쓰이는 문자열만 찾음   \\B  단어의 경계가 아님   ^  문장의 시작   $  문장의 끝    3.4. 문자열 character classes  \\  특수 문자가 아닌 문자 예시)  정규표현의 .이 아닌 실제 문장에서 .을 찾고자 할 때  \\.   정규표현식에서 그룹지정의 ()가 아닌, 실제 텍스트에서 ()를 찾고자 할 때,  \\(\\)       .  줄바꿈을 제외한 모든 문자   \\d  숫자   \\D  숫자 아님   \\w  문자열   \\W  문자열 아님   \\s  공백   \\S  공백 아님    4. 간단한 예제 4.1. 전화번호 찾기  \\d{2,3}[- .]\\d{3,4}[- .]\\d{4}  4.2. E-mail 찾기  [\\S]+@[a-zA-Z0-9-]+\\.[a-z.]+  4.3. URL 찾기 (Youtube 주소에서 default url을 제외한 영상 고유의 id찾기)\n (https?:\\/\\/)?(www\\.)?youtu\\.be\\/([a-zA-Z0-9-]+)  5. 파이썬과 정규표현식 5.1. 정규표현식 패키지 - re  파이썬에 내장된 re 패키지를 사용하면 됨. (문서)  import re 5.2. 주요 기능 5.2.1. 정규식 패턴 정의  re.compile  정규식 패턴을 직접 변수로 전달할 수도 있겠지만, re.compile을 통해 미리 컴파일하여 객체로 사용하는 것이 편함    pat = re.compile(\u0026#39;[a-z]+\u0026#39;) print(pat) re.compile('[a-z]+') 5.2.2. 검색  .match  문자열의 처음부터 정규식 패턴과 일치하는지 검색 묹자열의 처음부터 일치하지 않을 경우 검색되지 않음    m = re.match(pat, \u0026#34;python @789\u0026#34;) print(m) \u0026lt;re.Match object; span=(0, 6), match='python'\u0026gt; m = re.match(pat, \u0026#34;#123 python @789\u0026#34;) print(m) None  .search  문자열 전체에 대해서 정규식 패턴과 일치하는 문자열 검색    m = re.search(pat, \u0026#34;#123 python @789\u0026#34;) print(m) \u0026lt;re.Match object; span=(5, 11), match='python'\u0026gt;  re.findall  정규식 패턴과 매치되는 모든 부분 문자열(substring)을 리스트로 리턴    m = re.findall(pat, \u0026#34;#123 python @789\u0026#34;) print(m) ['python']  re.finditer  정규식 패턴과 매치되는 모든 부분 문자열을 iter 객체로 리턴    m = re.finditer(pat, \u0026#34;#123 python pythonista, python @789\u0026#34;) for match in m: print(match) \u0026lt;re.Match object; span=(5, 11), match='python'\u0026gt; \u0026lt;re.Match object; span=(12, 22), match='pythonista'\u0026gt; \u0026lt;re.Match object; span=(24, 30), match='python'\u0026gt;  검색 객체의 메서드  .group() : 매치된 문자열 리턴 .start() : 매치된 문자열의 시작 위치 리턴 .end() : 매치된 문자열의 끝 위치 리턴 .span() : 매치된 문자열의 (시작, 끝) 튜플 리턴    m = re.finditer(pat, \u0026#34;#123 python pythonista, python @789\u0026#34;) for match in m: print(\u0026#34;매치된 문자열 match : {}\u0026#34;.format(match.group())) print(\u0026#34;매치된 문자열 start : {}\u0026#34;.format(match.start())) print(\u0026#34;매치된 문자열 end : {}\u0026#34;.format(match.end())) print(\u0026#34;매치된 문자열 span : {}\u0026#34;.format(match.span())) print(\u0026#34;----------------------------\u0026#34;) 매치된 문자열 match : python 매치된 문자열 start : 5 매치된 문자열 end : 11 매치된 문자열 span : (5, 11) ---------------------------- 매치된 문자열 match : pythonista 매치된 문자열 start : 12 매치된 문자열 end : 22 매치된 문자열 span : (12, 22) ---------------------------- 매치된 문자열 match : python 매치된 문자열 start : 24 매치된 문자열 end : 30 매치된 문자열 span : (24, 30) ---------------------------- 5.3. 문자열 변경  re.sub(정규식패턴, 변경할문자열, 타겟문자열)  문자열 바꾸기 파이썬에서 .replace()와 같은 역할을 함    m = re.sub(pat, \u0026#34;hello\u0026#34;, \u0026#34;#123 python @789\u0026#34;) print(m) #123 hello @789 Reference  정규표현식 , 더이상 미루지 말자 (Youtube 영상) Programiz - Python Programming 정규 표현식에 대한 쉬운 설명 점프 투 파이썬(위키독스)  ","permalink":"https://wonyoungseo.github.io/posts/2021-04-22-python-regex-basic/","summary":"1. 정규표현식이란 Regular Expression (또는 Regex)\n 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어. (Wikipedia)\n 2. 정규표현식의 구조 /PATTERN/FLAG\n / :  정규표현식임을 알리는 기호. 정규표현식의 시작과 끝에 위치함.   PATTERN  정규표현식으로 찾고자하는 문자열의 패턴   FLAG  옵션    3. 정규표현식, 표현의 종류 3.1. 그룹과 범위 group and ranges  |  OR, 또는   ()  그룹 지정 복수의 패턴을 하나의 그룹으로 묶어 찾는 식으로 사용할 수 있음 예시)  gray 또는 grey 를 찾고자 할 때  /gr(a|e)y/gm   URL 패턴을 찾고자 할 때  /(http|https):// .","title":"[KR] 정규표현식 기본 개념과 파이썬 re 패키지"},{"content":" 본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Image - 도커는 레이어드 파일 시스템 기반\n 도커 이미지는 프로세스가 실행되는 파일들의 집합 또는 환경이라고 할 수 있음 프로세스가 실행되면 발생하는 파일들에 대한 변경을 이미지에 쌓는 것  Image  이미지는 두가지로 나뉠 수 있음  읽기전용 Only Read 쓰기가능 Writable   Base Image  읽기 전용 수정할 수 없음 대신 해당 이미지 위에 또 다른 층을 추가할 수 있음    예시 1) docker commit을 통해 이미지 만들기 우분투 베이스 이미지에 Git을 설치하여 새로운 이미지로 저장하기\n 우분투 베이스 이미지에 Git 설치  docker pull ubuntu:latest docker run -it --name git ubuntu:latest bash apt-get update apt-get install -y git git --version # git version 2.25.1 변경이 생긴 베이스 이미지를 커밋하기   docker commit으로 이미지 만들기  :git이라는 태그를 붙여 새로운 이미지를 만듬    docker commit git ubuntu:git docker run -it --name git2 ubuntu:git bash Dockerfile   Dockerfile 핵심 명령어\n FROM : 베이스 이미지 지정 RUN : 쉘 명령어 실행 (도커 이미지를 만들기 위해 사용하는 명령여) CMD : 컨테이너 기본 실행 명령어 (도커 컨테이너를 만들때, 실행할 때 사용하는 명령어) EXPOSE : 컨테이너에서 사용하는 포트 정보 ENV : 환경변수 설정 ADD : 파일 도는 디렉토리 추가. (URL / ZIP 사용 가능) COPY : 로컬에 있는 파일 또는 디렉토리를 복사하여 이미지 내에 추가 ENTRYPOINT : 컨테이너 기본 실행 명령어 VOLUMN : 외부 마운트포인트 생성 USER : RUN, CMD, ENTRYPOINT를 실행하는 사용자 WORKDIR : 작업 디렉토리 변경/설정 ARGS : 빌드타임 환경변수 설정 LABEL : key - value 데이터 ONBUILD : 다른 빌드의 베이스로 사용될 때 사용하는 명령어    docker build로 이미지 만들기\n 전체 명령어 규칙  docker build -t {유저네임스페이스/이미지이름:태그} {빌드컨텍스트} -t : 이미지의 이름 지정 -f : Dockerfile의 위치가 다른 디렉토리라면 해당 옵션을 활용할 수 있음   빌드컨텍스트  현재 디렉토리일 경우 점(.)을 사용 필용한 경우 다른 디렉토리를 지정할 수 있음      .dockerignore 지정\n .gitignore와 비슷한 역할을 함 도커 빌드 컨텍스트에서 지정된 패턴의 파일을 무시함 민감한 정보를 제외하는 용도로 사용됨 빌드속도를 개선하기 위해 불필요한 파일을 제외하는 역할로도 사용됨 빌드할 때 사용되는 파일을 .dockerignore에 포함시켜서는 안 됨    예시 2) Dockerfile과 docker build를 통해 이미지 만들기  Dockerfile  FROMubuntu:latestRUN apt-get updateRUN apt-get install -y git Dockerfile을 빌드  docker build -t ubuntu:git-dockerfile . 도커 이미지 만들기 예시 3) Node 기반 Web app 이미지 만들기  Dockerfile  # 1. base image 정의 FROM ubuntu:20.04 RUN apt-get update # 2. node 설치 ## DEBIAN_FRONTEND=noninteractive ## RUN DEBIAN_FRONTEND=noninteractive apt-get install -y nodejs npm # 3. 현재 로컬 디렉토리에 있는 소스를 이미지 내 디렉토리에 복사 COPY . /usr/src/app # 4. Nodejs 패키지 설치 WORKDIR /usr/src/app # 이동 RUN npm install\t# npm install 실행 # 5. WEB 서버 실행 (Listen 포트 정의) EXPOSE 3000\t# 포트 정보 CMD node app.js\t# 컨테이너에서 실행  .dockerignore  node_modules/*  빌드 시작  docker build -t webapp .  빌드 된 도커 이미지 실행  docker run -p 3000:3000 webapp Dockerfile 최적화 하기 예시 4) Dockerfile 최적화 하기  Dockerfile  # 최적화 포인트1: 이미 node가 설치되어 있는 base image 정의 FROM node:12 # 최적화 포인트2: 패키지를 우선적으로 복사 COPY .package* /usr/src/app/ WORKDIR /usr/src/app RUN npm install COPY . /usr/src/app WORKDIR /usr/src/app EXPOSE 3000 CMD node app.js Reference  인프런 초보를 위한 도커 안내서  ","permalink":"https://wonyoungseo.github.io/posts/2021-04-21-docker-image-basic/","summary":"본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Image - 도커는 레이어드 파일 시스템 기반\n 도커 이미지는 프로세스가 실행되는 파일들의 집합 또는 환경이라고 할 수 있음 프로세스가 실행되면 발생하는 파일들에 대한 변경을 이미지에 쌓는 것  Image  이미지는 두가지로 나뉠 수 있음  읽기전용 Only Read 쓰기가능 Writable   Base Image  읽기 전용 수정할 수 없음 대신 해당 이미지 위에 또 다른 층을 추가할 수 있음    예시 1) docker commit을 통해 이미지 만들기 우분투 베이스 이미지에 Git을 설치하여 새로운 이미지로 저장하기","title":"[KR] Docker Image 개념"},{"content":" 본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Compose Docker Compose란  지금까지는 도커에서 개별의 명령어를 쳐서 이미지를 다운로드하고 컨테이너를 띄우는 과정을 거쳤음 도커 명령어를 통한 작업은 실수가 발생할 수 있는데, 도커 컴포즈는 이 문제를 해결할 수 있는 간결한 방법을 제시함 도커 컴포즈는 띄우려고 하는 복수의 컨테이너에 대한 사항을 Yaml(야믈)파일에 정리하여, 한번에 실행시키는 방식으로 동작함.  Docker Compose 설치   도커 컴포즈는 기본적으로 docker for mac을 설치할 때 함께 설치됨 (리눅스의 경우 그러하지 않기 때문에 따로 설치가 필요함)\n  리눅스의 설치\n  $ sudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.29.0/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose  설치 버전 확인하기  $ docker-compose version docker-compose version 1.28.5, build c4eb3a1f docker-py version: 4.4.4 CPython version: 3.9.0 OpenSSL version: OpenSSL 1.1.1h 22 Sep 2020 Yaml(.yml) 개념 XML, JSON과 같이 시스템 사이에 데이터를 주고 받을 때 사용하는 포맷\n XML 의 구조:  key-value : \u0026lt;key\u0026gt;value\u0026lt;/key\u0026gt; array :    \u0026lt;parent\u0026gt; \u0026lt;child1\u0026gt;\u0026lt;/child1\u0026gt; \u0026lt;child2\u0026gt;\u0026lt;/child2\u0026gt; \u0026lt;child3\u0026gt;\u0026lt;/child3\u0026gt; \u0026lt;/parent\u0026gt;  JSON 의 구조  key-value : {key: value} array    {key: [array1], [array2], [array3], }  Yaml의 구조  key-value : key: value array : 하이픈을 통해 array임을 구별    key : - array1 - array2 docker-compose.yml 예제  워드프레스 블로그를 도커로 띄우기  요구사항  MySQL 컨테이너 띄우기  블로그 데이터베이스가 저장될 볼륨 마운트 하기 워드프레스를 위한 데이터베이스 생성하기   Wordpress 컨테이너 띄우기  볼륨 마운트 하기 MySQL 컨테이너에 연결하여 데이터베이스 사용하기        # docker-compose.yml version: \u0026#39;2\u0026#39; services: db: image: mysql/mysql-server:8.0.15 volumes: - ./mysql:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: wordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: image: wordpress:latest volumes: - ./wp:/var/www/html ports: - \u0026#34;8000:80\u0026#34; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_USER: wordpress 실행 $ docker-compose up Reference  인프런 초보를 위한 도커 안내서  ","permalink":"https://wonyoungseo.github.io/posts/2021-04-15-docker-compose-basic/","summary":"본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n Docker Compose Docker Compose란  지금까지는 도커에서 개별의 명령어를 쳐서 이미지를 다운로드하고 컨테이너를 띄우는 과정을 거쳤음 도커 명령어를 통한 작업은 실수가 발생할 수 있는데, 도커 컴포즈는 이 문제를 해결할 수 있는 간결한 방법을 제시함 도커 컴포즈는 띄우려고 하는 복수의 컨테이너에 대한 사항을 Yaml(야믈)파일에 정리하여, 한번에 실행시키는 방식으로 동작함.  Docker Compose 설치   도커 컴포즈는 기본적으로 docker for mac을 설치할 때 함께 설치됨 (리눅스의 경우 그러하지 않기 때문에 따로 설치가 필요함)","title":"[KR] Docker Compose 개념"},{"content":" 본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n 1. 도커의 기본적인 명령어 ls (또는 ps)  docker container ls (= docker ps)  실행 중인 컨테이넝   docker container ls -a  실행이 중지된 컨테이너까지 출력    stop docker stop [OPTIONS] CONTAINER [CONTAINER ... ]  실행 중인 컨테이너를 중지하는 명령어 실행 중인 컨테이너를 복수로 중지시킬 수도 있음  rm docker rm [OPTIONS] CONTAINER [CONTAINER ... ]  종료된 컨테이너를 삭제하는 명령어  logs docker logs [OPTIONS] CONTAINER  기본 옵션  -f : fetch. 새로고침 할 때마다 로그 발생 시 실시간으로 업데이트 됨 --detail: 가장 마지막 부분을 보여줌   컨테이너가 정상적으로 동작하는지 확인하는 좋은 방법 중 하나  pull ``` docker pull [OPTIONS] NAME[:TAG|@DIGEST] ``` - 도커 이미지 다운로드  rmi ``` docker rmi [OPTIONS] IMAGE [IMAGE...] ``` - 도커 이미지 삭제 - 단, 컨테이너가 실행중인 이미지는 삭제되지 않음.  2. run docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]  기능  컨테이너 실행   명령어  -d : detached mode (백그라운드 모드) -p : 호스트와 컨테이너의 포트를 연결 -v : 호스트와 컨테이너의 디렉토리 연결 -e : 컨테이너 내에서 사용할 환경변수 설정 --name : 컨테이너 이름 설정 --rm : 프로세스 종료시 컨테이너 자동 제거  이 옵션이 없다면, 컨테이너가 종료되더라도 삭제되지 않고 남아있어 수동으로 삭제해야 함.   -it : 사용자가 사용할 수 있는 터미널을 위한 옵션  -i : 입력을 가능하게 하기 -t : 터미널 띄우기   --network : 네트워크 연결   설명  run 명령어를 사용하면, 사용할 이미지가 이미 저장되어 있는지 확인 후, 없다면 pull한 뒤, 컨테이너를 create하고 start함  각각의 동작들은 각각의 명령어로도 따로 실행 가능함.   컨테이너가 실행되지만, 별 다른 명령어를 전달하지 않으면, 생성되자마자 종료됨.  컨테이너는 기본적으로 프로세스이기 때문에, 실행 중인 프로세스가 없으면 컨테이너는 종료됨. 예시: docker run ubuntu:20.04      3. exec  이미 실행중인 도커 컨테이너에 접속할 때 사용 컨테이너 안에 ssh server 등을 설치하지 않고 접속함.  4. network docker network create [OPTIONS] NETWORK  도커 컨테이너끼리 이름으로 통신할 수 있는 가상 네트워크를 만듬  docker network connect [OPTIONS] NETWORK CONTAINER  기존에 생성된 컨테이너에 네트워크를 추가  5. volumn (-v)  호스트의 드라이브를 마운트 시키는 명령어 마운트된 드라이브 없이 컨테이너를 중지/삭제시키면 데이터가 날라가게 됨 따라서 데이터를 저장하고 싶다면, 드라이브를 볼륨으로 마운트시켜야 함.  -v [my own dir]:[container dir] 6. 예제  우분투 이미지 컨테이너로 띄우기  docker run --rm -it ubuntu:20.04 /bin/sh  /bin/sh : sh를 실행함 -it : 키보드 입력 --rm : 프로세스 종료 후 컨테이너 자동 삭제  웹 어플리케이션 실행  docker run --rm -p 5678:5678 hashicorp/http-echo -text=\u0026quot;helllo world\u0026quot;  -p : 포트 연결 [호스트 포트]:[도커 내 포트] -text : 웹 어플리케이션에 들어가는 변수 전달  Redis 실행하기 (메모리기반 데이터베이스)  docker run --rm -p 1234:6379 redis  레디스 테스트 해보기  set hello world get hello quit MySQL 실행하기  docker run -d -p 3306:3306 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true \\ --name mysql \\ mysql/mysql-server:8.0.15   mysql 패스워드를 입력하지 않겠다는 환경변수를 -e로 패스\n  --name : 컨테이너에 이름 설정\n  -d : 백그라운드로 실행\n  실제로 실행을 해서 워드프레스용 DB를 생성함.\n docker exec -it mysql mysql mysql 이라는 컨테이너에서 mysql이라는 명령어를 실행    create database wp CHARACTER SET utf8; CREATE USER wp@'%' IDENTIFIED BY 'wp'; GRANT ALL PRIVILEGES ON wp.* TO wp@'%' WITH GRANT OPTION; flush privileges; quit 워드프레스 블로그 실행하기 (앞서 MySQL이 실행 중인 상태에서)  docker run -d -p 8080:80 \\ -e WORDPRESS_DB_HOST=host.docker.internal \\ -e WORDPRESS_DB_NAME=wp \\ -e WORDPRESS_DB_USER=wp \\ -e WORDPRESS_DB_PASSWORD=wp \\ wordpress  기존에 띄워져 있던 mysql 컨테이너의 데이터베이스를 이용해서 워드프레스 블로그를 띄움.  네트워크  docker network create app-network  app-network라는 이름으로 네트워크를 만듬 (wordpress - mysql을 연결)  docker network connect app-network mysql  mysql 컨테이너에 네트워크를 추가  docker run -d -p 8080:80 \\ --network=app-network \\ -e WORDPRESS_DB_HOST=mysql \\ -e WORDPRESS_DB_NAME=wp \\ -e WORDPRESS_DB_USER=wp \\ -e WORDPRESS_DB_PASSWORD=wp \\ wordpress  워드프레스는 이미지를 실행할 때 네트워크를 지정해볼 수 있음 network는 위에서 생성한 app-network를 지정하고, host는 mysql의 이름으로 접근함.  docker stop mysql docker rm mysql docker run -d -p 3306:3306 \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=true \\ --network=app-network \\ --name mysql \\ -v /Users/wg/dvlp/Lucas/learn_container/volumn:/var/lib/mysql \\ mysql/mysql-server:8.0.15 docker exec -it mysql mysql create database wp CHARACTER SET utf8; CREATE USER wp@'%' IDENTIFIED BY 'wp'; GRANT ALL PRIVILEGES ON wp.* TO wp@'%' WITH GRANT OPTION; flush privileges; quit\t 기존의 mysql 컨테이너를 삭제하고 volumn을 마운트 시켜 다시 띄움 이 경우 mysql의 데이터베이스는 마운트된 디렉토리에 데이터가 남게 되므로, 나중에 다시 동일한 연결하더라도 데이터를 유지할 수 있음.  Reference  인프런 초보를 위한 도커 안내서  ","permalink":"https://wonyoungseo.github.io/posts/2021-04-11-docker-basic-commands/","summary":"본 포스트는 인프런: 초보를 위한 도커 안내서 강의를 수강하며 정리한 내용입니다.\n 1. 도커의 기본적인 명령어 ls (또는 ps)  docker container ls (= docker ps)  실행 중인 컨테이넝   docker container ls -a  실행이 중지된 컨테이너까지 출력    stop docker stop [OPTIONS] CONTAINER [CONTAINER ... ]  실행 중인 컨테이너를 중지하는 명령어 실행 중인 컨테이너를 복수로 중지시킬 수도 있음  rm docker rm [OPTIONS] CONTAINER [CONTAINER .","title":"[KR] Docker 기본 명령어"},{"content":" 이번 포스트에서는 현재 일하고 있는 Who\u0026rsquo;s Good에서 웹어플리케이션 기반의 간단한 툴을 개발한 과정을 기술합니다. 자세한 사내 업무 내용은 생략하며, 유사한 상황 및 시나리오로 대체했음을 밝힙니다.\n \u0026amp;nbsp\n1. Quality Check (QC) 현재 일하고 있는 Who\u0026rsquo;s Good에서는 뉴스 기사를 기반으로 기업의 *ESG 리스크를 평가합니다. 모델을 통해 산출된 결과에 대해서는 꼭 검토 및 검증 프로세스를 거치는데요, 산출된 결과값이 정답인지 아닌지 여부를 확인하는 과정이고, Quality Check 또는 줄여서 QC라고 합니다. 주로 ESG 도메인의 전문가인 ESG 리서쳐 또는 QC 스태프가 이 과업을 수행합니다.\n(*ESG : Environment, Social, Governance 기업의 환경, 사회, 지배구조)\n1.1. 기존의 QC과정 기존의 QC과정에서는 (1)FTP서버를 통해 모형의 결과를 다운 받아 (2)구글 스프레드시트에서 QC를 진행하고 (3)이를 개발자 또는 DB관리자에 전달하여 DB에 업로드 되게끔 하는 과정을 거쳤습니다. 이 과정을 간단히 아래의 이미지와 같이 간단히 도식화 해보았습니다.\n   1.2. QC과정의 불편한 부분 현재 QC 작업은 불가피하게도 ESG 도메인을 가진 전문가가 수행해야 하고, 데이터의 퀄리티를 유지하기 위해 절대 생략할 수 없는 과정입니다. 하지만 옆에서 제가 관찰한 바로는 기존의 QC 과정에서 몇 가지 불편한 점과 개선할 수 있는 부분들이 분명히 보였습니다.\n 파일질라(Filezilla) 같은 프로그램으로 FTP서버에 접속하여 QC 템플릿을 다운 받는 것이 불편함. 앞서 기술한 바와 같이 뉴스기사를 통해 기업을 분석하기 때문에 기업 관련 기사 발생량에 따라 QC 작업에 걸리는 시간이 매우 다를 수 있음. 따라서, 완료된 QC 결과를 전달하는 시간이 일정하지 못했고, 이에 개발자 또는 DB관리자가 불규칙하게 일일이 대응해야함. QC하는 과정에서 데이터를 수정하거나, 결과를 기입하는 과정에 실수가 발생할 수 있음. 따라서 DB입력 과정에서 에러가 발생하거나, 입력 후 추후 데이터 퀄리티 면에서 문제가 이어지는 등, 불확실한 상황이 이어짐.  물론 이러한 불편한 점들에 대한 피드백과, QC스태프들이 편리하게 사용할 수 있는 QC툴에 대한 니즈가 많았습니다. 하지만 프로젝트의 우선순위(항상 외부를 대응 하는 일들이 더 급하고 중요해서 내부가 밀려나가는 상황)와 개발팀의 일정이 있어, 이러한 부분의 개선이 빠른 시일 내에 이루어질 것 같아 보이진 않았죠 \u0026hellip;\n\u0026amp;nbsp\n2. 그래도 이 상태로 계속 갈 수는 없으니까 2.1. 최소한의 요구사항 그래도 제가 도울 수 있는 점이 있지 않을까 싶어, QC툴에 대해 팀원들로부터 의견을 취합해보았고, 수많은 희망사항들을 거르고 걸러서 아래와 같이 의견을 좁혀봤습니다.\n QC템플릿을 다운로드하는 과정이 더 간편했졌으면 좋겠다. QC는 아직 엑셀로 하는게 편하다. 엑셀을 이길 수 있는 걸 가져올 생각이 아니라면 쉽게 생각하지 않는게 좋을 거야 찡끗 QC 결과에 대한 검증이 자동으로 이루어졌으면 좋겠다. QC 결과에 오류가 있다면 업로드가 되지 말아야 한다. 직접 업로드하는 과정이 간편하면 좋다.  2.2. 그러면 Streamlit 을 써보자 저는 개발자는 아닙니다\u0026hellip;만, 제가 애용하는 Streamlit을 활용하면 QC툴의 니즈를 어느 정도 충족할 수 있는 툴을 만들 수 있을 것 같았습니다.\n   \u0026amp;nbsp\nStreamlit은 간단하게 ML/DL 어플리케이션을 구현하고 프로토타입을 만들 수 있는 파이썬 기반의 웹어플리케이션 프레임워크로, 2019년 하반기 등장 이후 아주 빠른 속도로 성장하고 있습니다.\nStreamlit은 아주 다양한 기능을 제공하고 있어, ML/DL 관련의 프로토타이핑이라는 기존의 목적 이외에도 더 다양한 용도로 활용 가능합니다. 저 같이 웹 개발에 대한 지식이 전무하고 없고 파이썬 하나만 알고 있더라도, (Django나 Flask를 알지 못 해도) 웹 어플리케이션을 만들 수 있습니다.\nStreamlit이 제공하는 다양한 기능은 Streamlit 소개 글 (클릭)를 참고해주세요.\n2.3. 이렇게 개선해볼 수 있지 않을까 Streamlit을 통해 (1) QC파일 다운로드 (2) QC 결과 검증 (3) QC 파일 업로드 기능을 가진 웹어플리케이션을 개발하고, 슬랙 알림과 DB저장 등의 과정을 스케줄링하는 부분은 Airflow를 통해 구축하기로 했습니다. (이 포스트에서 Airflow에 대한 자세한 내용은 생략하도록 하겠습니다.)\n   \u0026amp;nbsp\n3. 완성된 QC 툴의 예시 완성된 QC툴은 실제 업무 내용이 많이 포함되어 그대로 기술하기에는 어려운 점이 많기 때문에 이 포스트에서는 매우 유사한 형태와 기능을 하는 예제로 대체 했습니다.\n아래 QC툴은 영화 댓글 감성분석 모델의 결과를 검토 및 검증하는 가상의 QC 과정을 수행합니다. 이 툴에서 활용한 데이터는 네이버 영화 댓글 데이터를 가공하여 만든 가상의 데이터(fake data)입니다.\n\u0026amp;nbsp\nQC스태프, 사용자는 QC툴을 활용하여 아래의 과정을 수행합니다.\n  영화 댓글의 긍정/부정 감성분석 결과를 QC 템플릿으로 다운로드 각 산출 결과에 대해 ACCEPT/REJECT 을 입력하여 QC 작업을 수행 결과를 업로드   \u0026amp;nbsp\n그럼 툴을 한번 살펴보도록 하겠습니다. (전체 소스코드는 github에서 확인할 수 있습니다.)\n3.1. 메인 \u0026amp; QC 가이드라인 페이지 사용자는 간단한 인증 기능을 통해 툴을 활성화 시키고, QC작업을 수행하기에 앞서 QC 가이드라인과 주의사항 등을 확인할 수 있습니다.\n   3.2. QC 템플릿 다운로드 페이지 날짜를 설정하고, 해당 날짜의 데이터에 대한 QC 템플릿을 생성합니다. 사용자는 링크를 클릭해서 QC 템플릿 파일을 다운로드 할 수 있습니다.\n   \u0026amp;nbsp\n각 데이터에 대해 감성분석 모델이 1(긍정) , 0(부정)으로 분류한 결과를 검토하는 QC 템플릿 파일입니다. 사용자는 QC 템플릿 파일 내 qc_result 칼럼에 ACCEPT/REJECT를 입력하며 QC 작업을 수행합니다.\n   3.3. QC 템플릿 업로드 페이지 업로드 페이지에서 사용자는 QC 완료 파일을 업로드 합니다. 이 과정에서는 .xlsx 확장자인 파일만 올릴 수 있도록 하고, 업로드 파일명이 다른 형태를 띄고 있다면 업로드를 제한하는 기능을 포함하고 있습니다.\n그리고 qc_result 컬럼에 기입하지 않은 blank cell이 있는지, ACCEPT/REJECT 이 아닌 다른 값이 입력되진 않았는지 검증 작업을 거쳐 QC Result Validity Test를 통해 사용자에게 패스 여부를 알려줍니다. 검증 작업을 패스하지 못 한다면, 파일을 업로드할 수 없습니다.\n   \u0026amp;nbsp\n마지막으로 사용자는 QC 완료 결과를 다시 한번 검토할 수 있으며, 몇 가지 셀프 체크와 파일을 업로드 하는 본인이 누구인지 선택한 후, QC 완료 파일을 업로드할 수 있습니다.\n   \u0026amp;nbsp\n4. 후기 이런 식으로 개발한 QC툴은 현재도 사내에서 사용 중에 있습니다. 이 대신 잇몸이라는 말처럼, 정식으로 개발한 툴은 아니지만, 불편했던 점을 어느 정도 해소하고, 리소스와 시간을 절약할 수 있게 된 부분이 가장 큰 추후 제대로된 QC툴이나 시스템을 구축할 때 개발팀에서 참조할 수 있는 프로토타입의 역할도 하고 있습니다.\n언뜻 보면 간단한 것들도 실제로 개발하고 구현하려면, 밑바닥에서부터 새로 배워야 하는 것들이 많아서 예전에는 엄두가 안 났었습니다만, Streamlit이 아주 간편해서 앞으로도 급하게 프로토타이핑을 할 때 자주 사용할 것 같습니다. 빛과소금 또트림릿\n5.Source code  Github  6.Reference  Streamlit Documentation Streamlit Discussion NSMC Dataset  ","permalink":"https://wonyoungseo.github.io/posts/2021-03-06-streamlit-simple-tool-development/","summary":"이번 포스트에서는 현재 일하고 있는 Who\u0026rsquo;s Good에서 웹어플리케이션 기반의 간단한 툴을 개발한 과정을 기술합니다. 자세한 사내 업무 내용은 생략하며, 유사한 상황 및 시나리오로 대체했음을 밝힙니다.\n \u0026amp;nbsp\n1. Quality Check (QC) 현재 일하고 있는 Who\u0026rsquo;s Good에서는 뉴스 기사를 기반으로 기업의 *ESG 리스크를 평가합니다. 모델을 통해 산출된 결과에 대해서는 꼭 검토 및 검증 프로세스를 거치는데요, 산출된 결과값이 정답인지 아닌지 여부를 확인하는 과정이고, Quality Check 또는 줄여서 QC라고 합니다. 주로 ESG 도메인의 전문가인 ESG 리서쳐 또는 QC 스태프가 이 과업을 수행합니다.","title":"[KR] 파이썬 기반 데이터 QC 툴 개발기(근데 이제 또 Streamlit을 곁들인)"},{"content":"1. 트리의 개념 1.1. 트리의 정의  노드와 브랜치를 활용하여 구성한 데이터 구조  1.2. 트리와 관련된 용어 트리 관련 용어  노드(Node)  데이터를 저장하는 기본 요소 다른 노드와 연결되는 브랜치에 대한 정보도 포함   브랜치(Branch)  상위 노드와 하위 노드를 연결하는 가지   루트노드(Root Node)  트리 최상단에 위치한 최상위 노드   레벨(Level)  최상위 노드를 Level 0이라고 할 때, 특정 레벨에 위치한 노드의 집합   부모 노드(Parent Node)  상위 노드   자식 노드(Child Node)  하위 노드   단말 노드(Leaf Node)  하위 노드가 없는 노드   형제 노드(Sibling Node)  동일한 부모 노드를 가진 노드   깊이(Depth)  루트에서 어떤 노드에 도달하기 위해 거쳐야 하는 간선의 수   크기(Size)  자신을 포함한 모든 자식노드의 개수   높이(Height)  하위 트리 개수 / 간선 수 (degree) = 각 노드가 지닌 가지의 수   노드의 차수(Degree of Node)  각 노드가 지닌 가지의 수   트리의 차수(Degree of Tree)  트리의 최대 차수    트리의 종류  이진 트리 vs 이진 탐색 트리  이진 트리(Binary Tree)  노드의 최대 브랜치가 2개인 트리   이진 탐색 트리(Binary Search Tree: BST)  왼쪽 노드는 해당 노드보다 작은 값, 오른쪽 노드는 해당 노드보다 큰 값을 가지는 조건이 적용된 이진트리      1.3. 트리의 적용 예시  주요 용도: 데이터 검색(탐색)  1.4. 이진 탐색 트리의 시간복잡도  트리의 높이(Depth)를 \\(h\\)라고 표기한다면, 시간복잡도는 \\(O(h)\\) 노드의 개수가 \\(n\\)일 때,  트리의 높이 \\(h = \\log_2n\\) 에 가까움. 시간 복잡도는 \\(O(\\log n)\\) -\u0026gt; (빅오 표기법에서 \\(\\log\\)의 밑은 2) 한번 판단할 때마다, 50%씩 탐색할 후보를 제외할 수 있음. 시간이 단축 됨.    1.5. 이진 탐색 트리(BST)의 장단점  장점: 탐색 속도를 개선할 수 있음 단점:  평균 시간 복잡도는 \\(O(\\log n)\\)이지만, 이는 트리의 양쪽이 모두 균등할 때의 평균 시간복잡도라고 할 수 있음. 따라서, 트리가 한쪽으로만 치우쳐져 있는 최악의 경우에는 링크드리스트와 동일한 \\(O(n)\\)의 성능을 보여줌.    2. 파이썬을 통한 기본적인 이진탐색트리 구현  노드와 그 다음 노드를 연결된 형태를 띄기 때문에, 링크드 리스트로 구현하면 용이함.  2.1. 노드 클래스 class Node: def __init__(self, value): self.value = value self.left = None self.right = None 2.2. 최상단에 위치한 head 설정 def __init__(self, head): self.head = head 2.3. 이진탐색트리에 데이터 저장 (insert) def insert(self, value): self.current_node = self.head # 각 노드를 순회 while True: if value \u0026lt; self.current_node.value: #현재노드보다 작은 경우 : 왼쪽 가지로 이동 if self.current_node.left != None: # 왼쪽 가지로 이동했을 때, 이미 데이터 노드가 있다면 self.current_node = self.current_node.left # 비교대상 노드를 왼쪽가지 노드로 교체 else: # 데이터 노드가 없다면 self.current_node.left = Node(value) # value를 가지는 새로운 노드를 만들어서 왼쪽 가지에 삽입 break else: # 오른쪽도 동일함. if self.current_node.right != None: self.current_node = self.current_node.right else: self.current_node.right = Node(value) break 2.4. 이진 탐색 트리의 탐색 (search) def search(self, value): self.current_node = self.head # HEAD 노드에서부터 찾고자 하는 노드를 순회하며 찾음 while self.current_node: if self.current_node.value == value: return True elif value \u0026lt; self.current_node.value: self.current_node = self.current_node.left else: self.current_node = self.current_node.right assert False, \u0026#34;Value does not exist\u0026#34; 2.3. 이진 탐색 트리의 삭제 (delete)  노드 삭제의 경우  삭제할 노드에 브랜치가 없을 때 : Leat Node 삭제 삭제할 노드에 브랜치가 한 개 있을 때 : Childe Node가 하나인 노드 삭제 삭제할 노드에 브랜치가 두 개 있을 때 : Childe Node가 둘인 노드 삭제    2.3.1. Case1: Leaf Node 삭제  삭제할 Node의 Parent Node가 삭제할 Node를 가리키지 않게 함.  2.3.2. Case2: Childe Node가 하나인 노드 삭제  삭제할 Node의 Parent Node가 삭제할 Node의 Child Node를 가리키게 함.  2.3.3. Case3: Childe Node가 둘인 노드 삭제  구현 방식  삭제할 Node의 오른쪽 자식들 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 함.  삭제할 노드의 오른쪽 자식 선택  오른쪽 자식의 가장 왼쪽에 있는 노드를 선택  해당 노드를 삭제할 노드의 부모 노드의 왼쪽 브랜치가 가리키게 연결 해당 노드의 왼쪽 브랜치가 삭제할 노드의 왼쪽 자식 노드를 가리키게 함 해당 노드의 오른쪽 브랜치가 삭제할 노드의 오른쪽 자식 노드를 가리키게 함  만약 해당 노드가 오른쪽 자식 노드를 가지고 있었을 경우, 해당 노드의 본래 부모 노드의 왼쪽 브랜치가 해당 노드의 오른쪽 자식 노드를 가리키게 함            Case3-1: 삭제할 노드가 부모 노드의 왼쪽에 있을 때\n 삭제할 Node의 오른쪽 자식들 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 함.  Case3-1-1: 삭제할 노드가 부모 노드의 왼쪽에 있고, 삭제할 노드의 오른쪽 자식 중 가장 작은 값을 가진 노드의 자식노드가 없을 때. Case3-1-2: 삭제할 노드가 부모 노드의 왼쪽에 있고, 삭제할 노드의 오른쪽 자식 중 가장 작은 값을 가진 노드의 오른쪽에 자식 노드가 있을 때  노드의 왼쪽으로는 더 작은 값을 가진 노드가 존재하기 때문에, 가장 작은 값을 가진 노드의 자식노드가 왼쪽에 있을 경우는 없음.      Case3-2: 삭제할 노드가 부모 노드의 오른쪽에 있을 때\n 삭제할 Node의 오른쪽 자식들 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 함.  Case3-2-1: 삭제할 노드가 부모 노드의 오른쪽에 있고, 삭제할 노드의 오른쪽 자식 중, 가장 작은 값을 가진 노드의 자식 노드가 없을 때 Case3-2-2: 삭제할 노드가 부모 노드의 오른쪽에 있고, 삭제할 노드의 오른쪽 자식 중, 가장 작은 값을 가진 노드의 오른 쪽에 자식 노드가 있을 때.  노드의 왼쪽으로는 더 작은 값을 가진 노드가 존재하기 때문에, 가장 작은 값을 가진 노드의 자식노드가 왼쪽에 있을 경우는 없음.      2.3.4. 삭제 코드 구현 def delete(self, value): searched = False # 삭제할 노드가 있는지 판단하는 boolean self.current_node = self.head # 현재 노드 선언 self.parent = self.head # 부모 노드 선언 while self.current_node: if self.current_node.value == value: # 삭제하고자 하는 노드를 찾았다면 searched = True # 삭제할 노드가 있다고 판단함 break elif value \u0026lt; self.current_node.value: self.parent = self.current_node self.current_node = self.current_node.left else: self.parent = self.current_node self.current_node = self.current_node.right if searched == False: assert searched, \u0026#34;Node does not exist\u0026#34; # Case 1 if (self.current_node.left == None) and self.current_node.right == None: # Leaf Node if value \u0026lt; self.parent.value: # 부모노드의 왼쪽일 경우 self.parent.left = None else: # 부모노드의 오른쪽일 경우 self.parent.right = None del self.current_node # Case 2-1 # 삭제할 노드가 왼쪽에 자식노드 한 개를 가지고 있을 경우 elif (self.current_node.left != None) and self.current_node.right == None: if value \u0026lt; self.parent.value: self.parent.left = self.current_node.left else: self.parent.right = self.current_node.left # Case 2-2 # 삭제할 노드가 오른쪽에 자식노드 한 개를 가지고 있을 경우 elif (self.current_node.left == None) and self.current_node.right != None: if value \u0026gt; self.parent: self.parent.left = self.current_node.right else: self.parent.right = self.current_node.left # Case 3: 삭제할 노드에 브랜치가 좌우로 존재할 때.  elif (self.current_node.left != None) and self.current_node.right != None: # Case 3-1: if value \u0026lt; self.parent_value: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.chage_node.left if self.chage_node.right != None: # Case 3-1-2 self.change_node_parent.left = self.change_node.right else: # Case 3-1-1 self.change_node_parent.left = None # 삭제 대상 노드의 부모/자식 노드 간 연결을 끊고, change_node로 대체함.  self.parent.left = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.elft # Case 3-2: else: #value \u0026gt; self.parent_value self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: # Case 3-2-2 self.change_node_parent.left = self.change_node_parent.right else: # Case 3-2-1 self.change_node_parent.left = None # 삭제 대상 노드의 부모/자식 노드 간 연결을 끊고, change_node로 대체함.  self.parent.right = self.change_node self.change_node.left = self.current_node.left self.change_node.right = self.current_node.right 2.4. 전체 코드 class Node: def __init__(self, value): self.value = value self.left = None self.right = None class NodeManagement: def __init__(self, head): self.head = head def insert(self, value): self.current_node = self.head while True: if value \u0026lt; self.current_node.value: if self.current_node.left != None: self.current_node = self.current_node.left else: self.current_node.left = Node(value) break else: if self.current_node.right != None: self.current_node = self.current_node.right else: self.current_node.right = Node(value) break def search(self, value): self.current_node = self.head while self.current_node: if self.current_node.value == value: return True elif value \u0026lt; self.current_node.value: self.current_node = self.current_node.left else: self.current_node = self.current_node.right assert False, \u0026#34;Number does not exist.\u0026#34; def delete(self, value): searched = False self.current_node = self.head self.parent = self.head while self.current_node: if self.current_node.value == value: searched = True break elif value \u0026lt; self.current_node.value: self.parent = self.current_node self.current_node = self.current_node.left else: self.parent = self.current_node self.current_node = self.current_node.right if searched == False: assert False, \u0026#34;Number does not exist.\u0026#34; if (self.current_node.left == None) and self.current_node.right == None: if value \u0026lt; self.parent.value: self.parent.left = None else: self.parent.right = None elif (self.current_node.left != None) and self.current_node.right == None: if value \u0026lt; self.parent.value: self.parent.left = self.current_node.left else: self.parent.right = self.current_node.left elif (self.current_node.left == None) and self.current_node.right != None: if value \u0026gt; self.parent.value: self.parent.left = self.current_node.right else: self.parent.right = self.current_node.left elif (self.current_node.left != None) and self.current_node.right != None: if value \u0026lt; self.parent.value: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.chage_node.left if self.chage_node.right != None: self.change_node_parent.left = self.change_node.right else: self.change_node_parent.left = None self.parent.left = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.elft else: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: self.change_node_parent.left = self.change_node_parent.right else: self.change_node_parent.left = None self.parent.right = self.change_node self.change_node.left = self.current_node.left self.change_node.right = self.current_node.right return True 데이터 입력 import random nums = set() while len(nums) != 100: nums.add(random.randint(0, 999)) print(nums) {512, 518, 525, 526, 536, 25, 539, 551, 53, 57, 570, 61, 62, 576, 577, 592, 597, 87, 600, 90, 602, 619, 110, 112, 113, 124, 130, 643, 133, 654, 143, 663, 154, 670, 160, 690, 188, 204, 209, 723, 212, 226, 232, 747, 748, 237, 754, 759, 248, 251, 259, 262, 775, 774, 272, 273, 274, 275, 277, 278, 791, 796, 798, 809, 815, 307, 829, 325, 332, 333, 847, 851, 852, 853, 355, 358, 888, 376, 894, 909, 400, 913, 912, 409, 929, 930, 422, 936, 425, 426, 446, 458, 466, 986, 477, 996, 488, 495, 499, 500}  head = Node(500) bst = NodeManagement(head) for num in nums: bst.insert(num) 데이터 탐색 # 탐색 대상 노드가 존재하는 경우 bst.search(643) True # 탐색 대상 노드가 존재하지 않는 경우 bst.search(879) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) \u0026lt;ipython-input-10-03b987c0a6aa\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 bst.search(879) \u0026lt;ipython-input-6-bbcc66356cab\u0026gt; in search(self, value) 39 self.current_node = self.current_node.right 40 ---\u0026gt; 41 assert False, \u0026quot;Number does not exist.\u0026quot; 42 43 AssertionError: Number does not exist. 데이터 삭제 # 숫자 10개 랜덤 선택 target_nums = set() while len(target_nums) != 10: target_nums.add(list(nums)[random.randint(0, 99)]) print(target_nums)  {160, 325, 518, 332, 525, 912, 62, 57, 602, 25}  for delete_num in target_nums: bst.delete(delete_num) 4. Reference  잔재미코딩  ","permalink":"https://wonyoungseo.github.io/posts/2021-03-01-python-datastructure-tree-bst/","summary":"1. 트리의 개념 1.1. 트리의 정의  노드와 브랜치를 활용하여 구성한 데이터 구조  1.2. 트리와 관련된 용어 트리 관련 용어  노드(Node)  데이터를 저장하는 기본 요소 다른 노드와 연결되는 브랜치에 대한 정보도 포함   브랜치(Branch)  상위 노드와 하위 노드를 연결하는 가지   루트노드(Root Node)  트리 최상단에 위치한 최상위 노드   레벨(Level)  최상위 노드를 Level 0이라고 할 때, 특정 레벨에 위치한 노드의 집합   부모 노드(Parent Node)  상위 노드   자식 노드(Child Node)  하위 노드   단말 노드(Leaf Node)  하위 노드가 없는 노드   형제 노드(Sibling Node)  동일한 부모 노드를 가진 노드   깊이(Depth)  루트에서 어떤 노드에 도달하기 위해 거쳐야 하는 간선의 수   크기(Size)  자신을 포함한 모든 자식노드의 개수   높이(Height)  하위 트리 개수 / 간선 수 (degree) = 각 노드가 지닌 가지의 수   노드의 차수(Degree of Node)  각 노드가 지닌 가지의 수   트리의 차수(Degree of Tree)  트리의 최대 차수    트리의 종류  이진 트리 vs 이진 탐색 트리  이진 트리(Binary Tree)  노드의 최대 브랜치가 2개인 트리   이진 탐색 트리(Binary Search Tree: BST)  왼쪽 노드는 해당 노드보다 작은 값, 오른쪽 노드는 해당 노드보다 큰 값을 가지는 조건이 적용된 이진트리      1.","title":"[KR] 자료구조 \u0026 알고리즘 : 트리(Tree)"},{"content":"SQL Practices - SELECT Practices set resources are referenced from w3resource.com\n SQL Exercises, Practice, Solution - Retrieve data from tables SQL Exercises, Practice, Solution - Using Boolean and Relational operators All practice set and answers are written in PostgreSQL   Sample Tables salesman    salesman_id name city commission     5001 James Hoog New York 0.15   5002 Nail Knite Paris 0.13   5005 Pit Alex London 0.11   5006 Mc Lyon Paris 0.14   5007 Paul Adam Rome 0.13   5003 Lauson Hen San Jose 0.12    \u0026amp;nbsp\norders    ord_no purch_amt ord_date customer_id salesman_id     70001 150.5 2012-10-05 3005 5002   70009 270.65 2012-09-10 3001 5005   70002 65.26 2012-10-05 3002 5001   70004 110.5 2012-08-17 3009 5003   70007 948.5 2012-09-10 3005 5002   70005 2400.6 2012-07-27 3007 5001   70008 5760 2012-09-10 3002 5001   70010 1983.43 2012-10-10 3004 5006   70003 2480.4 2012-10-10 3009 5003   70012 250.45 2012-06-27 3008 5002   70011 75.29 2012-08-17 3003 5007   70013 3045.6 2012-04-25 3002 5001    \u0026amp;nbsp\ncustomer    customer_id cust_name city grade salesman_id     3002 Nick Rimando New York 100 5001   3007 Brad Davis New York 200 5001   3005 Graham Zusi California 200 5002   3008 Julian Green London 300 5002   3004 Fabian Johnson Paris 300 5006   3009 Geoff Cameron Berlin 100 5003   3003 Jozy Altidor Moscow 200 5007   3001 Brad Guzan London  5005    \u0026amp;nbsp\nnobel_win    year subject winner country category     1970 Physics Hannes Alfven Sweden Scientist   1970 Physics Louis Neel France Scientist   1970 Chemistry Luis Federico Leloir France Scientist   1970 Physiology Julius Axelrod USA Scientist   1970 Physiology Ulf von Euler Sweden Scientist   1970 Physiology Bernard Katz Germany Scientist   1970 Literature Aleksandr Solzhenitsyn Russia Linguist   1970 Economics Paul Samuelson USA Economist   1971 Physics Dennis Gabor Hungary Scientist   1971 Chemistry Gerhard Herzberg Germany Scientist   1971 Peace Willy Brandt Germany Chancellor   1971 Literature Pablo Neruda Chile Linguist   1971 Economics Simon Kuznets Russia Economist   1978 Peace Anwar al-Sadat Egypt President   1978 Peace Menachem Begin Israel Prime Minister   1994 Peace Yitzhak Rabin Israel Prime Minister   1987 Physics Johannes Georg Bednorz Germany Scientist   1987 Chemistry Donald J. Cram USA Scientist   1987 Chemistry Jean-Marie Lehn France Scientist   1987 Physiology Susumu Tonegawa Japan Scientist   1987 Literature Joseph Brodsky Russia Linguist   1987 Economics Robert Solow USA Economist   1994 Literature Kenzaburo Oe Japan Linguist   1994 Economics Reinhard Selten Germany Economist    \u0026amp;nbsp\nitem_mast    pro_id pro_name pro_price pro_com     101 Mother Board 3200.00 15   102 Key Board 450.00 16   103 ZIP drive 250.00 14   104 Speaker 550.00 16   105 Monitor 5000.00 11   106 DVD drive 900.00 12   107 CD drive 800.00 12   108 Printer 2600.00 13   109 Refill cartridge 350.00 13   110 Mouse 250.00 12    \u0026amp;nbsp\nemp_details    emp_idno emp_fname emp_lname emp_dept     631548 Alan Snappy 27   839139 Maria Foster 57   127323 Michale Robbin 57   526689 Carlos Snares 63   843795 Enric Dosio 57   328717 Jhon Snares 63   444527 Joseph Dosni 47   659831 Zanifer Emily 47   847674 Kuleswar Sitaraman 57   748681 Henrey Gabriel 47   555935 Alex Manuel 57   539569 George Mardy 27   733843 Mario Saule 63     Practice Questions 1. Simple Select 1.1. Write a SQL statement to display all the information of all salesmen. SELECT * FROM salesman; 1.2. Write a SQL statement to display specific columns like name and commission for all the salesmen. SELECT name, commission FROM salesman; 1.3. Write a query to display the columns in a specific order like order date, salesman id, order number and purchase amount from for all the orders. SELECT ord_date, salesman_id, ord_no, purch_amt FROM orders; 1.4. Write a query which will retrieve the value of salesman id of all salesmen, getting orders from the customers in orders table without any repeats. SELECT DISTINCT salesman_id FROM orders; 1.5. Write a SQL statement to display names and city of salesman, who belongs to the city of Paris. SELECT name, city FROM salesman WHERE city = \u0026#39;Paris\u0026#39;; 1.6. Write a SQL statement to display all the information for those customers with a grade of 200. SELECT * FROM Customer WHERE grade=200; 1.7. Write a SQL query to display the order number followed by order date and the purchase amount for each order which will be delivered by the salesman who is holding the ID 5001. SELECT ord_date, ord_no, purch_amt FROM orders WHERE salesman_id=5001; 1.8. Write a SQL query to display the Nobel prizes for 1970. SELECT * FROM nobel_win WHERE YEAR=1970; -- different interpretation of instruction SELECT year,subject,winner FROM nobel_win WHERE year=1970; 1.9. Write a SQL query to know the winner of the 1971 prize for Literature. SELECT winner, country FROM nobel_win WHERE year=1971 AND subject=\u0026#39;Literature\u0026#39;; 1.10. Write a SQL query to display the year and subject that won \u0026lsquo;Dennis Gabor\u0026rsquo; his prize. SELECT year, subject FROM nobel_win WHERE winner=\u0026#39;Dennis Gabor\u0026#39;; 1.11. Write a SQL query to give the name of the \u0026lsquo;Physics\u0026rsquo; winners since the year 1950. SELECT winner FROM nobel_win WHERE subject=\u0026#39;Physics\u0026#39; AND year\u0026gt;=1950; 1.12. Write a SQL query to Show all the details (year, subject, winner, country ) of the Chemistry prize winners between the year 1965 to 1975 inclusive. SELECT year, subject, winner, counter FROM nobel_win WHERE subect=\u0026#39;Chemistry\u0026#39; AND year\u0026gt;=1965 AND year\u0026lt;=1975; 1.13. Write a SQL query to show all details of the Prime Ministerial winners after 1972 of Menachem Begin and Yitzhak Rabin. SELECT * FROM nobel_win WHERE category=\u0026#39;Prime Minister\u0026#39; AND year \u0026gt; 1972; -- different interpretation of instruction SELECT * FROM nobel_win WHERE year \u0026gt;1972 AND winner IN (\u0026#39;Menachem Begin\u0026#39;, \u0026#39;Yitzhak Rabin\u0026#39;); 1.14. Write a SQL query to show all the details of the winners with first name Louis. SELECT * FROM nobel_win WHERE winner LIKE \u0026#39;Louse %\u0026#39;; 1.15. Write a SQL query to show all the winners in Physics for 1970 together with the winner of Economics for 1971. SELECT * FROM nobel_win WHERE subject=\u0026#39;Physics\u0026#39; AND year=1970 UNION SELECT * FROM nobel_win WHERE subject=\u0026#39;Economics\u0026#39; AND year=1971; 1.16. Write a SQL query to show all the winners of nobel prize in the year 1970 except the subject Physiology and Economics. SELECT * FROM nobel_win WHERE year=1970 AND subject NOT IN (\u0026#39;Physiology\u0026#39;, \u0026#39;Economics\u0026#39;); 1.17. Write a SQL query to show the winners of a \u0026lsquo;Physiology\u0026rsquo; prize in an early year before 1971 together with winners of a \u0026lsquo;Peace\u0026rsquo; prize in a later year on and after the 1974. SELECT * FROM nobel_win WHERE subject=\u0026#39;Pysiology\u0026#39; AND year\u0026lt;1971 UNION SELECT * FROM nobel_win WHERE subject=\u0026#39;Peace\u0026#39; AND year\u0026gt;1974; 1.18. Write a SQL query to find all details of the prize won by Johannes Georg Bednorz. SELECT * FROM nobel_win WHERE winner=\u0026#39;Johannes Georg Bednorz; 1.19. Write a SQL query to find all the details of the nobel winners for the subject not started with the letter \u0026lsquo;P\u0026rsquo; and arranged the list as the most recent comes first, then by name in order. SELECT * FROM nobel_win WHERE subject NOT LIKE \u0026#39;P%\u0026#39; ORDER BY year DESC, winner; 1.20. Write a SQL query to find all the details of 1970 winners by the ordered to subject and winner name; but the list contain the subject Economics and Chemistry at last. SELECT * FROM nobel_win WHERE year=1970 ORDER BY CASE WHEN subject IN (\u0026#39;Economics\u0026#39;, \u0026#39;Chemistry\u0026#39;) THEN 1 ELSE 0 END ASC, subject, winnerl; 1.21. Write a SQL query to find all the products with a price between Rs.200 and Rs.600. SELECT * FROM item_mast WHERE pro_price\u0026gt;=200 AND pro_price\u0026lt;=600; -- better approach SELECT * FROM item_mast WHERE pro_price BETWEEN 200 AND 600; 1.22. Write a SQL query to calculate the average price of all products of the manufacturer which code is 16. SELECT AVG(pro_price) FROM item_mast WHERE pro_com=16; 1.23. Write a SQL query to find the item name and price in Rs. SELECT pro_name AS \u0026#39;item name\u0026#39;, pro_price AS \u0026#39;price in Rs\u0026#39; FROM item_mast; 1.24. Write a SQL query to display the name and price of all the items with a price is equal or more than Rs.250, and the list contain the larger price first and then by name in ascending order. SELECT pro_name, pro_price FOM item_mast WHERE pro_price\u0026gt;=250 ORDER BY pro_price desc, pro_name; 1.25. Write a SQL query to display the average price of the items for each company, showing only the company code. SELECT AVG(pro_price), pro_com FROM item_mast GROUP BY pro_com; 1.26. Write a SQL query to find the name and price of the cheapest item(s). SELECT pro_name, pro_price FROM item_mast WHERE pro_price = (SELECT MIN(pro_price) from item_mast); 1.27. Write a query in SQL to find the last name of all employees, without duplicates. SELECT DISTINCT emp_lname FROM emp_details; 1.28. Write a query in SQL to find the data of employees whose last name is \u0026lsquo;Snares\u0026rsquo;. SELECT * FROM emp_details WHERE emp_lname=\u0026#39;Snares\u0026#39;; 1.29. Write a query in SQL to display all the data of employees that work in the department 57. SELECT * FROM emp_details WHERE emp_dept=57; 2. Boolean \u0026amp; Relational Operators 2.1. Write a query to display all customers with a grade above 100. SELECT * FROM customer WHERE grade \u0026gt; 100; 2.2. Write a query statement to display all customers in New York who have a grade value above 100. SELECT * FROM customer WHERE city = \u0026#39;New York\u0026#39; AND grade \u0026gt; 100; 2.3. Write a SQL statement to display all customers, who are either belongs to the city New York or had a grade above 100. SELECT * FROM customers WHERE city = \u0026#39;New York\u0026#39; OR grade \u0026gt; 100; 2.4. Write a SQL statement to display all the customers, who are either belongs to the city New York or not had a grade above 100. SELECT * FROM customer WHERE city = \u0026#39;New York\u0026#39; OR grade \u0026lt;= 100; -- other approach SELECT * FROM customer WHERE city = \u0026#39;New York\u0026#39; OR NOT grade \u0026gt; 100; 2.5. Write a SQL query to display those customers who are neither belongs to the city New York nor grade value is more than 100. SELECT * FROM customer WHERE city NOT \u0026#39;New York\u0026#39; AND NOT grade \u0026gt; 100; -- other approach SELECT * FROM customer WHERE NOT (city = \u0026#39;New York\u0026#39; AND grade \u0026gt; 100); 2.6. Write a SQL statement to display either those orders which are not issued on date 2012-09-10 and issued by the salesman whose ID is 5005 and below or those orders which purchase amount is 1000.00 and below. SELECT * FROM orders WHERE NOT ( (ord_date = \u0026#39;2012-09-10\u0026#39; AND salesman_id \u0026gt; 5005) OR purch_amt \u0026gt; 1000.00 ); 2.7. Write a SQL statement to display salesman_id, name, city and commission who gets the commission within the range more than 0.10% and less than 0.12%. SELECT salesman_id, name, city, commission FROM salesman WHERE commission \u0026gt; 0.10 AND commission \u0026lt; 0.12; 2.8. Write a SQL query to display all orders where purchase amount less than 200 or exclude those orders which order date is on or greater than 10th Feb,2012 and customer id is below 3009. SELECT * FROM orders WHERE ( purch_amt \u0026lt; 200 OR NOT(ord_date \u0026gt;= \u0026#39;2012-02-10\u0026#39; AND customer_id \u0026lt; 3009) ); 2.9. Write a SQL statement to exclude the rows which satisfy 1) order dates are 2012-08-17 and purchase amount is below 1000 2) customer id is greater than 3005 and purchase amount is below 1000. SELECT * FROM orders WHERE NOT ( (ord_date = \u0026#39;2012-08-17\u0026#39; OR customer_id \u0026gt; 3005 ) AND purch_amt \u0026lt; 1000 ); 2.10. Write a SQL query to display order number, purchase amount, achieved, the unachieved percentage for those order which exceeds the 50% of the target value of 6000. SELECT ord_no, purch_amt, (purch_amt / 6000 * 100) AS \u0026#34;Achieved%\u0026#34;, ((6000-purch_amt) / 6000 * 100) AS \u0026#34;Unachieved%\u0026#34; FROM orders WHERE (purch_amt / 6000 * 100) \u0026gt; 50; 2.11. Write a query in SQL to find the data of employees whose last name is Dosni or Mardy. SELECT * FROM emp_details WHERE emp_lname = \u0026#39;Dosni\u0026#39; OR emp_lname = \u0026#39;Mardy\u0026#39;; 2.12. Write a query in SQL to display all the data of employees that work in department 47 or department 63. SELECT * FROM emp_details WHERE emp_dept = 47 OR emp_dept = 63; ","permalink":"https://wonyoungseo.github.io/posts/2021-02-28-sql-practice-select/","summary":"SQL Practices - SELECT Practices set resources are referenced from w3resource.com\n SQL Exercises, Practice, Solution - Retrieve data from tables SQL Exercises, Practice, Solution - Using Boolean and Relational operators All practice set and answers are written in PostgreSQL   Sample Tables salesman    salesman_id name city commission     5001 James Hoog New York 0.15   5002 Nail Knite Paris 0.13   5005 Pit Alex London 0.","title":"[EN] SQL Practice : select"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 해쉬 테이블 (Hash Table) 1.1. 해쉬 테이블의 구조   키(Key)에 데이터(Value)가 매핑되어 저장되어 있는 구조\n Key를 통해 데이터를 바로 받아올 수 있으므로, 속도가 빠름 파이썬에서는 딕셔너리(Dictionary)가 해쉬 테이블의 예시.  dict = {\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}   1.2. 해쉬 테이블의 용어  해쉬(Hash)  임의의 값을 고정된 길이로 변환하는 것   해쉬 테이블(Hash Table)  Key값의 연산에 의해 직접 접근이 가능한 데이터 구조   해싱 함수(Hashing Function)  Key에 대해 특정 산술 연산을 이용하여 데이터의 위치(해쉬 주소)가 리턴되는 함수   해쉬 값(Hash Value) 또는 해쉬 주소(Hash Address)  Key를 해싱 함수로 연산하여 얻는 값 Key를 해싱 함수로 연산하여 해쉬 값이 데이터의 위치.   슬롯(Slot)  한 개의 데이터를 저장할 수 있는 공간    1.3. 해시테이블의 장단점  장점  데이터 저장/읽기 속도가 빠름 특히 검색 속도가 빠름 Key에 대한 데이터가 있는지 확인이 쉬움. 중복 처리 및 확인이 쉬움   단점  일반적으로 저장공간이 많이 요구됨. 공간효윬어이 떨어짐.  해시함수에 따른 값이 저장될 공간이 확보되어야 하기 때문   해싱 함수로 인해 연산된 해시값/해시주소가 동일한 경우 충돌을 해결해야 함.  해시함수에 대한 의존도가 높음. 따라서 별도의 자료구조가 요구됨.      1.4. 해시테이블의 용도  검색이 많이 필요한 경우 저장, 삭제, 읽기가 빈번한 경우 캐쉬 구현  이미 데이터가 캐시에 있는지 없는지 중복확인을 할 때 해시테이블이 용이하게 적용됨.    \u0026amp;nbsp\n2. 파이썬을 통한 해쉬함수 이해 2.1. Hash Table 2.1.1. Slot hash_table = list([i for i in range(10)]) hash_table [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 2.1.2. Hash Function  해쉬 함수에는 다양한 기법들이 있음. Division 방식은 가장 기본적인 형태로 알려져 있음.  특정 값으로 나눈 후 나머지 값을 이용하는 기법.    def hash_func(key): return key % 5 2.1.3. 데이터 준비 data1 = \u0026#39;Andy\u0026#39; data2 = \u0026#39;Dave\u0026#39; data3 = \u0026#39;Trump\u0026#39; data4 = \u0026#39;Anthor\u0026#39; ## ord(): 문자의 ASCII(아스키)코드 리턴 print (ord(data1[0]), ord(data2[0]), ord(data3[0])) print (ord(data1[0]), hash_func(ord(data1[0]))) print (ord(data1[0]), ord(data4[0])) 65 68 84 65 0 65 65 2.1.4. 데이터 저장 def storage_data(data, value): index_key = ord(data[0]) hash_address = hash_func(index_key) hash_table[hash_address] = value storage_data(\u0026#39;Andy\u0026#39;, \u0026#39;01055553333\u0026#39;) storage_data(\u0026#39;Dave\u0026#39;, \u0026#39;01044443333\u0026#39;) storage_data(\u0026#39;Trump\u0026#39;, \u0026#39;01022223333\u0026#39;) print(hash_table) ['01055553333', 1, 2, '01044443333', '01022223333', 5, 6, 7, 8, 9] 2.1.4. 데이터 읽기 def get_data(data): key = ord(data[0]) hash_address = hash_func(key) return hash_table[hash_address] get_data(\u0026#39;Andy\u0026#39;) '01055553333' \u0026amp;nbsp\n3. 파이썬 예시  해시함수를 다르게 설정하고 일괄적으로 저장, 추출까지 설정했을 때.  해시함수 : key % 8    hash_table = list([0 for i in range(10)]) def hash_function(key): return key \u0026amp; 8 def get_key(data): return hash(data) def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) hash_table[hash_address] = value def read_data(data): hash_address = hash_function(get_key(data)) return hash_table[hash_address] # 데이터 저장 save_data(\u0026#39;Dave\u0026#39;, \u0026#39;01020302000\u0026#39;) save_data(\u0026#39;Andy\u0026#39;, \u0026#39;01033232200\u0026#39;) print(hash_table) ['01033232200', 0, 0, 0, 0, 0, 0, 0, 0, 0] # 데이터 읽기 read_data(\u0026#39;Dave\u0026#39;) '01033232200' \u0026amp;nbsp\n4. 해시 충돌(Hash Collision) 처리를 위한 문제 접근 방법 4.1. Separate Chaining 방식  해시 테이블 저장공간 이외의 공간을 활용함 충돌이 일어났을 때, 데이터를 뒤에 추가로 저장.  이 때, 다양한 자료구조를 활용하며 Linked List가 하나의 예가 될 수 있음. (Separate chaining with linked list)    4.1.1. Separate chaining with Linked List  데이터 저장 시, 동일한 hash_address가 존재하여 충돌이 발생하면, Linked list에 노드를 추가하여 값을 추가함. (파이썬으로는 일반 리스트로 구현함) 데이터 추출 시, hash_address에 대하여 선형 탐색하며, 해당 key에 대한 데이터를 검색 후 결과를 리턴함.  def get_key(data): return hash(data) def hash_function(key): return key % 8 def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) # 이미 Hash table의 공간이 차있어 충돌이 발생할 경우 if hash_table[hash_address] != 0: for index in range(len(hash_table[hash_address])): if hash_table[hash_address][index][0] == index_key: hash_table[hash_address][index][1] == value return None hash_table[hash_address].append([index_key, value]) # 새로 추가 else: hash_table[hash_address] = [[index_key, value]] def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(len(hash_table[hash_address])): if hash_table[hash_address][index][0] == index_key: return hash_table[hash_address][index][1] # value return return None else: return None return hash_table[hash_address] hash_table = list([0 for i in range(10)]) # pring hashed index_key for each data print(hash_function(get_key(\u0026#39;Dash\u0026#39;))) print(hash_function(get_key(\u0026#39;Donald\u0026#39;))) print(hash_function(get_key(\u0026#39;Dave\u0026#39;))) print(hash_function(get_key(\u0026#39;David\u0026#39;))) print(hash_function(get_key(\u0026#39;Dwayne\u0026#39;))) print(hash_function(get_key(\u0026#39;Dusan\u0026#39;))) print(hash_function(get_key(\u0026#39;Don\u0026#39;))) print(hash_function(get_key(\u0026#39;Dean\u0026#39;))) print(hash_function(get_key(\u0026#39;Dingo\u0026#39;))) print(hash_function(get_key(\u0026#39;Johnson\u0026#39;))) 7 6 5 3 2 4 7 6 5 0 # index_key = 1 save_data(\u0026#39;Dash\u0026#39;, \u0026#39;1111111111\u0026#39;) save_data(\u0026#39;Donald\u0026#39;, \u0026#39;2222222222\u0026#39;) # index_key = 3 save_data(\u0026#39;Dave\u0026#39;, \u0026#39;3333333333\u0026#39;) # index_key = 4 save_data(\u0026#39;David\u0026#39;, \u0026#39;4444444444\u0026#39;) save_data(\u0026#39;Dwayne\u0026#39;, \u0026#39;5555555555\u0026#39;) save_data(\u0026#39;Dusan\u0026#39;, \u0026#39;6666666666\u0026#39;) # index_key = 5 save_data(\u0026#39;Don\u0026#39;, \u0026#39;7777777777\u0026#39;) # index_key = 6 save_data(\u0026#39;Dean\u0026#39;, \u0026#39;8888888888\u0026#39;) save_data(\u0026#39;Dingo\u0026#39;, \u0026#39;9999999999\u0026#39;) # index_key = 7 save_data(\u0026#39;Johnson\u0026#39;, \u0026#39;0000000000\u0026#39;) print(hash_table) [[[-2635316466179689368, '0000000000']], 0, [[6215479457786385290, '5555555555']], [[-7904014224011995085, '4444444444']], [[1661792002016286988, '6666666666']], [[-7424204428908836315, '3333333333'], [2760365508324363629, '9999999999']], [[-6894406110985197394, '2222222222'], [-7882665379891136098, '8888888888']], [[-8106021915874705937, '1111111111'], [6082400908374278295, '7777777777']], 0, 0] read_data(\u0026#39;Dusan\u0026#39;) '6666666666' read_data(\u0026#34;Dance\u0026#34;) # 데이터 존재 하지 않음 4.2. Open Addressing 방식  추가 메모리 공간을 사용하지 않고, 해시 테이블의 빈 공간을 사용하는 방법.  Separate chainging에 비해 메모리를 덜 사용함.    4.2.1. Linear probing  충돌이 발생할 시, 해당 hash_address의 다음 hash_address부터 가장 먼저 등장하는 빈 공간에 저장하는 기법 장점  저장공간 활용도를 높일 수 있음. 저장 시 별도의 별도의 공간이나 추가 작업이 필요 없음.   단점  해시 함수의 퍼포먼스에 따라 해시테이블의 성능이 결정됨. 대신 빈 공간을 미리 확보하기 위해 해시 테이블 저장공간을 다시 확대하거나 미리 마련이 되어 있어야 함..    def get_key(data): return hash(data) def hash_function(key): return key % 8 def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) # 이미 Hash table의 공간이 차있어 충돌이 발생할 경우 if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] return elif hash_table[index][0] == index_key: hash_table[index][1] = value return else: hash_table[hash_address] = [index_key, value] def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] else: None hash_table = list([0 for i in range(12)]) # index_key = 1 save_data(\u0026#39;Dash\u0026#39;, \u0026#39;1111111111\u0026#39;) save_data(\u0026#39;Donald\u0026#39;, \u0026#39;2222222222\u0026#39;) # index_key = 3 save_data(\u0026#39;Dave\u0026#39;, \u0026#39;3333333333\u0026#39;) # index_key = 4 save_data(\u0026#39;David\u0026#39;, \u0026#39;4444444444\u0026#39;) save_data(\u0026#39;Dwayne\u0026#39;, \u0026#39;5555555555\u0026#39;) save_data(\u0026#39;Dusan\u0026#39;, \u0026#39;6666666666\u0026#39;) # index_key = 5 save_data(\u0026#39;Don\u0026#39;, \u0026#39;7777777777\u0026#39;) # index_key = 6 save_data(\u0026#39;Dean\u0026#39;, \u0026#39;8888888888\u0026#39;) save_data(\u0026#39;Dingo\u0026#39;, \u0026#39;9999999999\u0026#39;) # index_key = 7 save_data(\u0026#39;Johnson\u0026#39;, \u0026#39;0000000000\u0026#39;) print(hash_table) [[-2635316466179689368, '0000000000'], 0, [6215479457786385290, '5555555555'], [-7904014224011995085, '4444444444'], [1661792002016286988, '6666666666'], [-7424204428908836315, '3333333333'], [-6894406110985197394, '2222222222'], [-8106021915874705937, '1111111111'], [6082400908374278295, '7777777777'], [-7882665379891136098, '8888888888'], [2760365508324363629, '9999999999'], 0] \u0026amp;nbsp\n5. Hash 함수와 Key 생성 5.1. 대표적인 해시 함수들  SHA (Secure Hash Algorithm)  어떠한 데이터도 고정된 크기의 unique한 값으로 리턴하므로, 해시 함수로 유용하게 활용 가능 해시함수들의 모음이기에, 여러가지 함수를 선택할 수 있음.    import hashlib # SHA-1를 사용한 예시 data = \u0026#39;David\u0026#39;.encode() hash_object = hashlib.sha1() hash_object.update(data) hash_address = hash_object.hexdigest() print(hash_address) d27937f914ebe99ee315f04449678eccfb658191 # SHA-256을 사용한 예시 data = \u0026#39;David\u0026#39;.encode() hash_object = hashlib.sha256() hash_object.update(data) hash_address = hash_object.hexdigest() print(hash_address) a6b54c20a7b96eeac1a911e6da3124a560fe6dc042ebf270e3676e7095b95652 5.2. SHA-256 알고리즘을 사용한 Linear Probing 방식구현 import hashlib def get_key(data): hash_object = hashlib.sha256() hash_object.update(data.encode()) hash_address = hash_object.hexdigest() return int(hash_address, 16) def hash_function(key): return key % 8 def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) # 이미 Hash table의 공간이 차있어 충돌이 발생할 경우 if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] return elif hash_table[index][0] == index_key: hash_table[index][1] = value return else: hash_table[hash_address] = [index_key, value] def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] else: None hash_table = list([0 for i in range(12)]) print(hash_function(get_key(\u0026#39;Dash\u0026#39;))) print(hash_function(get_key(\u0026#39;Donald\u0026#39;))) print(hash_function(get_key(\u0026#39;Dave\u0026#39;))) print(hash_function(get_key(\u0026#39;David\u0026#39;))) print(hash_function(get_key(\u0026#39;Dwayne\u0026#39;))) print(hash_function(get_key(\u0026#39;Dusan\u0026#39;))) print(hash_function(get_key(\u0026#39;Don\u0026#39;))) print(hash_function(get_key(\u0026#39;Dean\u0026#39;))) print(hash_function(get_key(\u0026#39;Dingo\u0026#39;))) print(hash_function(get_key(\u0026#39;Johnson\u0026#39;))) 3 6 0 2 2 5 5 4 6 6 save_data(\u0026#39;Dash\u0026#39;, \u0026#39;1111111111\u0026#39;) save_data(\u0026#39;Donald\u0026#39;, \u0026#39;2222222222\u0026#39;) save_data(\u0026#39;Dave\u0026#39;, \u0026#39;3333333333\u0026#39;) save_data(\u0026#39;David\u0026#39;, \u0026#39;4444444444\u0026#39;) save_data(\u0026#39;Dwayne\u0026#39;, \u0026#39;5555555555\u0026#39;) save_data(\u0026#39;Dusan\u0026#39;, \u0026#39;6666666666\u0026#39;) save_data(\u0026#39;Don\u0026#39;, \u0026#39;7777777777\u0026#39;) save_data(\u0026#39;Dean\u0026#39;, \u0026#39;8888888888\u0026#39;) save_data(\u0026#39;Dingo\u0026#39;, \u0026#39;9999999999\u0026#39;) save_data(\u0026#39;Johnson\u0026#39;, \u0026#39;0000000000\u0026#39;) print(hash_table) [[58168926492874022204843410240616221587430711422315320988033179720499944676464, '3333333333'], 0, [75404257596651192996495076349601554552549513252973852817536161452854420788818, '4444444444'], [63434467723890717949172920093925024550717963975746208715791640357658818776859, '1111111111'], [103158016914344531977983463060013032302915828748947913551605310269665217945786, '5555555555'], [90558914996105951668787733552590627218772546758158603367772415150980389476661, '6666666666'], [40513459897764969709188365008375736156728765495033312981181177193702355922238, '2222222222'], [16606146580844896176716406780736496581454102609573324990177790343105877227493, '7777777777'], [88623518743408414412271740834380503561141448764593279404613947210397492361580, '8888888888'], [22241017530888154973558349121945220497843199841401728659273049527650898379222, '9999999999'], [21745812297715092507978491799105903853662369235937786557584049993744107100774, '0000000000'], 0] read_data(\u0026#39;Dingo\u0026#39;) '9999999999' \u0026amp;nbsp\n6. 시간 복잡도  저장 (insertion), 삭제 (deletion), 검색(search)  Collision이 없는 경우: O(1) Collision이 모두 발생하는 최악의 경우: O(n)     해쉬 테이블의 경우, 일반적인 경우를 기대하고 만들기 때문에, 시간 복잡도는 O(1) 이라고 말할 수 있음\n \u0026amp;nbsp\n7. Reference  Fastcampus 알고리즘 / 기술면접 강의 Hash Table Wikipedia  ","permalink":"https://wonyoungseo.github.io/posts/2021-02-19-python-datastructure-hash-table/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 해쉬 테이블 (Hash Table) 1.1. 해쉬 테이블의 구조   키(Key)에 데이터(Value)가 매핑되어 저장되어 있는 구조\n Key를 통해 데이터를 바로 받아올 수 있으므로, 속도가 빠름 파이썬에서는 딕셔너리(Dictionary)가 해쉬 테이블의 예시.  dict = {\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}   1.2. 해쉬 테이블의 용어  해쉬(Hash)  임의의 값을 고정된 길이로 변환하는 것   해쉬 테이블(Hash Table)  Key값의 연산에 의해 직접 접근이 가능한 데이터 구조   해싱 함수(Hashing Function)  Key에 대해 특정 산술 연산을 이용하여 데이터의 위치(해쉬 주소)가 리턴되는 함수   해쉬 값(Hash Value) 또는 해쉬 주소(Hash Address)  Key를 해싱 함수로 연산하여 얻는 값 Key를 해싱 함수로 연산하여 해쉬 값이 데이터의 위치.","title":"[KR] 자료구조 \u0026 알고리즘 : 해시 테이블(Hash Table)"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.1.3. 알고리즘 시간 복잡도의 주요 요소  반복문이 얼마나 시행되었는지에 따라 시간 복잡도의 성능이 결정된다고 할 수 있음. 입력의 크기가 커지면 커질 수록 반복문이 알고리즘 수행 시간을 지배함.  1.2. 복잡도 표기법 유형 1.2.1. Big-O 표기법 \\(O(N) \\)  알고리즘 최악의 실행시간을 표기함 아무리 최악의 상황이라도 이 정도의 성능은 보장한다는 의미 가장 많이(일반적으로) 사용함  1.2.2. 오메가 표기법 \\(\\Omega(N) \\)  최상의 알고리즘 실행 시간을 표기  1.2.3. 세타 표기법 \\(\\Theta(N) \\)  알고리즘 평균 실행 시간을 표기  1.3. Big-O 표기법 1.3.1. \\( O(n) \\)  입력 \\(n \\)에 따라 결정되는 시간 복잡도 함수.  \\(O(1) \\) \\(O(\\log n) \\) \\(O(n) \\) \\(O(n \\log n) \\) \\(O(n^2) \\) \\(O(2^n) \\) \\(O(n!) \\)   입력에 따라 기하급수적으로 시간 복잡도가 늘어날 수 있음.  \\(O(1) \u0026lt; O(\\log n) \u0026lt; O(n) \u0026lt; O(n \\log n) \u0026lt; O(n^2) \u0026lt; O(2^n) \u0026lt; O(n!) \\)    1.3.2. 계산법  \\(O(1) \\)  단순하게 입력 \\(n \\)에 따라 멸번 실행이 되는지 계산함. 실행은 무조건 2회(또는 상수회) 실행한다.  if n \u0026gt; 10: print(n)  \\(O(n) \\)  \\(n \\)에 따라 \\(n \\)번 또는 \\(k \\cdot n + b \\) 실행한다.  for idx in range(n): print(idx)  \\(O(n^2) \\)  \\(n \\)에 따라 \\(n^2 \\)번 또는 \\(k \\cdot n^2 + b \\) 등을 실행한다.  for num in range(n): for index in range(n): print(index)   1.3.3. 표기 방법  시간복잡도는 결국 입력값 \\(n \\)에 따라 성능이 결정됨. 결국 알고리즘 성능에 가장 영향을 끼치는 값을 기준으로 표기함. 따라서 상수 \\(k, b \\)는 표기할 때 생략함 \\(k \\cdot n^2 + b \\)의 경우 Big-O 표기법으로는 \\(O(n^2) \\)으로 표기함.  2. Reference  Fastcampus 알고리즘 / 기술면접 강의  ","permalink":"https://wonyoungseo.github.io/posts/2021-02-10-complexity/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.","title":"[KR] 알고리즘 복잡도"},{"content":"Hello World 데이터사이언티스트 \u0026amp; ML 엔지니어 입니다. 스타트업에서 기업의 ESG 데이터를 기반으로 하는 ESG 사건사고 분석 서비스와 지속가능 여신 및 신용평가 (ESG CB) 를 개발한 경험이 있습니다.\n관심 분야는 MLOps, 데이터 파이프라인, 프로토타입 어플리케이션 개발 등 중구난방이지만 호기심이 많습니다. (원래 시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\n\u0026amp;nbsp\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다. 평점은 왓챠에서 매기고, 넷플릭스, 왓챠, 디즈니 플러스, 쿠팡플레이를 구독 중입니다. 테니스를 열심히 치고 있습니다. 테린이입니다. 힙합, R\u0026amp;B 등의 음악에 관심이 많고 웹매거진 플랫폼인 HIPHOPLE.com에서 스태프로 일 한 경험이 있습니다. 스페인어를 아주 조금 할 줄 압니다. 그리고 언젠가는 독일어를 배우고 싶습니다. 유투브를 통해 경험하지 못한 새로운 세상을 접하는 걸 정말 좋아합니다.  ","permalink":"https://wonyoungseo.github.io/about/","summary":"Hello World 데이터사이언티스트 \u0026amp; ML 엔지니어 입니다. 스타트업에서 기업의 ESG 데이터를 기반으로 하는 ESG 사건사고 분석 서비스와 지속가능 여신 및 신용평가 (ESG CB) 를 개발한 경험이 있습니다.\n관심 분야는 MLOps, 데이터 파이프라인, 프로토타입 어플리케이션 개발 등 중구난방이지만 호기심이 많습니다. (원래 시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\n\u0026amp;nbsp\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다. 평점은 왓챠에서 매기고, 넷플릭스, 왓챠, 디즈니 플러스, 쿠팡플레이를 구독 중입니다. 테니스를 열심히 치고 있습니다.","title":"About"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 링크드 리스트 : Linked List 의 개념 1.1. 링크드 리스트의 구조  데이터와 데이터 사이를 화살표로 연결하여 관리하는 데이터 구조. 배열(Array)와의 차이점  1.2. 링크드 리스트와 배열(Array)와 차이점  배열 : 번호가 붙여진(인덱싱이 된) 칸에 원소들을 채워 넣어 관리함. 링크드 리스트: 각 원소들을 줄줄이 엮어서 관리함.  1.2.1. 배열과 링크드 리스트의 비유적 비교 (출처: 생활코딩)  메모리라는 개념을 우리는 건물에 비유할 수 있을 것 같습니다. 아래 예시는 배열을 사용하는 것과 linked list를 사용하는 것을 비유해서 보여주고 있습니다. 여러분의 회사가 한 건물의 일부를 임대해서 사용한다고 생각해주세요.\n    Source https://opentutorials.org/module/1335/8821    Array list의 첫 번째 회사는 모든 직원이 한곳에 모여있어야 한다는 철학이 있기 때문에 사무실이 모여있습니다. 배열은 건물을 이런 식으로 사용하는 것과 비슷합니다. 만약 회사가 성장해서 사무실이 좁아지면 더 이상 새로운 직원을 뽑을 수 없습니다. 붙어있는 공간이 없기 때문이죠. 만약 더 많은 공간이 필요하다면 더 많은 사람을 수용할 수 있는 공간을 찾아서 전체가 이사해야 합니다. Array list는 엘리먼트가 같은 곳에 모여있습니다. 만약에 3번째 자리로 가고 싶다면 한번에 3번째 방으로 갈 수 있습니다. 찾고자 하는 사무실이 몇 번째에 있는지 알고 있다면 Array list는 매우 빠릅니다.\n    Source https://opentutorials.org/module/1335/8821    Linked List의 두 번째 회사는 한 건물 내에서 한 회사가 임대한 사무실이 서로 떨어져 있습니다. 덕분에 직원이 늘어도 큰 걱정이 없습니다. 건물에서 비어있는 곳 아무데나 임대해서 들어가면 되니까요. 그런데 방문자가 사무실을 찾는 방법이 좀 비효율적입니다. 위의 그림에 있는 방문자가 3번째 사무실을 찾아가려면 우선 첫 번째 화살표의 사무실을 찾아가야 합니다. 이 사무실의 직원에게 다음 사무실이 어딘지 물어봅니다. 그럼 알려주는 사무실로 이동 한 후에 다시 물어봐서 그다음 사무실로 이동합니다. 이렇게 물어물어 사무실을 찾아가야 하는 방식이 Linked List입니다. 그래서 Linked List에서는 몇 번째 엘리먼트를 찾는 것이 느립니다.\n 1.3. 링크드 리스트 관련 용어  노드(node):  데이터가 저장되는 단위. [데이터값, 포인터]로 구성   포인터(pointer):  다음 데이터의 주소를 담고 있는 공간. 노드에서 다음 또는 이전 노드와의 연결 정보를 가지고 있는 공간.    1.4. 링크드 리스트의 장단점 장점:\n 데이터 공간을 미리 할당하지 않아도 됨. (array는 미리 데이터 공간을 할당해야 함.) 삽입과 삭제가 빠름. 따라서 삽입/삭제가 빈번히 일어날 때 많이 사용됨.  단점:\n 데이터 구조 표현에 소요되는 저장공간이 비교적 큼.  연결을 위한 별도의 데이터공간이 필요하기 때문에 저장공간 효율이 높지 않음.   데이터를 찾는 시간이 오래 걸림.  인덱싱이 된 배열와는 달리, 특정 N번째 원소에 접근하려면 링크드 리스트의 처음부터 순차적으로 원소를 훑으며 N번째 원소를 찾아가야 함.   중간에 위치한 데이터 삭제 시, 앞뒤 데이터의 연결을 다시 구현해야하는 부가적인 작업이 필요함.  2. 파이썬에서의 링크드 리스트 pt.1  링크드 리스트는 C언어에서 주요한 데이터구조이지만, 파이썬에서는 리스트 타입이 링크드 리스트의 기능을 모두 지원함.  2.1. 노드 구현하기 # 포인트가 없는 노드를 구현 class Node: def __init__(self, data): self.data = data self.next = None # 포인터가 있는 노드를 구현하기 class Node: def __init__(self, data, next=None): self.data = data self.next = next # 포인터 \u0026amp;nbsp\n2.2. 포인터를 활용하여 노드와 노드를 연결하기 node1 = Node(1) node2 = Node(2) head = node1 # 링크드리스트의 첫 시작은 node1로 지정함. node1.next =node2 # node1의 포인터에 node2를 저장하여 연결함. print(head.data) # node1의 데이터 출력 print(node1.next.data) # node2의 데이터 출력 1 2 \u0026amp;nbsp\n2.3. 링크드 리스트로 데이터 추가하기 class Node: def __init__(self, data, next=None): self.data = data self.next = next def add(data): node = head while node.next: node = node.next node.next = Node(data) node1 = Node(1) # 첫 번째 노드 생성 head = node1 # 첫 번째 노드를 head로 지정 for idx in range(2, 10): # 추가할 데이터를 iteration하며 추가하기 add(idx) 2.4. 링크드 리스트 데이터 접근하여 출력하기 # 한개씩 접근하기 print(head.data) # 1번째 노드 print(head.next.data) # 2번째 노드 print(head.next.next.data) # 3번째 노드 print(head.next.next.next.data) # 4번째 노드 1 2 3 4 \u0026amp;nbsp\n# 한꺼번에 접근하기 node = head while node.next: # 노드의 데이터 순차적으로 출력 print(node.data) node = node.next print(node.data) # next가 없는 마지막 노드의 데이터 출력 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n2.5. 링크드 리스트의 중간 위치에 데이터 추가하기 2.5.1. 단순하게 추가해보기 # 기존의 데이터 출력하기 node = head while node.next: print(node.data) node = node.next print(node.data) 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\nnode3_5 = Node(3.5) # 2와 3 사이에 위치할 데이터(2.5)가 담긴 노드 생성 node = head search = True while search: if node.data == 3: # 새 데이터(3.5)의 직전 데이터가 되는 노드(3)를 찾음. search = False # search 종료 else: node = node.next node_next = node.next # 노드(3)의 다음 노드(4)를 따로 빼기 node.next = node3_5 # 노드(3.5)를 노드(3)의 다음 노드로 연결 node3_5.next = node_next # 노드(4)을 노드(3.5)의 다음 노드로 연결 node = head while node.next: print(node.data) node = node.next print(node.data) 1 2 3 3.5 4 5 6 7 8 9 \u0026amp;nbsp\n3. 파이썬에서의 링크드 리스트 pt.2 3.1. 객체지향 프로그램으로 구현해보기 class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next 링크드 리스트의 노드 모두 순회하며 데이터를 출력하기\nlinkedlist_1 = NodeManagement(0) linkedlist_1.print_all() 0 \u0026amp;nbsp\nfor data in range(1, 10): linkedlist_1.add(data) linkedlist_1.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n3.2. 특정 데이터를 가진 노드 검색하기  search_node() 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: # 노드의 데이터가 찾고자 하는 데이터가 맞다면 그대로 리턴 return node else: node = node.next # 다음 노드로 순회 \u0026amp;nbsp\nlinkedlist_2 = NodeManagement(0) for data in range(1, 10): linkedlist_2.add(data) linkedlist_1.print_all() linkedlist_2.search_node(4).data # 데이터가 4인 노드의 데이터 출력 0 1 2 3 4 5 6 7 8 9 4 \u0026amp;nbsp\n3.3. 특정 인덱스의 노드 검색하기  get_node 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node \u0026amp;nbsp\nlinkedlist_3 = NodeManagement(0) for data in range(1, 10): linkedlist_3.add(data) linkedlist_1.print_all() # 전체 노드 데이터 출력 linkedlist_3.get_node(3).data # 3번째 노드의 데이터 출력  0 1 2 3 4 5 6 7 8 9 3 \u0026amp;nbsp\n3.2. 특정 노드 삭제하기  delete ㅁㅔ서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node def delete(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 삭제 \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: print(\u0026#34;No node avaible\u0026#34;) return if self.head.data == data: # 삭제하고자 하는 데이터가 가장 첫 번째 노드일 때 temp = self.head self.head = self.head.next del temp else: node = self.head while node.next: if node.next.data == data: # 삭제하고자 하는 데이터가 다음 노드에 있을 때 temp = node.next node.next = node.next.next # 다음 노드를 건너 뛰고, 다음다음 노드를 바로 연결 del temp pass else: node = node.next 노드를 한개(헤드) 만들어서 delete 메서드 실험하기\nlinkedlist_4 = NodeManagement(0) linkedlist_4.print_all() 0 \u0026amp;nbsp\nlinkedlist_4.head # 헤드가 존재함 \u0026lt;__main__.Node at 0x107e94d10\u0026gt; \u0026amp;nbsp\nlinkedlist_4.delete(0) # 헤드를 삭제 linkedlist_4.head # 헤드 삭제 후에는 헤드가 출력되지 않음 linkedlist_4.print_all() # 링크드 리스트에 노드가 존재하지 않음 \u0026amp;nbsp\n여러 노드를 추가해서 delete 메서드 실험하기\nlinkedlist_5 = NodeManagement(0) for data in range(1, 10): linkedlist_5.add(data) linkedlist_5.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\nlinkedlist_5.delete(6) # 노드 한개를 삭제함 linkedlist_5.print_all() 0 1 2 3 4 5 7 8 9 \u0026amp;nbsp\nlinkedlist_5.delete(3) linkedlist_5.print_all() 0 1 2 4 5 7 8 9 \u0026amp;nbsp\n3.3. 새 노드 중간에 삽입하기  insert 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node def delete(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 삭제 \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: print(\u0026#34;No node avaible\u0026#34;) return if self.head.data == data: # 삭제하고자 하는 데이터가 가장 첫 번째 노드일 때 temp = self.head self.head = self.head.next del temp else: node = self.head while node.next: if node.next.data == data: # 삭제하고자 하는 데이터가 다음 노드에 있을 때 temp = node.next node.next = node.next.next # 다음 노드를 건너 뛰고, 다음다음 노드를 바로 연결 del temp pass else: node = node.next def insert(self, data, position_index): \u0026#34;\u0026#34;\u0026#34; 지정한 위치에 데이터 노드 삽입하기 \u0026#34;\u0026#34;\u0026#34; if position_index == 0: # head 위치에 삽입하기 temp = self.head self.head = Node(data) self.head.next = temp self.nodeCount += 1 elif position_index \u0026gt; self.nodeCount + 1: print(\u0026#34;Error: Insert index exceeds the length of linked list\u0026#34;) return else: node = self.head search_index = 0 search_status = True while search_status: search_index += 1 if search_index == position_index: search_status = False insert_node = Node(data) node_next = node.next node.next = insert_node insert_node.next = node_next self.nodeCount += 1 else: node = node.next linkedlist_6 = NodeManagement(0) for data in range(1, 10): linkedlist_6.add(data) linkedlist_6.print_all() # 노드 순회하며 데이터 출력하기 linkedlist_6.nodeCount # 노드 개수 출력하기 0 1 2 3 4 5 6 7 8 9 10 \u0026amp;nbsp\nlinkedlist_6.insert(17, 7) # 17이라는 데이터를 7번째 인덱스에 넣기 linkedlist_6.print_all() linkedlist_6.nodeCount # 노드 삽입 후 노드 개수 출력 0 1 2 3 4 5 6 17 7 8 9 11 \u0026amp;nbsp\n4. 링크드 리스트의 개선된 타입 4.1. 더블 링크드 리스트 : Doubly-Linked List  링크드 리스트의 단점을 보완하기 위해 등장함.  4.1.1. 기본 구조  양방향으로 연결되어 있어, 노드 탐색이 양쪽으로 모두 가능한 구조 즉, 다음 노드를 가리키는 포인터(next) 뿐만 아니라, 이전 노드를 가리키는 포인터(prev)도 존재함.  \u0026amp;nbsp\n4.1.2. 더블 링크드 리스트의 장단점 장점:\n 양방향으로 탐색이 가능함.  단점:\n 이전 노드를 가리키는 포인터가 추가되기 때문에 메모리 사용량이 늘어남. 삽입/삭제 연산에 있어, 앞/뒤 연결링크를 조정해줘야 하기 때문에 구조가 복잡해짐.  \u0026amp;nbsp\n4.1.2. 파이썬으로 구현한 더블 링크드 리스트  더블 링크드 리스트의 노드는 이전 노드와 다음 노드를 가리키는 포인터를 포함하는 것이 핵심.  class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next \u0026amp;nbsp\ndouble_linked_list = NodeManagement(0) for data in range(1, 10): double_linked_list.add(data) double_linked_list.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n4.2. 더블 링크드 리스트의 검색과 삽입 4.2.1. 특정 노드 이전에 새로운 노드 삽입하기 class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next def search_from_head(self, data): if self.head == None: return None node = self.head while node: if node.data == data: return node else: node = node.next return None def search_from_tail(self, data): if self.head == None: return None node = self.tail while node: if node.data == data: return node else: node = node.prev return None def insert_before(self, data, prev_data): if self.head == None: self.head = Node(data) return None else: node = self.tail while node.data != prev_data: node = node.prev if node == None: return None new_node = Node(data) prev_new = node.prev prev_new.next = new_node new_node.prev = prev_new new_node.next = node node.prev = new_node return None \u0026amp;nbsp\ndouble_linked_list = NodeManagement(0) for data in range(1, 10): double_linked_list.add(data) double_linked_list.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n# 뒤에서부터 찾기 node_3 = double_linked_list.search_from_tail(3) node_3.data 3 \u0026amp;nbsp\n# 앞에서부터 찾기 node_8 = double_linked_list.search_from_head(8) node_8.data 8 \u0026amp;nbsp\ndouble_linked_list.insert_before(4.5, 5) double_linked_list.print_all() 0 1 2 3 4 4.5 5 6 7 8 9 \u0026amp;nbsp\nnode_4_5 = double_linked_list.search_from_tail(4.5) node_4_5.data 4.5 \u0026amp;nbsp\n4.2.2. 특정 노드 이후에 새로운 노드 삽입하기 class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next def search_from_head(self, data): if self.head == None: return None node = self.head while node: if node.data == data: return node else: node = node.next return None def search_from_tail(self, data): if self.head == None: return None node = self.tail while node: if node.data == data: return node else: node = node.prev return None def insert_before(self, data, prev_data): if self.head == None: self.head = Node(data) return None else: node = self.tail while node.data != prev_data: node = node.prev if node == None: return None new_node = Node(data) prev_new = node.prev prev_new.next = new_node new_node.next = node return None def insert_after(self, data, next_data): if self.head == None: self.head = Node(data) return None else: node = self.head while node.data != next_data: node = node.next if node == None: return None new_node = Node(data) next_new = node.next new_node.next = next_new new_node.prev = node if new_node.next == None: self.tail = new_node else: node.next = new_node return True double_linked_list_2 = NodeManagement(0) for data in range(1, 10): double_linked_list_2.add(data) double_linked_list_2.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\ndouble_linked_list_2.insert_after(2.5, 2) True \u0026amp;nbsp\ndouble_linked_list_2.print_all() 0 1 2 2.5 3 4 5 6 7 8 9 5. 정리해보기  배열 리스트는 일정한 공간을 할당 후, 그 공간 안에서 물리적인 순서를 가지는 자료구조이다. 반대로 링크드 리스트(연결리스트)는 데이터를 담고 있는 노드들의 논리적인 순서를 위한 자료구조이다. 논리적인 구조는 이전/다음 노드를 가리키는 포인터를 통해 정의한다. 링크드 리스트는 처음부터 특정 공간을 할당하지 않아 메모리 활용도가 유연하고, 메모리 낭비를 막을 수 있다. 삽입과 삭제가 빠르다. 구현 작업은 다소 복잡할 지 모르나, 구현 후 실행은 간단하다. 하지만 포인터라는 요소까지 포함하여야 하기 때문에 비교적 더 큰 공간을 잡아먹는다. 또한 배열와는 달리, 순차적으로 노드를 훑으며 노드를 찾아가야 하기 때문에 시간이 좀 더 걸린다. 단순한 링크드 리스트는 한방향으로만 노드를 탐색하기 때문에 비효율적일 수 있다. 양방향 링크드 리스트는 앞/뒤에서부터 동시에 노드를 탐색하기 때문에 단순 링크드 리스트의 단점을 보완할 수 있다.  6. Reference  Fastcampus 알고리즘 / 기술면접 강의 프로그래머스 프로그램밍 강의 어서와! 자료구조는 처음이지? 생활코딩  ","permalink":"https://wonyoungseo.github.io/posts/2021-02-01-python-datastructure-linked-list/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 링크드 리스트 : Linked List 의 개념 1.1. 링크드 리스트의 구조  데이터와 데이터 사이를 화살표로 연결하여 관리하는 데이터 구조. 배열(Array)와의 차이점  1.2. 링크드 리스트와 배열(Array)와 차이점  배열 : 번호가 붙여진(인덱싱이 된) 칸에 원소들을 채워 넣어 관리함. 링크드 리스트: 각 원소들을 줄줄이 엮어서 관리함.  1.2.1. 배열과 링크드 리스트의 비유적 비교 (출처: 생활코딩)  메모리라는 개념을 우리는 건물에 비유할 수 있을 것 같습니다.","title":"[KR] 자료구조 \u0026 알고리즘 : 링크드 리스트(Linked List)"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 0. 자료구조? 알고리즘?  자료구조 Data Structure  대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조    \u0026amp;nbsp\n 체계적인 데이터 구조화의 필요성  코드 상에서 효율적인 데이터 처리하기 위함 어떤 데이터 구조를 사용하느냐에 따라 효율이 달라짐.    \u0026amp;nbsp\n 알고리즘이란  어떠한 문제를 풀기 위한 절차 / 방법 특정 문제에 해당하는  특정 입력을 넣으면 특정 출력을 얻을 수 있도록 하는 프로그래밍      \u0026amp;nbsp\n 문제를 푸는 방법은 각양각색이지만, 다음을 고려하여 계산을 하고 최적의 방법을 찾는다.  어느 정도의 시간을 쓰는가? 어느 정도의 저장 공간을 활용하는가?    \u0026amp;nbsp\n 자료구조와 알고리즘이 중요한 이유  어떤 자료구조와 알고리즘을 쓰느냐에 따라 성능 면에서 아주 큰 차이가 생김.     1. 배열 : Array  같은 종류의 데이터를 순차적으로 저장하는 형태의 데이터타입  1.1. 배열의 필요성  같은 종류의 데이터를 효율적으로 관리 같은 종류의 데이터를 순차적으로 데이터를 저장  1.2. 배열의 장단점 (파이썬이 아닌 C로 봤을 때)\n장점:\n 구현이 쉬움. 빠른 접근이 가능함.  인덱스index가 매겨지기에, 첫 데이터(인덱스 0)의 위치를 기준으로 상대적인 위치의 데이터에 빠르게 접근 가능 즉, 일단 만들어 놓으면 빠른 접근이 가능.   검색에 용이함.  \u0026amp;nbsp\n단점:\n 데이터 추가와 삭제가 어려움.  미리 최대 길이를 지정해야 하기 때문   데이터를 추가하거나 삭제를 하면 길이에 변화가 생김.  변수를 새로 만드는 수 밖에 없음   즉, 일단 만들어 놓으면 수정이 어렵고 메모리 재사용이 불가함.  1.3. 파이썬에서의 배열  파이썬에서는 리스트(list) 타입  # 1차원 배열 array_1d = [1, 2, 3, 4, 5] # 2차원 배열 array_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] print(array_1d) print(array_2d) [1, 2, 3, 4, 5] [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \u0026amp;nbsp\n### 연습(1) # 2차원 배열에서 9, 8, 7을 순서대로 출력해보기 array_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] print(array_2d[2][2]) print(array_2d[2][1]) print(array_2d[2][0]) 9 8 7 \u0026amp;nbsp\n### 연습(2) # 아래 데이터셋에서 전체 이름 안에서 M은 몇 번 나왔는지 빈도수 출력 dataset = [\u0026#39;Braund, Mr. Owen Harris\u0026#39;, \u0026#39;Cumings, Mrs. John Bradley (Florence Briggs Thayer)\u0026#39;, \u0026#39;Heikkinen, Miss. Laina\u0026#39;, \u0026#39;Futrelle, Mrs. Jacques Heath (Lily May Peel)\u0026#39;, \u0026#39;Allen, Mr. William Henry\u0026#39;, \u0026#39;Moran, Mr. James\u0026#39;, \u0026#39;McCarthy, Mr. Timothy J\u0026#39;, \u0026#39;Palsson, Master. Gosta Leonard\u0026#39;, \u0026#39;Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\u0026#39;, \u0026#39;Nasser, Mrs. Nicholas (Adele Achem)\u0026#39;, \u0026#39;Sandstrom, Miss. Marguerite Rut\u0026#39;, \u0026#39;Bonnell, Miss. Elizabeth\u0026#39;, \u0026#39;Saundercock, Mr. William Henry\u0026#39;, \u0026#39;Andersson, Mr. Anders Johan\u0026#39;, \u0026#39;Vestrom, Miss. Hulda Amanda Adolfina\u0026#39;, \u0026#39;Hewlett, Mrs. (Mary D Kingcome) \u0026#39;, \u0026#39;Rice, Master. Eugene\u0026#39;, \u0026#39;Williams, Mr. Charles Eugene\u0026#39;, \u0026#39;Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\u0026#39;, \u0026#39;Masselmani, Mrs. Fatima\u0026#39;, \u0026#39;Fynney, Mr. Joseph J\u0026#39;, \u0026#39;Beesley, Mr. Lawrence\u0026#39;, \u0026#39;McGowan, Miss. Anna \u0026#34;Annie\u0026#34;\u0026#39;, \u0026#39;Sloper, Mr. William Thompson\u0026#39;, \u0026#39;Palsson, Miss. Torborg Danira\u0026#39;, \u0026#39;Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\u0026#39;, \u0026#39;Emir, Mr. Farred Chehab\u0026#39;, \u0026#39;Fortune, Mr. Charles Alexander\u0026#39;, \u0026#39;Dwyer, Miss. Ellen \u0026#34;Nellie\u0026#34;\u0026#39;, \u0026#39;Todoroff, Mr. Lalio\u0026#39;] count = 0 for data in dataset: for idx in range(len(data)): if data[idx]==\u0026#39;M\u0026#39;: count += 1 print(count) 38  2. 큐 : Queue 2.1. 큐의 구조  가장 먼저 넣은 데이터를 가장 먼저 꺼낼 수 있는 구조  FIFO 또는 LILO 정책을 씀. FIFO를 더 많이 씀.  FIFO : First in, First out LILO : Last in, Last out      2.2. 큐와 관련된 용어  Enqueue: 큐에 데이터를 넣는 기능 Dequeue: 큐에서 데이터를 꺼내는 기능  2.3. 큐가 많이 사용되는 예시  재귀 알고리즘 역추적을 해야할 때 (i.e. 문서 작업 시 실행 취소) 운영체제 멀티태스킹을 위한 프로세스 스케쥴링을 구현할 때  2.4. 큐의 시간 복잡도  삽입 / 삭제  원소를 삽입하거나 삭제하는 경우  O(1)      2.5. 큐의 장단점 장점:\n 데이터의 삽입과 삭제가 빠름  \u0026amp;nbsp\n단점:\n 정책에 따라 가장 위쪽의 원소만 접근 가능함.  i.e. FIFO의 경우 맨 위의 원소만 접근이 가능함.   탐색이 상당히 비효율적임. 다 꺼내보면서 탐색해야 함.  2.6. 파이썬에서의 Queue  queue 라이브러리를 사용.  Queue() : 가장 일반적인 큐(FIFO) 자료구조 LifoQueue() : 나중에 입력된 데이터일 수록 먼저 출력되는 구조 (스택) PriorityQueue() : 입력된 데이터마다 우선순위를 설정, 우선순위 순으로 데이터 출력    2.6.1. queue.Queue() : FIFO # queue 라이브러리 import queue fifo_queue = queue.Queue() # FIFO fifo_queue.put(3) # enqueue fifo_queue.put(15) fifo_queue.put(26) fifo_queue.put(56) print(fifo_queue.qsize()) # 크키 print(fifo_queue.queue) 4 deque([3, 15, 26, 56]) \u0026amp;nbsp\nfifo_queue.get() # dequeue 3 \u0026amp;nbsp\nfifo_queue.get() 15 \u0026amp;nbsp\nfifo_queue.get() 26 \u0026amp;nbsp\nfifo_queue.get() 56 \u0026amp;nbsp\n2.6.2. queue.LifoQueue() : LIFO (Last in, First out) lifo_queue = queue.LifoQueue() # LIFO lifo_queue.put(3) # enqueue lifo_queue.put(15) lifo_queue.put(26) lifo_queue.put(56) print(lifo_queue.qsize()) print(lifo_queue.queue) 4 [3, 15, 26, 56] \u0026amp;nbsp\nlifo_queue.get() # dequeue 56 \u0026amp;nbsp\nlifo_queue.get() 26 \u0026amp;nbsp\nlifo_queue.get() 15 \u0026amp;nbsp\nlifo_queue.get() 3 2.6.3. queue.PriorityQueue() : 우선순위에 따라 dequeue pri_queue = queue.PriorityQueue() pri_queue.put((34, \u0026#39;a\u0026#39;)) # 튜플 형태로 넣음 (우선순위, 데이터) pri_queue.put((14, \u0026#39;b\u0026#39;)) pri_queue.put((72, \u0026#39;c\u0026#39;)) pri_queue.put((11, \u0026#39;d\u0026#39;)) pri_queue.put((48, \u0026#39;e\u0026#39;)) print(pri_queue.qsize()) # size print(pri_queue.queue) # 숫자가 낮을 수록 우선순위가 높음 (i.e. 1순위, 2순위) 5 [(11, 'd'), (14, 'b'), (72, 'c'), (34, 'a'), (48, 'e')] \u0026amp;nbsp\npri_queue.get() # dequeue (11, 'd') \u0026amp;nbsp\npri_queue.get() (14, 'b') \u0026amp;nbsp\npri_queue.get() (34, 'a') \u0026amp;nbsp\npri_queue.get() (48, 'e') \u0026amp;nbsp\npri_queue.get() (72, 'c') \u0026amp;nbsp\n연습\nqueue라이브러리가 아닌, 파이썬의 리스트를 가지고 enqueue, dequeue 구현해본다.\nclass queue_list: def __init__(self): self.queue = list() def __size__(self): return len(self.queue) def __items__(self): return self.queue def enqueue(self, data): self.queue.append(data) def dequeue(self): target_data = self.queue[0] del self.queue[0] return target_data ls_queue = queue_list() ls_queue.enqueue(4) ls_queue.enqueue(9) ls_queue.enqueue(1) print(ls_queue.__size__()) print(ls_queue.__items__()) 3 [4, 9, 1] \u0026amp;nbsp\nls_queue.dequeue() 4 \u0026amp;nbsp\nls_queue.dequeue() 9 \u0026amp;nbsp\nls_queue.dequeue() 1 \u0026amp;nbsp\n 3. 스택 : Stack  데이터를 제한적으로 넣을 수 있는 구조  한쪽 끝에서만 자료를 넣거나 뺄 수 있는 구조    큐와의 차이점\n 큐 : FIFO 정책 스택 : LIFO 정책 \u0026ndash;\u0026gt; Last in, First out  3.1. 스택의 구조 스택은 LIFO, FILO 구조이지만, LIFO, FILO라고 말하기보다는 통상 이러한 구조를 스택 Stack 그 자체로 부름.\n 가장 마지막에 넣은 것을 가장 먼저. 가장 먼저 넣은 것을 가장 마지막에.  LIFO : 마지막에 넣은 데이터를 가장 먼저 추출하는 데이터 관리 정책. FILO : 처음 넣은 데이터를 가장 마지막에 추출하는 데이터 관리 정책.    3.2. 스택 관련 용어  push() : 데이터를 스택에 삽입하는 연산 (넣기) pop() : 데이터를 스택에서 삭제하는 연산 (꺼내기)| stack underflow : 비어있는 스택에서 데이터를 꺼내려고 할 때 생기는 오류 stack overflow : 가득 차있는 스택에 데이터를 삽입하려고 할 때 생기는 오류  3.3. 스택의 활용  컴퓨터 내부의 프로세스의 함수들이 동작하는 방식에 쓰임 실행 취소 : 가장 최근에 했던 작업부터 거슬러 올라가며 취소 웹 브라우저 뒤로 가기 기능 : 가장 최근에 봤던 페이지 순으로 거슬러 가며 브라우징  3.4. 스택의 구현 방법  배열(array)  장점:  구현이 쉬움 접근이 빠름   단점:  데이터의 최대 개수를 미리 정해야 함. 데이터 삽입/삭제 시 매우 비효율적.     연결리스트(linked list)  장점:  데이터 최대 개수가 정해져 있지 않음. 데이터 삽입 삭제가 용이함.   단점:  데이터 접근이 한번에 가능하지 않음. 따라서 시간이 걸림.      3.5. 파이썬에서의 Stack  리스트를 통해서 배열(array) 기반의 스택을 구현해볼 수 있음.  .append() : push .pop() : pop    data_stack = list() data_stack.append(1) # push data_stack.append(3) print(data_stack) [1, 3] \u0026amp;nbsp\ndata_stack.pop() # pop 3 \u0026amp;nbsp\nprint(data_stack) [1] \u0026amp;nbsp\nReference  Fast Campus 알고리즘 / 기술면접 강의 [자료구조] 스택(Stack), 큐(Queue), 덱(Deque)  ","permalink":"https://wonyoungseo.github.io/posts/2021-01-27-python-datastructure-array-que-stack/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 0. 자료구조? 알고리즘?  자료구조 Data Structure  대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조    \u0026amp;nbsp\n 체계적인 데이터 구조화의 필요성  코드 상에서 효율적인 데이터 처리하기 위함 어떤 데이터 구조를 사용하느냐에 따라 효율이 달라짐.    \u0026amp;nbsp\n 알고리즘이란  어떠한 문제를 풀기 위한 절차 / 방법 특정 문제에 해당하는  특정 입력을 넣으면 특정 출력을 얻을 수 있도록 하는 프로그래밍      \u0026amp;nbsp","title":"[KR] 자료구조 \u0026 알고리즘 : 배열(array), 큐(queue), 스택(stack)"},{"content":"0. 데이터와 librosa 실제로 소리 데이터를 다뤄보기 위해서 음악 데이터를 준비하겠습니다. 음악 장르 분류 데이터셋으로 유명한 GTZAN Dataset을 다운받아 음악 파일을 하나 선택했습니다.\n그리고 Librosa는 오디오와 음악 분석을 위 빠질 수 없는 파이썬 패키지입니다. 음원이나 소리 파일을 불러와 waveform을 시각화 하거나 다른 형태로 변환할 수 있는 기능을 제공합니다.\n( Librosa는 pip install librosa 명령어를 통해 설치할 수 있습니다. )\nimport warnings warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt import IPython.display as ipd import librosa import librosa.display file_path = \u0026#39;disco.00054.wav\u0026#39; # 실습에 사용할 음악 파일 어떤 음악인지 한번 들어볼까요?\nipd.Audio(file_path)   1. Waveform 음원을 아래처럼 간단하게 읽어와보겠습니다. 튜플인 결과값에서 첫 번째는 numpy array형태의 waveform의 amplitude값이고, 두 번째 값은 sampling rate으로, 초당 샘플 갯수를 의미합니다. Sampling rate는 파일을 읽을 때, parameter로 설정할 수 있습니다. Default값은 22050입니다.\nwav, sr = librosa.load(file_path) print(\u0026#34;Amplitude: \\n\u0026#34;, wav) print(\u0026#34;Sampling rate: \u0026#34;, sr) Amplitude: [-0.08001709 -0.07550049 -0.08358765 ... 0.08270264 0.10083008 0.10562134] Sampling rate: 22050 시각화를 통해 데이터가 아래와 같은 형태의 waveform을 띄고 있음을 확인해 볼 수 있습니다.\nfig = plt.figure(figsize = (14,5)) librosa.display.waveplot(wav, sr=sr) plt.ylabel(\u0026#34;Amplitude\u0026#34;) plt.show()    2. FFT (Fast Fourier Transform) 앞서 다룬 포스트에서 time-domain의 waveform을 FFT(Fast Fourier Transform)을 통해 분해(decompose)하고, frequency-domain으로 변환하여 원본 소리 데이터를 형성하는 주파수(frequency)의 정도를 파악하고 시각화 할 수 있다고 이해했습니다. 이번에는 numpy를 통해 앞서 구한 waveform amplitude에 FFT를 적용할 수 있습니다.\nFFT를 적용하여 시각화한 결과는 아래와 같습니다. 주로 1000Hz 이하로 많이 분포해 있는 것을 확인할 수 있네요.\nfft = np.fft.fft(wav) magnitude = np.abs(fft) frequency = np.linspace(0, sr, len(magnitude)) left_frequency = frequency[:int(len(frequency)/2)] left_magnitude = magnitude[:int(len(magnitude)/2)] fig = plt.figure(figsize = (14,5)) plt.plot(left_frequency, left_magnitude) plt.xlabel(\u0026#34;Frequency\u0026#34;) plt.ylabel(\u0026#34;Magnitude\u0026#34;) plt.show()    3. STFT (Short-Time Fourier Transform) STFT(Short-Time Fourier Transform)은 시간 정보가 유실되는 것을 방지하기 위해, 사전에 정의한 시간의 간격(window 또는 frame) 단위로 쪼개어 푸리에 변환을 적용하는 기법입니다. STFT는 librosa를 통해 적용할 수 있습니다. 이때, window의 크기(n_fft)와 window 간에 겹치는 사이즈(hop_length)를 설정해줍니다. 일반적으로는 n_fft의 1/4 정도가 겹치도록 설정한다고 합니다.\nn_fft = 2048 hop_length = 512 stft = librosa.stft(wav, n_fft = n_fft, hop_length = hop_length) spectrogram = np.abs(stft) print(\u0026#34;Spectogram :\\n\u0026#34;, spectrogram) Spectogram : [[1.42030740e+00 7.47260690e-01 5.37097007e-02 ... 1.88164175e-01 1.21684396e+00 2.43966293e+00] [1.24079692e+00 6.81115210e-01 6.51928782e-02 ... 2.08189130e-01 1.22743416e+00 2.52433753e+00] [1.09137118e+00 4.82469022e-01 1.85490116e-01 ... 6.99194148e-02 1.43492615e+00 2.53518319e+00] ... [3.84226470e-04 1.63909295e-04 9.80101977e-05 ... 2.23124225e-04 2.80503096e-04 1.98973445e-04] [3.00532440e-04 1.77996873e-04 1.29194887e-04 ... 9.72686321e-05 2.01086092e-04 9.30428141e-05] [2.59254826e-04 9.42422412e-05 5.96536411e-05 ... 7.49909232e-05 1.41018099e-04 1.10232315e-04]] 3.1. Spectogram STFT를 적용하여 구한 spectogram을 아래와 같이 시각화 해봤습니다. x축은 시간, y축은 주파수, 그리고 주파수의 정도를 색깔로 확인할 수 있습니다. 그런데 값이 너무 미세해서 차이를 파악하고 관찰하기 적합하지 않습니다.\nfig = plt.figure(figsize = (14,5)) librosa.display.specshow(spectrogram, sr=sr, hop_length=hop_length) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.plasma() plt.show()    3.2. Log-spectogram 그래서 보통 푸리에변환 이후 dB(데시벨) scaling을 적용한 Log-spectogram을 구합니다. 다분히 시각적인 이유뿐만 아니라, 사람의 청각 또한 소리를 dB scale 로 인식하기 때문에, 이를 반영하여 spectogram을 나타내는 것이 분석에 용이합니다.\nlibrosa.amplitude_to_db()를 통해 Log-spectogram을 구하여 시각화 한 결과입니다. 대부분의 에너지가 1024Hz이하의 낮은 주파수대역에 모여 있는 것을 볼 수 있네요.\nlog_spectrogram = librosa.amplitude_to_db(spectrogram) fig = plt.figure(figsize = (14,5)) librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length, x_axis=\u0026#39;time\u0026#39;, y_axis=\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;) plt.show()    4. MFCC 마지막으로 MFCC(Mel Frequency Cepstral Coefficient)를 구하고 시각화해보겠습니다. MFCC는 오디오 신호 처리 분야에서 많이 사용되는 소리 데이터의 특징값(Feature)으로, 사람의 청각이 예민하게 반응하는 정보를 강조하여 소리가 가지는 고유한 특징을 추출한 값입니다.\n마찬가지로 librosa.feature.mfcc()를 통해 feature값을 아래와 같이 추출할 수 있습니다. 파라미터 중 n_mfcc는 추출하고자 하는 mfcc의 개수입니다. 이번 실습에서는 13개로 설정했습니다.\nMFCCs = librosa.feature.mfcc(wav, sr = 22050, n_fft = n_fft, hop_length = hop_length, n_mfcc = 13) # number of coefficient we want to extract print(\u0026#34;MFCCs Shape: \u0026#34;, MFCCs.shape) print(\u0026#34;MFCCs: \\n\u0026#34;, MFCCs) MFCCs Shape: (13, 1293) MFCCs: [[-176.91516 -173.07141 -171.01598 ... -144.9992 -153.77185 -155.61522 ] [ 118.94415 117.39079 108.01162 ... 111.50748 108.44453 113.359665 ] [ -12.249197 -16.364796 -20.116379 ... -68.0366 -40.615326 -32.124104 ] ... [ -7.0000467 -8.825797 -10.732431 ... -16.528994 -17.69807 -21.954914 ] [ 10.861979 10.393564 7.8947186 ... -8.206779 -3.6493917 -4.4267316] [ -12.490692 -10.728968 -14.610505 ... -2.9667187 -12.053108 -9.9868355]] fig = plt.figure(figsize = (14,5)) librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length, x_axis=\u0026#39;time\u0026#39;,) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;) plt.show()    5. 한줄 요약  Librosa라는 킹갓제너럴 파이썬 패키지를 이용해서 소리 데이터를 불러오고, 변형할 수 있다!  6. Reference  audio-processing-wave by scpark20  ","permalink":"https://wonyoungseo.github.io/posts/2021-01-22-hands-on-preprocess-audio-data/","summary":"0. 데이터와 librosa 실제로 소리 데이터를 다뤄보기 위해서 음악 데이터를 준비하겠습니다. 음악 장르 분류 데이터셋으로 유명한 GTZAN Dataset을 다운받아 음악 파일을 하나 선택했습니다.\n그리고 Librosa는 오디오와 음악 분석을 위 빠질 수 없는 파이썬 패키지입니다. 음원이나 소리 파일을 불러와 waveform을 시각화 하거나 다른 형태로 변환할 수 있는 기능을 제공합니다.\n( Librosa는 pip install librosa 명령어를 통해 설치할 수 있습니다. )\nimport warnings warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt import IPython.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(3) - 파이썬으로 음악 데이터 읽어오기"},{"content":"2020년 올해는 \u0026hellip; \u0026amp;nbsp\n1. 올해의 가장 큰 변화 재택근무\nCOVID-19 방역에 다들 지쳐가고 서서히 경각심도 조금씩 희미해질 때쯤, 줄지 않는 확진자수에 결국 재택근무가 주기적으로 자리잡게 되었습니다. 사실 나는 개인적으로 재택근무가 별로라고 생각합니다. 집에서는 집중이 안 되기도 하고, 점심을 제 돈으로 해결해야 합니다. 집에서는 의자도 불편한데 새 의자를 사자니 가격이 만만치 않습니다. 재택근무를 원활하기 하기 위한 문화가 정착이 되지 않다보니, 으레 팀원들끼리 커뮤니케이션도 덜하게 되고 일하다가도 뭔지 모를 답답함을 느끼기도 했습니다.\n그래도 적응해야지 어쩌겠습니까. 춥지만 기분전환을 하기 위해 환기를 하거나 동네를 한바퀴 돌고 들어옵니다. 점심도 돈 아낀다고 직접 준비하기보다는 그냥 나가서 포장을 하구요, 나간 김에 바깥 공기도 좀 쐬고, 커피도 미리 테이크아웃을 해옵니다. 내려놓는게 있으니, 확실히 마음이 편해지는 부분이 확실히 있습니다. 커뮤니케이션도 마찬가지였습니다. 재택 근무 중 동료로부터 응답이 오지 않거나 슬랙에서 2~3 문장 이상의 커뮤니케이션이 이어질 때면, 바로 전화를 걸거나 구글 행아웃 링크를 바로 보내줍니다. 바로 어젠다에 대해 이야기를 나누거나, 바쁘면 조금 이따 이야기 하자고 답변이 옵니다. 전에는 왜 안 그랬는지 잘 모르겠네요. (대충 속이 시원해지는 개비스콘 짤.)\n부모님을 포함해서 제 주변에서는 재택하면 다들 와 좋겠다 하는 반응이지만, 그러나 저러나 저는 여전히 오피스가 더 좋기는 합니다. 그래도 시국이 시국인만큼 재택을 해야하니, 내년에도 계속해서 저만의 방식을 찾아가야겠습니다.\n\u0026amp;nbsp\n2. 올해의 깨달음 하\u0026hellip; 일단은 그냥 내가 하자 될 진 모르겠지만\n사실 2020년은 제가 속해있는 프로젝트의 부족한 부분과 기술부채를 해결했어야 하는 해입니다. 제가 혼자서 전부 하기에는 무리가 있던 부분이었고, 그래서 팀에 도움을 많이 요청했었습니다. 새로운 시니어 분이 오셔서 개발의 전반적인 부분을 손봐주시는 만큼 저도 기대하는 부분이 많았는데요. 하지만 영업으로 인해 생기는 ad-hoc 요청사항과 더 우선순위로 취급되는 프로젝트들이 등장하면서 기존의 프로젝트에는 제동이 걸렸습니다. 점점 우선순위에서 밀려나는걸 느끼게 되면서 스트레스를 많이 받기도 했습니다. 애매한 상태로 몇 개월이 그냥 지나가게 되었고 멘탈 관리도 조금 힘들었던 시기였습니다.\n그렇게 저는 지속적으로 서포트를 요청을 드리고, 이것도 해야하고 저것도 해야하는 상대방은 상대방대로 스트레스를 받았습니다. 교통정리가 되지 않은 채 다들 당장 눈 앞에 놓인 요청사항들을 해결하느라 경주마 같이 달리는 것을 보고, 이게 지금 조직의 체질이라는 것을 깨닫게 되었습니다. 체질이란 건 쉽게 바뀌지 않죠. 저도 지금 회사에서 2년을 일했으니, 그 체질이 형성되는데 기여하던 부분이 분명 있을 것 입니다.\n그래서 늦었지만 일단 저 혼자 어떻게든 해보기로 했습니다. 제가 도입해보고, 문서를 만들어서 공유를 조금씩 하고 있습니다. 시행착오가 많지만 이렇게라도 안 하면 아무것도 개선할 수 없을 것 같았습니다. 나중에 봤을 때 이게 좋아보이면 따라와 줄 것이고, 구리면 피드백을 해줄 것이라 생각합니다. 이도저도 아니고 그냥 아무 말도 없으면 \u0026hellip; 그건 일은 없기를 바랍니다 ㅠㅠ\n사실 이런 문제점은 마지막에 이렇게저렇게 해서 문제를 없앨 수 있었다! 하고 뿌듯하게 마무리 해야하는데, 그게 아니라 이제 시작인 것 같습니다. 그래서 2020년은 답답했고, 2021년에는 답답해하기보다 느려도 나 혼자 해보겠다. 인 것입니다.\n\u0026amp;nbsp\n3. 올해의 사서고생 방송통신대 두번째학기\n2020년 2월에는 방송통신대 정보통계학과에 3학년으로 편입했고, 어느 새 2번째 학기가 끝났습니다. 기존의 온라인 강의 방식을 고수하면서, 코로나로 인해 대면 기말고사는 기말과제물로 대체되었습니다. 호기롭게 7과목을 신청했다가 한 학기 내내 과제물로 허덕인 것이 많이 기억납니다.\n솔직히 방송통신대의 커리큘럼은 다른 4년제 대학교 동일 학부의 커리큘럼과 비교했을 때 깊이가 떨어지는 것이 사실입니다. 누군가에게는 우스워보일 수도 있을 것 같아요. 이렇게까지 해야하나 싶을 정도로 반복적이고 손이 많이 가는 노가다스러운 과제가 있는 반면, 영상 강의에서는 너무 성의없이 대충 짚고 넘어가서, 도저히 이해가 되지 않아 따로 찾아가며 공부해야 했던 과제도 있었습니다. 그렇지만 직장을 다니며 바쁜 와중에도 성실하게 강의를 듣고 과제를 제출했습니다.\n어제 성적이 나왔는데 모든 과목에서 A+를 받았습니다. 중고등학교, 대학교때는 정말 공부를 안 해서 상상도 못 했던 평점인데 말이죠. 사실 기존처럼 객관식 시험을 봤다면 이렇게 좋은 성적을 기대하지 못 했을 수도 있습니다. 성실하게 과제를 작성한 덕분인 것 같아요.\n이번 학기에 들었던 과목들 중 인상적이었던 몇몇 과목에 대해 조금 소감을 남겨보겠습니다.\n 파이썬과 R: 가볍게 생각한 과목이었고 난이도도 높지 않습니다. 하지만 담당이신 K교수님은 역시나 기대를 저버리지 않으시고 어마어마한 양의 과제를 내주셨습니다. 게다가 파이썬과 R로 동일한 문제를 풀고, 스크린샷을 찍고, 코드를 옮겨적고, 설명까지 적어야 하니\u0026hellip; 손가락이 많이 뻐근해졌달까요. 빅데이터의 이해: 이론적인 부분이 많고, 교재의 내용이 비교적 오래됐기 때문에 인상적이지 않았습니다. 하지만 과제물을 작성하면서 많이 생소했던 MapReduce에 대한 직접 그래프를 그려가며 작동원리를 알아갔던 부분과, 추천시스템 케이스 스터디를 하면서 제가 개인적으로 좋아하는 스포티파이에 대해 조사하고 정리했던 것이 정말 재미있었습니다. 시간 가는 줄 몰라서 시간 관리가 안 됐던 것은 함정 \u0026hellip; 통계학의 개념 및 제문제: 내용은 수리통계학인데 왜 과목명을 이렇게 정했는지 의문인 과목이었습니다. 그리고 역시나 많이 어려워서, 종강 후에도 마음을 놓지 않고 다시 관련 서적을 사서 제대로 공부해야겠다는 생각이 들었던 과목이었습니다.  아무튼 이렇게 방송통신대 정보통계학과에서의 두번째 학기가 지나갔습니다. 다음 학기부터는 통계,데이터과학과로 이름이 바뀐다고 하네요. 아마 다음 학기부터 저는 컴공 과목을 주로 듣게 될 것 같습니다.\n\u0026amp;nbsp\n4. 올해의 엉겁결 ADsP 자격증 취득\nADsP는 그냥 한번 봐볼까 하는 생각으로 접수했다가, 응시날이 코앞에 다가와 벼락치기를 했습니다. 퇴근길에 서점에서 교재 뒤쪽에 붙어있던 모의고사를 한번 풀어보는 방식으로 공부하고, 부족한 부분은 인터넷으로 찾아가며 부족한 부분을 위주로 준비했습니다.\nADsP는 데이터 이해, 데이터분석 기획, 데이터분석 의 세가지 과목으로 나뉘어져 있습니다. 응시일까지 과락을 걱정하며 가장 많이 걱정했던 과목은 데이터분석 기획 입니다. 데이터분석의 경우 방송통신대에서 수강한 데이터마이닝 과목과 겹치는 내용이 많기 때문에 걱정이 없었지만, 데이터 이해와 데이터분석 기획의 경우 실무에서는 많이 다루지 않는 암기위주의 이론적인 내용이 대부분이었기 때문에 체화하는 게 쉽지 않았고 오답이 많이 발생했었습니다.\n하지만 합격기준은 60/100점 이상과 과목별 40% 이상 취득 이기 때문에, 합격 기준만 넘자는 심정으로 선택과 집중을 한 결과, 합격을 할 수 있게 되었습니다. 엉겁결에 접수해놓고 너무 바빠서 완전히 까먹고 있었는데, 벼락치기 한 것 치고는 너무 다행이었던 그런 살짝은 부끄러운 자격증 취득이었습니다. 응시일이 생일날이었어서, 아침부터 투덜투덜 응시장 간 건 안 비밀 \u0026hellip;\n\u0026amp;nbsp\n5. 번외 어워즈 시상식 올해의 노래 뱃사공 - 다와가\n쓸데없이 아둥버둥 바득바득 치열하게 살고 있는 것은 아닌지, 나 자신을 다시 되돌아보게 해준 노래.\n\u0026amp;nbsp\n올해의 음식점 익선동 르블란서\n생일을 맞이해서 갔던 곳인데 코로나 스트레스가 날라갔던, 즐거운 시간만 기억나는 곳.\n\u0026amp;nbsp\n올해의 카페 커피온리 영등포구청점\n900원 아이스아메리카노. 재택하면서 아이스아메리카노를 매번 사먹어야 하는 걱정을 조금이나마 해소해준 테이크아웃 커피집.\n( + 커피 맛을 잘 몰라서 신경 안 씀)\n\u0026amp;nbsp\n올해의 IT기기 맥북 프로 16인치\n근데 이제 어마어마한 성능의 M1 맥북이 출시된.\n( + 그리고 떡락하는 중고가)\n\u0026amp;nbsp\n올해의 손떨림 생애 첫 대출로 1억을 넘어가는 후덜덜한 전세대출\n( + 그리고 대출받자마자 추락한 신용등급)\n\u0026amp;nbsp\n올해의 소망 COVID-19 종식과 해외여행\n\u0026amp;nbsp\n 2021년에 기대하는 것. 글또 5기 글또는 글쓰는 또라이가 세상을 바꾼다!고 하는 글쓰는 개발자 모임입니다. 4기에 이어 5기에 참여하고 있고, 글또 활동을 통해 2021년에도 글쓰는 습관을 기를 수 있도록 꾸준히, 제가 만족할 수 있는 퀄리티의 글을 쓰는 것이 목표입니다.\nCS 관련 기초 공부 방송통신대에서의 3번째 학기. 컴퓨터과학과 위주의 수강과목들 생각입니다. 그리고 알고리즘과 코딩테스트 준비도 할 계획입니다.\nSound of AI - Open Source Research Project Sound of AI는 Musimap의 시니어 데이터사이언티스트인 Valerio Velardo 씨가 운영하는 AI \u0026amp; Audio 커뮤니티입니다. Valerio의 튜토리얼을 따라 Audio와 AI에 관련된 공부를 하면서, 2021년에 진행될 Open Source Research 프로젝트에도 참여할 예정입니다. 아직 초기 단계이지만, 많은 기대가 됩니다.\n가짜연구소 스터디 가짜연구소는 머신러닝을 중심으로 스터디, 밋업 등의 이벤트가 이루어지고 있는 커뮤니티입니다. 2021년에는 가짜연구소의 스터디를 통해 캐글에도 도전해 볼 계획입니다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-12-31-review-2020-2nd-half/","summary":"2020년 올해는 \u0026hellip; \u0026amp;nbsp\n1. 올해의 가장 큰 변화 재택근무\nCOVID-19 방역에 다들 지쳐가고 서서히 경각심도 조금씩 희미해질 때쯤, 줄지 않는 확진자수에 결국 재택근무가 주기적으로 자리잡게 되었습니다. 사실 나는 개인적으로 재택근무가 별로라고 생각합니다. 집에서는 집중이 안 되기도 하고, 점심을 제 돈으로 해결해야 합니다. 집에서는 의자도 불편한데 새 의자를 사자니 가격이 만만치 않습니다. 재택근무를 원활하기 하기 위한 문화가 정착이 되지 않다보니, 으레 팀원들끼리 커뮤니케이션도 덜하게 되고 일하다가도 뭔지 모를 답답함을 느끼기도 했습니다.","title":"[KR] 2020년 하반기가 지났다"},{"content":"이번 포스트에서는 소리의 파형을 분석하기 위해 사용되는 기법인 푸리에 변환과 특징 추출값으로 사용되는 MFCC의 개념에 대해서 알아보겠습니다.\n1. 소리는 주파수의 합산    Piano in Waveform   \u0026amp;nbsp\n위의 이미지는 실제 피아노 소리 파일을 파형(waveform) 형태로 시각화 한 것입니다. 간단한 피아노 소리이지만 매우 복잡한 파형을 그리고 있는 것을 볼 수 있는데요. 사실 우리가 흔히 들을 수 있는 이러한 \u0026ldquo;소리\u0026quot;라는 것은 각기 다른 단일 주파수를 가진 무수히 많은 정현파(sinewave)가 합산되어 형성된 것입니다. 제 경우에는 처음에 이해가 잘 되지 않았는데, 이런 시각화들이 많은 도움이 되었습니다. 직관적인 이해가 되시나요?\n\u0026amp;nbsp\n2. Fourier Transform 무수히 많은 정현파의 모음이 \u0026ldquo;소리\u0026quot;를 구성한다는 것은 이해를 했습니다. 하지만 이와 반대로 \u0026ldquo;소리\u0026quot;를 분석하기 위해서는 복잡한 소리(complex sound)가 어떠한 단일주파수들로 이루어져있는지를 분해(decompose)해봐야겠죠. 이를 위해 원본 소리에 행하는 작업을 푸리에변환(Fourier transform) 이라고 합니다. 다른 말로는 푸리에변환을 통해서 어떤 정현파가 얼마나 원본 소리를 구성하는지 파악할 수 있습니다.\n2.1. Spectrum 파형의 형태를 띄는 원본에 푸리에 변환을 적용하여 산출되는 결과물은 **스펙트럼(spectrum)**입니다. 스펙트럼이란, 각 주파수의 정도를 시각화하여 보이는 기법입니다. 아래 그림은 파형에 푸리에변환 기법 중 하나인 FFT(Fast Fourier Transform)을 적용한 결과를 보여주고 있는데요, 스펙트럼에서 X축은 0~12000 가량의 주파수이고, Y축은 각 주파수의 진폭(amplitude) 또는 그 정도(magnitude)를 나타냅니다.\n푸리에 변환을 통해서 파형이 스펙트럼으로 표현될 때 주목해야하는 점은, 파형은 시간의 흐름에 따라 변화하는 time-domain의 성질을 띄는 반면, 스펙트럼은 각 frequency마다 그 정도가 달라지는 frequency-domain의 성질을 띈다는 것입니다. 아래 그림의 스펙트럼에서는 전체 9초 가량의 시간대에 대한 주파수와 진폭을 모두 보여주고 있습니다. 즉, 스펙트럼에는 시간의 흐름에 따른 정보는 유실되는 것이죠.\n   푸리에변환의 결과 예시   2.2. STFT **STFT(Short-Time Fourier Transform)**는 푸리에변환의 한계를 보완하는 기법입니다. STFT는 전체 길이보다는 짧은 어떠한 시간 간격(window)을 설정한 후, 이 간격을 시간의 흐름에 따라 움직여가며(slide) 복수의 변환을 행하여, 시간의 흐름에 따른 주파수 정보를 얻습니다. 쉽게 말해, 9초의 소리가 있다면, 1초 간격으로 쪼갠 후 1초 간격으로, 푸리에변환을 하는 식인 것이죠. STFT의 산출물은 주파수, 진폭과 더불어 시간의 정보도 포함된 스펙토그램(spectogram)입니다. 아래의 스펙토그램 시각화에서 X축은 시간, Y축은 주파수, 그리고 주파수의 정도가 데시벨(색깔)로 표현되었습니다.\n   STFT의 동작 예시   \u0026amp;nbsp\n3. Mel Frequency Cepstral Coefficient (MFCC) 실제 소리나 음성을 분석할 때는 푸리에 변환만을 하지 않고, 그로부터 차원울 축소하고 분석에 용이한 특징을 추출하는 과정(Feature Extraction)을 거칩니다. 특히 오디오 신호 처리 분야에서 많이 사용되는 소리 데이터의 특징값(Feature)으로는 MFCC가 있습니다.(MFCC의 세부적인 내용과 구하는 방법은 다른 포스트에서 따로 다루도록 하겠습니다) MFCC는 갖가지 신호가 합쳐 생성된 \u0026ldquo;소리\u0026quot;가 가지는 고유한 특징을 추출한 값이라는 장점이 있습니다. MFCC를 이해하기 위해서 멜-스펙토그램과 캡스트럼 분석에 대해서 짚고 가겠습니다.\n3.1. Mel-Spectogram \u0026amp; Mel-scale MFCC를 설명하기 전에 멜-스펙토그램(Mel-Spectogram)에 대해서 간단히 알아보겠습니다. 멜스펙토그램이란, 사람의 달팽이관의 특성을 반영한 Mel-scale을 적용한 스펙토그램 표현법입니다. 달팽이관은 저주파 대역을 감지하는 구간이 조밀하고, 고주파 대역을 감지하는 구간은 넓게 이루어져 있습니다. 따라서 저주파 대역에 의미 있는 정보가 집중되어 있으며, 인간의 청각은 저주파 대역에서 더 민감하게 반응을 한다는 점을 반영하여, 주파수의 대역에 차등적으로 중요도를 적용하는 Mel-scale이 제안 되었습니다.\n$$ \\text{Mel}(f) = 2595 \\log_{10} \\left(1+ \\frac{f}{700}\\right) $$\n3.2. Cepstral Analysis 캡스트럼 분석(Cepstral Analysis)이란, 스펙토그램으로부터 소리의 대표적인 특징을 추출하는 기법입니다. 소리가 큰 진폭으로 진동할 때, 이를 공명(resonance)라고 합니다. 예를 들어 성대가 떨림으로 인해서 어떠한 소리가 최초로 생성될 때, 이를 공명이라고 하죠. 스펙트럼에서 공명은 뾰족한 봉우리(peak)로 표현이 되고, 이 봉우리들을 포먼트(formant)라고 합니다. 우리가 듣는 소리의 대표적인 특징이 스펙토그램의 포먼트로 표현된다는 것이죠. 따라서 캡스트럼 분석은 아래의 그림처럼 스펙토그램으로부터 포먼트를 찾아서 분리하는 기법입니다. (Spectral 에서 spec을 뒤집으면 Cepstral 이 되는 것은 우연일까요?)\n캡스트럼은 스펙트럼에 역푸리에변환(IFT: Inverse Fourier Transform)을 취하여 구합니다.\n   Cepstral Analysis (https://brightwon.tistory.com/11)   3.3. MFCC 구하기 MFCC를 구하는 과정은 다음과 같습니다.\n  주어진 신호(오디오 데이터)를 일정한 간격(window)로 나누어 푸리에변환을 적용하여 스펙토그램을 구한다. 스펙토그램의 제곱(파워 스펙토그램)에 Mel-scale을 기반으로 필터뱅크(Filter banks)를 구성하고 필터뱅크에 로그 변환을 수행한다. 이렇게 하면 Mel-scale에 따라 나누어진 구간 별로 분포한 정보를 확보한다. 주어진 Log Mel-spectogram에 역푸리에 변환을 적용하여 Cepstrum을 구한다. MFCC는 이렇게 구한 결과의 진폭(amplitude)이다.   3.4. MFCC의 의미 MFCC를 구하기 위해 적용된 기법과 과정에 대해 알아보니 머리가 조금 아프네요(\u0026hellip;) 하지만 그 의미를 되새겨보자면 다음과 같이 요약할 수 있을 것 같습니다.\n MFCC는 무수히 많은 단일주파수로 복잡하게 구성된 소리로부터 (1)사람의 청각이 예민하게 반응하는 정보를 강조하고 (2)소리의 대표적인 부분만을 취사 선택한 피처(feature)이다.\n \u0026amp;nbsp\n4. 세줄 \u0026hellip; 아니 쓰다 보니 네줄 요약  소리는 무수히 많은 단일주파수의 합산으로 이루어져있다. 푸리에변환을 소리가 어느 주파수로 이루어져 있는지 분해할 수 있다. 푸리에변환만 수행하면 시간의 정보가 유실되므로 STFT를 적용한다. MFCC는 소리를 분석하는데 많이 사용되는 피쳐로, 인간의 청각에 유의미한 주파수대역을 강조하고, 소리의 대표적인 특징을 추출한 값이다.   이번 포스트에서는 푸리에변환과 MFCC의 개념에 대해서 간단히 알아보았습니다. 다음에는 파이썬으로 직접 소리 데이터를 읽고 처리하는 과정에 대해 정리해보겠습니다.\n\u0026amp;nbsp\n5. Reference  https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%AA%85 https://en.wikipedia.org/wiki/Mel-frequency_cepstrum#cite_note-2 https://en.wikipedia.org/wiki/Mel_scale https://en.wikipedia.org/wiki/Cepstrum https://brightwon.tistory.com/11 https://ratsgo.github.io/speechbook/docs/fe/mfcc https://tech.kakaoenterprise.com/66  ","permalink":"https://wonyoungseo.github.io/posts/2020-12-26-understanding-audio-data-techniques/","summary":"이번 포스트에서는 소리의 파형을 분석하기 위해 사용되는 기법인 푸리에 변환과 특징 추출값으로 사용되는 MFCC의 개념에 대해서 알아보겠습니다.\n1. 소리는 주파수의 합산    Piano in Waveform   \u0026amp;nbsp\n위의 이미지는 실제 피아노 소리 파일을 파형(waveform) 형태로 시각화 한 것입니다. 간단한 피아노 소리이지만 매우 복잡한 파형을 그리고 있는 것을 볼 수 있는데요. 사실 우리가 흔히 들을 수 있는 이러한 \u0026ldquo;소리\u0026quot;라는 것은 각기 다른 단일 주파수를 가진 무수히 많은 정현파(sinewave)가 합산되어 형성된 것입니다.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(2) - Fourier Transform, MFCC"},{"content":"1. 소리 데이터란 소리는 다음 과정에서 생산된 것을 의미합니다.\n (1) 어떠한 물체 또는 매질(object)의 진동(vibration)으로 인해 공기 입자들이 밀고 당겨지는 반복적인 과정(oscilation)에서 생긴 파동(wave) (2) 공기의 압력이 낮아지면 빈 공간이 생기면서 다른 입자들로 채워지고, 압력이 높아지만 입자들을 밀어내는, 밀고 당기는 반복적인 연쇄 작용(oscillation)으로 인해 생기는 파동(wave)  그리고 위에서 정의한 파동은 아래와 같은 파형(waveform)으로 나타낼 수 있습니다.\n   \u0026amp;nbsp\n2. 소리 데이터의 표현 2.1. 파형의 요소 파형을 통해서 우리는 다음과 같은 정보를 파악할 수 있습니다. 주로 파동을 표현하는 요소들이죠.\n    시간(time): 파동이 진행되는 시간. 주로 X축입니다. 진폭(amplitude): 진동의 중심에서 최대까지의 거리를 나타냅니다. 진동의 크기를 의미하고, $A$라고 나타냅니다. 주기(period): 1회 진동하는데 걸리는 시간입니다. \\(T\\) 라고 나타냅니다. 진동수(또는 주파수. frequency): 한 점이 1초동안 진동한 횟수. \\(f\\) 라고 나타냅니다.  주기와 진동수는 서로 역수인 관계를 보입니다. (\\(f = \\frac{1}{T}\\) 또는 \\(T = \\frac{1}{f}\\)) 따라서, 주기가 길수록 진동수는 작고 주기가 짧을 수록 진동수는 큰 관계를 보입니다.    \u0026amp;nbsp\n2.2. 파형의 수학적 표현 파형은 사인함수를 통해 다음과 같이 수학적으로도 표현할 수 있습니다.\n\\(y(t) = A \\sin(2 \\pi f t + \\phi)\\)\n \\(A\\) : 진폭(amplitude) \\(sin\\) : 사인함수 \\(f\\) : 진동수(frequency) \\(t\\) : 시간(time) \\(\\phi\\) : 위상(phase). 위상이란 진동체의 상대적인 위치변화를 나타내는 부분입니다. 위상값에 따라 파형이 왼쪽으로 또는 오른쪽으로 이동(shift)된 형상인지를 알 수 있습니다.  \u0026amp;nbsp\n2.3. 진폭, 진동수의 관계 진동과 진폭이 소리와 무슨 관계가 있는지 아직 많이 낯선데요. 우선 진동 수와 음의 높낮이(pitch)의 관계에 대해 알아보겠습니다. 진동수가 클 수록 음이 높아집니다. 즉, 진동한 횟수가 많을 수록 높은 음이 구현된다고 이해할 수 있습니다. 반대로 진동한 횟수가 적다면, 그만큼 음의 높이가 낮은 저음이 구현됩니다. 아래와 같이 정리해볼 수 있습니다.\n frequency \\(\\leftrightarrow\\) pitch  longer periods(\\(T\\)) \\(\\rightarrow\\) lower frequency(\\(f\\)) \\(\\rightarrow\\) lower pitch shorter periods(\\(T\\)) \\(\\rightarrow\\) higher frequency(\\(f\\)) \\(\\rightarrow\\) higher pitch    이번에는 진폭과 소리의 크기(loudness)의 관계에 대해 알아보겠습니다. 진폭이 클 수록 소리는 크고, 진폭이 작을 수록 소리도 작아집니다. 아래와 같이 정리해볼 수 있습니다.\n amplitude \\(\\leftrightarrow\\) loudness  larger amplitude(\\(A\\)) \\(\\rightarrow\\) louder smaller amplitude(\\(A\\)) \\(\\rightarrow\\) quietter    \u0026amp;nbsp\n3. ADC (Analog digital conversion) 우리가 듣는 소리 그 자체, 즉 아날로그 소리는 연속적(continuous)입니다. 따라서 소리를 분석하기 위해서는 아날로그 소리를 디지털적인 형태로 변환하는 작업을 거쳐야 하는데 이를 Analog digital conversion(ADC)이라고 합니다. 말 그대로 아날로그를 디지털로 변환하는 작업이죠.\n3.1. Sampling \u0026amp; Quantization ADC에는 샘플링(Sampling)과 양자화(Quantization)이라는 두 가지 과정을 통해 연속적인 아날로그 소리를 이산적인 데이터로 변환합니다.\n Sampling: 샘플링은 시간의 흐름에 따라 진행되는 연속적인 신호를 특정 주기(time intervial)에 맞추어 신호를 이산적인 시간으로 쪼개는 과정입니다. Quantization: 양자화는 샘플링되어 저장된 데이터를 연속적이지 않은 대표값으로 정수화하여 이산적인 값으로 변환합니다.     Müller, Fundamentals of Music Processing, Springer 2015   \u0026amp;nbsp\n3.2. Sample rate \u0026amp; Bit depth 샘플링과 양자화는 적용되는 방법에 따라, 변환된 데이터의 품질에 영향을 끼칠 수 있습니다. 예를 들어 샘플링 주기나 양자화가 너무 크게 적용된다면, 기존의 아날로그 소리가 지닌 세세한 정보가 손실될 수도 있습니다. 이러한 품질을 결정하는 기준에는 샘플링주기(Samplig rate)과 비트뎁스(Bit depth)가 있습니다.\n   Bit Depth and Sample Rate https://youtu.be/-0rIU9FHiU0    Sampling rate: 1초 동안 담긴 샘플의 개수. 샘플링주기가 높을 수록 시간의 흐름에 따른 신호의 손실이 적습니다. Bit depth: 진폭을 쪼개는 개수. 비트(bit) 개수가 클 수록 기존의 신호에 유사한 형태로 데이터를 저장할 수 있습니다.     \u0026amp;nbsp\n4. 세줄 \u0026hellip; 아니 쓰다 보니 다섯줄 요약 우리가 귀로 듣는 소리에 대한 간단한 정의와, 소리를 분석하기 위해 디지털적인 정보로 변환하는 작업에 대해 알아보았습니다.\n 소리는 공기의 진동(vibration)과 반복적인 움직임(oscillation)으로 인해 압력이 변화하면서 생긴 파형이며 연속적인 아날로그 신호이다. 아날로그 소리를 분석하기 위해서는 ADC를 통해 디지털적이고 이산적인 데이터로 변환한다. 샘플링(sampling)은 아날로그 소리의 시간흐름을 특정 주기로 쪼개어 변환하는 작업이며, 샘플링주기(sampling rate)에 따라 1초 동안 기록되는 샘플의 개수를 달리할 수 있다. 양자화(quantization)는 아날로그 소리의 크기를 특정 구간으로 쪼개어 변환하는 작업이며, 비트뎁스(bit depth)에 따라 그 구간의 세밀함을 조절할 수 있다. 샘플링 주기와 비트뎁스는 아날로그 소리를 디지털 데이터로 변환되는 품질을 결정하며, 세밀할 수록 정보의 손실을 줄이고 기존의 소리에 유사한 데이터를 기록할 수 있다.   다음 포스트에서는 파형을 분석하기 위해 사용되는 푸리에 변환 (Fourier Transform)에 대해서 정리해보도록 하겠습니다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-12-13-understanding-audio-data-sound-waveform-adc/","summary":"1. 소리 데이터란 소리는 다음 과정에서 생산된 것을 의미합니다.\n (1) 어떠한 물체 또는 매질(object)의 진동(vibration)으로 인해 공기 입자들이 밀고 당겨지는 반복적인 과정(oscilation)에서 생긴 파동(wave) (2) 공기의 압력이 낮아지면 빈 공간이 생기면서 다른 입자들로 채워지고, 압력이 높아지만 입자들을 밀어내는, 밀고 당기는 반복적인 연쇄 작용(oscillation)으로 인해 생기는 파동(wave)  그리고 위에서 정의한 파동은 아래와 같은 파형(waveform)으로 나타낼 수 있습니다.\n   \u0026amp;nbsp\n2. 소리 데이터의 표현 2.1. 파형의 요소 파형을 통해서 우리는 다음과 같은 정보를 파악할 수 있습니다.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(1) - Waveform, ADC"},{"content":"항상 내 맘 같지 않은 기술 블로그 개발이 비교적 익숙하지 않은 데이터분석가 또는 데이터사이언티스트가 Github Pages를 활용하여 기술블로그를 운영하기 위해서는 몇 가지 난관이 있습니다. 자료를 찾아보면 주로 Jekyll, Hugo, Hexo, Gatsby.JS 와 같이 낯설고 어려운 프레임워크을 사용해야 합니다. 튜토리얼은 간신히 따라갔다 하더라도, 기존의 테마를 내 입맛에 맞게 커스터마이징을 하거나 기능을 추가하기 위해서는 html, CSS 또는 NodeJS 같은 프레임워크를 알아야 합니다. 마음에 드는 테마가 있어도, 오랜 기간 관리가 되지 않아 Latex 엔진이 제대로 작동하지 않는 경우가 생기기도 하죠. 번거로웠던 적이 많았습니다. 그냥 조용히 다른 쉬운 플랫폼을 선택할 걸 하는 후회가 드는 순간도 있었습니다. 진짜 많은거 바라지 않으니, 가장 심플하게 기본만 딱 하는 테마는 없을까 찾아보기도 했구요. 이러한 와중에 fastpages를 접하게 되었습니다.\n 이번에는 fastpages    Source: https://github.com/fastai/fastpages   장점 fast.ai 에서 개발한 fastpages는 Jekyll을 기반으로 하는 툴입니다. 제가 생각하는 fastpages의 장점은 다음과 같습니다.\n 디자인이 군더더기 없이 깔끔하다. 부족한 것도 없지만 더 덧붙일 것도 없다. 따라서 코드를 수정할 필요가 없다.  Latex, Syntax highlighting 모두 깔끔하다. Altair, Plotly 와 같은 interactive visualization도 embedd 할 수 있다.   html을 쓰지 않아도 된다.  기존의 방식과 같이 markdown(.md) 파일로 블로그 포스트를 작성할 수 있다. jupyter notebook(.ipynb) 파일까지도 그대로 블로그 포스트로 변환해준다. (!!!!!) 아마 많이 사용할 일은 없겠지만, word 파일까지도 가능하다고 한다.   다른 프레임워크 기반 명령어가 따로 필요 없이, 그저 add, commit, push 명령어만으로 포스트를 간편하게 업데이트할 수 있다. 나머지는 Github Action을 배포과정 알아서 진행하기 때문.  단점 단점은 \u0026hellip; 아직까지는 없습니다. 단지, {github username}.github.io 형태의 주소는 지원하지 않고, {github username}.github.io/{user가 생성한 fastpages 블로그 레포지토리 이름} 을 통해서만 블로그가 생성된다는 점이 한계인 것 같네요.\n 한번 만들어봅시다 fastpages 블로그용 레포지토리 생성하기.  fastpages 공식 레포지토리에서 Use this template 버튼을 클릭하고 새로운 레포지토리를 생성합니다. 이때, 레포의 이름은 아무것이나 정해되 상관 없되, {계정명}.github.io는 피해야 합니다.  Generate a copy of this repo by clicking on this link. Name your repo anything you like except {your-username}.github.io.\n         \u0026amp;nbsp\n 잠시 기다리면, \u0026ldquo;Initial Setup\u0026quot;이라는 새로운 Pull Request가 자동으로 생성된 것을 확인할 수 있습니다.  GitHub Actions will automatically open a PR on your new repository ~ 30 seconds after the copy is created. Follow the instructions in that PR to continue.\n      \u0026amp;nbsp\nPR 가이드 따라가기  다음 링크에서 prive key와 public 키를 생성합니다. 이때, 옵션은 RSA와 4069를 선택 후 \u0026ldquo;Generate SSH-Keys\u0026rdquo; 버튼을 클릭합니다.  Create an ssh key-pair. Open this utility. Select: RSA and 4096 and leave Passphrase blank. Click the blue button Generate-SSH-Keys.\n      \u0026amp;nbsp\n 두번째 주어진 링크에서 새 \u0026ldquo;New repository secret\u0026rdquo; 버튼을 클릭하고, Value 입력 칸에 앞서 생성한 Private key 전체를 복사하여 붙여넣습니다. Name 입력 칸에는 \u0026ldquo;SSH_DEPLOY_KEY\u0026quot;라고 입력하고 저장합니다.  Navigate to this link and click New repository secret. Copy and paste the Private Key into the Value field. This includes the \u0026ldquo;\u0026mdash;BEGIN RSA PRIVATE KEY\u0026mdash;\u0026rdquo; and \u0026ldquo;\u0026ndash;END RSA PRIVATE KEY\u0026mdash;\u0026rdquo; portions. In the Name field, name the secret SSH_DEPLOY_KEY.\n   \u0026amp;nbsp\n  세번째 링크에서는 \u0026ldquo;Add deploy key\u0026rdquo; 버튼을 클릭하고, 앞서 생성한 Public key를 복사하여 붙여넣습니다. 이름은 아무렇게나 지정해도 된다고 합니다. 그리고 제일 아래 \u0026ldquo;Allow write access\u0026rdquo; 박스를 꼭 체크합니다.\n Navigate to this link and click the Add deploy key button. Paste your Public Key from step 1 into the Key box. In the Title, name the key anything you want, for example fastpages-key. Finally, make sure you click the checkbox next to Allow write access (pictured below), and click Add key to save the key.\n      \u0026amp;nbsp\n  마지막으로 PR을 merge합니다. 이후 fastpages 블로그가 배포되는 과정은 Github actions에서 아래와 같이 확인할 수 있습니다.         \u0026amp;nbsp\n 완료되면, {github username}.github.io/{레포지토리 이름} 에서 fastpages 기반의 블로그가 배포되었음을 확인할 수 있습니다.     \u0026amp;nbsp\n이것만 알아도 블로깅 문제 없다! (나머지는 저도 아직 잘 몰라요. 몰라도 크게 상관 없더라구요 \u0026hellip;)\n _config.yml  블로그의 이름, Latex사용 여부, 미리보기 여부, 태그 보여주기 여부 등을 설정할 수 있는 파일입니다. 자신의 SNS계정 버튼도 추가할 수 있고, description, pagination의 갯수 등의 사항들도 설정할 수 있습니다.   index.html  블로그 메인 페이지에 보여지는 컨텐츠를 작성하는 파일입니다. 저는 아무것도 입력하지 않았기에 디폴트로 작성되어 있던 fastpages 소개 텍스트를 삭제했습니다.   _pages/about.md  블로그의 자기소개 페이지 About페이지의 내용을 작성하는 파일입니다. markdown 포맷으로 작성하면 됩니다.   포스트를 저장하는 디렉토리는 아래 종류에 따라 달라지므로 디렉토리를 구분해서 작성하시면 됩니다.  _notebooks/ _posts/ _word/ images/    주의! 포스트를 작성할 때는 notebook, markdown, word 파일 포맷에 상관 없이 무조건 YYYY-mm-dd-{아무 이름} 형태로 작성되어야 fastpages가 이를 인식하고 파일을 html로 변환합니다.\n\u0026amp;nbsp\n블로그 로컬에서 확인하기 배포 또는 포스트 최종 업로드 전, 내가 작성한 포스트가 블로그에 잘 보여지는 지 확인하고 싶을 때가 있습니다. 이때는 make server 명령어를 칩니다. localhost:4000 에서 블로그가 생성되었음을 확인할 수 있습니다. (단, docker가 작동 중에 있을 때만 가능합니다.)\n    마무리 하며 기술 블로그를 초반에 시작했을 때는 정말 의욕은 많이 앞섰지만, 늘 \u0026ldquo;이 디자인은 마음에 들지 않아, 이건 이런 기능이 없어, 이건 계속 오류가 생겨\u0026quot;와 같은 쓸데 없는 생각들이 불쑥불쑥 튀어나왔던 것 같습니다. 하지만, 이번 기회에 fastpages가 아주 깔끔하면서도 만족스러워 다짜고짜 블로그를 바꿔버렸습니다. 이제 과거에 작성했던 글들도 슬슬 옮겨놔야할 것 같네요! 새로운 술은 새 포대에 담으라고 했던가요? 새로운 블로그를 만들었으니 글도 술술 써지기를 기대해봅니다!\n Reference  https://github.com/fastai/fastpages  ","permalink":"https://wonyoungseo.github.io/posts/2020-11-29-fastpages/","summary":"항상 내 맘 같지 않은 기술 블로그 개발이 비교적 익숙하지 않은 데이터분석가 또는 데이터사이언티스트가 Github Pages를 활용하여 기술블로그를 운영하기 위해서는 몇 가지 난관이 있습니다. 자료를 찾아보면 주로 Jekyll, Hugo, Hexo, Gatsby.JS 와 같이 낯설고 어려운 프레임워크을 사용해야 합니다. 튜토리얼은 간신히 따라갔다 하더라도, 기존의 테마를 내 입맛에 맞게 커스터마이징을 하거나 기능을 추가하기 위해서는 html, CSS 또는 NodeJS 같은 프레임워크를 알아야 합니다. 마음에 드는 테마가 있어도, 오랜 기간 관리가 되지 않아 Latex 엔진이 제대로 작동하지 않는 경우가 생기기도 하죠.","title":"[KR] fastpages에서 블로그 시작하기"},{"content":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp\n점검해보자 1. 회사일만 하지 않기 모호하지만, 매우 중요한 항목이다. 작년에는 회사에서 기존에 하던 일과 말도 안 되는 정부과제, 그리고 갑작스러운 혁신금융선정의 삼위일체가 합심하여 나에게 전신마사지를 해주는 바람에 야근과 주말출근을 밥 먹듯이 했다. 결국 기존에 받는 연차 15일과 야근, 주말 근무에 대한 대체휴무가 쌓여, 연말에는 잔여 연차가 23일이 되었다(??) 1년 간 쉰 기억이 나지 않고, 기존 연차보다 잔여연차가 더 많은 상태로 1년을 마무리 하게 되어, 이게 대체 누구를 위한 무슨 짓인가 싶기도 했다. (모든 게 힘듦으로 가득하던 시절 그래서 슬기롭게 내 생활을 챙기기로 했다. 그렇다고 아주 여유롭진 않지만, 너무 힘들지는 않을 만큼, 어떻게 보면 좀 노하우가 생긴 것 같은 삶을 살고 있다.\n\u0026amp;nbsp\n2. 데이터 사이언스 대회 도전   데이콘: 원자력 발전소 상태 판단 알고리즘 대회. 데이콘에서 주관하는 원자력 발전소 상태 판단 대회에 참가했다. 자세한 후기는 이미 블로그에서도 작성한 바 있다. 대회에서 중간에 포기하지 않고 꾸준히 참여했다는 점에서 소기의 목적을 달성했다고 생각하고, 좀 더 열심히 했다면 더 좋은 성적을 거둘 수 있었을 것이라 생각한다.\n  카카오 아레나: Melon Playlist Continuation. 사실 카카오 아레나의 멜론 플레이리스트 추천 대회는 정말 많은 관심을 가지고 있는 분야인 만큼 의욕이 매우 높았던 대회였다. 추천시스템에 대한 개념도 잘 없었기 때문에, 참가와 동시에 추천시스템에 대한 공부를 차근차근 하고자 했다. 하지만 참가만 걸어 놓은 상태로 그 이상의 제출은 해보지 못 하고 대회가 마무리 되어 많은 아쉬움이 남는다. 애초에 제출 이전부터 선행되어야 하는 추천시스템에 대한 공부가 지지부진 했기 때문에, 내가 생각하기에도 변명의 여지가 없는 좀 한심한 결과가 나오게 됐다.\n  \u0026amp;nbsp\n3. 간단한 웹어플리케이션 구축 연초부터 사이드 프로젝트로 여러가지 역할을 수행할 수 있는 유연한 웹어플리케이션을 구축하고자 하는 와중에 Streamlit을 경험하고, 이것저것 시도해보게 되었다. 글또에서도 관련된 포스팅을 작성한 바 있다. 프론트엔드에 대한 걱정 없이 순수 파이썬으로 매우 간편하게 개발할 수 있기 때문에, 실제로 직장 업무에서도 개발팀이 참고할 수 있도록 데이터 QC, 레이블링 기능에 대한 프로토타입이나 대시보드 등을 만들어서 사내 공유하거나 함께 사용하고 있다.\n\u0026amp;nbsp\n4. 독일 가족 방문하기 마지막으로 독일을 방문한건 2018년. 올해만큼은 꼭 시간을 내서 누나의 가족들이 사는 독일을 방문하고자 했지만 망할 COVID-19 때문에 패스하게 되었다. 내가 가야 그나마 조카들이 한국말을 덜 까먹는데, 이렇게 조카들의 한국말은 더욱 뒷걸음질 치는건가 싶다. 언제쯤 가능하게 될까. 이건 기약이 없어서 더욱 안타까운 항목이다.\n\u0026amp;nbsp\n5. 글또 솔직히 말하자면, 글또 생활은 생각보다 어렵다. 글을 잘 쓰는 것도 어렵고, 내가 잘 모르는 내용에 대해 정독하고 피드백이나 감상을 남기는 것도 쉽지는 않다. 모자라서 그런건 아니고 아마 처음해봐서 그럴 수도 있다. 그래서 더더욱 차감 없는 슬기로운 글또 생활을 영위하고자 노력하고 있다. 꾸역꾸역 \u0026hellip; 이라는 의성어는 좀 뉘앙스가 그렇지만, 어쨌든 그런 비슷한 상황이다. 나쁘게 말하면 꾸역꾸역이고 좋게 말하면 조금조금씩 발전을 꾀하는 존버라고 할 수 있겠다. 아무튼 현재까지는 예치금 차감은 없다. 요태까지 그래와고 아패로도 개속.\n\u0026amp;nbsp\n(+) 지난 3월 방송통신대 정보통계학과 3학년에 편입했다. 해외 대학원을 알아보던 중, 비전공자인 나는 데이터사이언스와 직접적인 연관이 있는 학과목의 수강학점이 없기 때문에 지원부터가 제한이 있다는 걸 알게 되어서, 이 부분을 타개해나갈 방법을 찾고 있었다. 경영과 연계된 MIS관련 대학원, 부분 수강, MOOC를 통한 크레딧 등 다양한 방법을 알아봤지만, 역시 편법은 없었고, 3학년으로 편입하여 정식 대학과목을 수강하는 것이 가장 확실했다. 직장생활과 병행하는 방법은 방송통신대 뿐이었는데, 한 학기 등록금이 30만원대로 매우 저렴했기 때문에, 부담 없이 경험해보는 셈 치고 편입을 결정했다.\n방송통신대는 대학교 특성상 COVID-19로 인한 타격도 1도 없이 매우 정상적으로 학사일정이 진행되었다. (1은 있었다고 볼 수도 있다. 기말고사만큼은 오프라인으로 이루어지는데, 이 조차 과제물로 전환되었기 때문.)\n계획에 없었던 편입이고 주변 사람들도 우려를 많이 표했지만, 직장생활 병행과 대학원 진학을 위한 장기 계획의 일부분인 만큼, 잃은 것보다는 얻은 것이 많은 한 학기였다. 방송통신대에 대한 자세한 후기는 이번 2학기가 끝난 이후 포스팅할 계획이다.\n\u0026amp;nbsp\n마무리 하며 난 나름 뭔가 많이 했다고 생각했는데, 막상 돌아보니 매우 소박했다는 걸 느꼈다. 동시에 남은 5개월과 2021년에 대한 구상도 할 수 있게 되어 의미있는 회고였다. 역시 글또를 하지 않았다면 이런 글도 남기지 않았을 거란 생각이 들기도 한다. 남은 5개월은 또 얼마나 빨리 지나갈까. 적절한 타이밍에 남은 연차를 마저 소진하고 (잘 휴식하겠다는 뜻) 글또도 잘 마무리 할 수 있는 (예치금 차감 당하지 않겠다는 뜻) , 그리고 방송통신대 두번 째 학기도 초심 잃지 않고 수강할 수 있는 (A+를 받겠다는 뜻) 하반기가 되었으면 한다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-08-02-review-2020-1st-half/","summary":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp","title":"[KR] 2020년 상반기가 지났다"},{"content":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)\n대회 개요    수력 원자력 공사인데 배경은 풍력 발전 ... ?   \u0026amp;nbsp\n원자력발전소 상태 판단 대회는 한국수력원자력(주)에서 제공한 발전소 모의 운전 데이터를 통해 원자력 발전소의 상태를 판단하는 것이 태스크로 주어진다.\n평가지표 평가 지표는 Log loss 이다. Log loss 값은 0 ~ 1 사이로 산출되며, 낮고 0에 가까울 수록 모델의 예측력이 좋음을 의미한다. (데이콘 측 평가지표 설명 영상)\n$$ \\text{logloss}(\\cdot) = \\frac{-1}{N}\\sum_i^N \\sum_j^M y_{ij} \\log{p_{ij}} $$\n데이터셋 원자력 발전소 모의 데이터는 기본적으로 828개의 발전소 운전 Train 데이터 파일과 각 파일에 부여된 198가지 상태 레이블이 매핑된 Label 파일이 주어진다. 압축을 해제하면 총 81GB에 달했다.\n![]({{ site.baseurl }}/images/2020-07-05-review-dacon-nuclear-competition/2.png)\n   \u0026amp;nbsp\n각각의 파일에 저장된 Train 데이터셋 위의 그림과 같으며, 발전소가 10분 동안 작동한, 즉 1행 당 1초 즉, 600행으로 이루어진 데이터가 주어진다. 모든 데이터셋은 발전소의 상태가 변하기 전 디폴트 상태_A(999) 와 상태가 변한 후 상태_B 데이터를 담고 있으며, 상태_A에서부터 시작된다. 데이터는 0초에서 15초 사이에서 상태가 변하기 시작한다. 따라서 데이터 상태의 변화가 0초에서 발생한다는 말은 상태_A가 없는 좌측의 데이터셋과 같고, 그 이외에는 우측과 같다고 보면 된다.\n   \u0026amp;nbsp\n다만 위의 그림과 같이, 실제로는 몇 초부터 상태가 변하는지에 대한 정보가 주어지지 않기 때문에, 각 데이터셋이 좌측과 같은지 우측과 같은지는 한 눈에 판단이 어렵다.\n데이터 전처리 EDA 과정에서 의미 있는 인사이트를 도출해내지 못 했다. 그리고 대회 경험이 적고 시간이 촉박했던 관계로, 바로 전처리에 돌입했다.\n Label이 999 인 경우 제외한다. 10분 간의 운전 데이터 기록 컬럼 내 unique한 값이 \u0026lt; 10 인 컬럼은 제외한다. 데이터셋 중 str타입의 데이터가 발생할 경우 NaN 치환한다. 마지막으로 NaN 데이터는 0으로 채운다. Train / Eval 데이터셋은 3:1의 비율로 분리한다.  #1 train = train[train[\u0026#39;label\u0026#39;]!=999].reset_index(drop=True) train_label = train.label train = train.drop([\u0026#39;id\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;label\u0026#39;], axis=1) #2 with open(\u0026#39;filter_col.txt\u0026#39;, \u0026#39;r\u0026#39;) as filehandle: list_ = filehandle.readlines() list_ = [col.replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) for col in list_] train= train[list_] # 3 for col in train.columns: if train[col].dtype != \u0026#39;float64\u0026#39;: train[col] = pd.to_numeric(train[col], errors=\u0026#39;coerce\u0026#39;) #4 train = train.fillna(value=0, axis=1) #5 X_train, X_valid = train_test_split(train, test_size = .25, random_state=42) y_train, y_valid = train_test_split(train_label, test_size = .25, random_state=42) 모델 모델은 LightGBM을 선택했다. Tabular dataset의 분류 문제에는 나무모형 기반의 모델이 가장 적합했고, LightGBM, XGBoost, Random Forest를 초기 테스트 한 결과 LightGBM이 가장 빠르고 성능이 좋았기 때문이다.\n부스팅 방법으로는 가장 기본적인 Gradient Boosted Decision Tree를 선택했다. 또한 데이터셋이 커서 연산이 큰 문제와 오버피팅을 방지하기 위해 bagging_fraction, feature_fraction 파라미터의 설정으로 데이터셋의 행과 열을 0.5 비율로 고정하여 학습 중 샘플링할 수 있도록 했다.\n그 외 objective, num_class, metric과 같이 대회의 목적에 맞게 변경한 파라미터를 제외하고는 default를 가져온 것들이 대부분이다.\nimport lightgbm as lgb X_train = X_train.to_numpy() X_valid = X_valid.to_numpy() y_train = y_train.to_numpy() y_valid = y_valid.to_numpy() train_data = lgb.Dataset(X_train, label=y_train) #, feature_name=X_train.columns) valid_data = lgb.Dataset(X_valid, label=y_valid) #, feature_name=X_valid.columns) param = { \u0026#39;objective\u0026#39;: \u0026#39;multiclass\u0026#39;, \u0026#39;num_class\u0026#39;: 198, \u0026#39;boosting\u0026#39;:\u0026#39;gbdt\u0026#39;, \u0026#39;num_leaves\u0026#39;:32, \u0026#39;max_depth\u0026#39;:20, \u0026#39;min_data_in_leaf\u0026#39;:20, \u0026#39;metric\u0026#39;:\u0026#39;multi_logloss\u0026#39;, \u0026#39;learning_rate\u0026#39; : 0.01, \u0026#39;verbose\u0026#39; : -1, \u0026#39;bagging_freq\u0026#39; : 1, \u0026#39;bagging_fraction\u0026#39; : 0.5, \u0026#39;feature_fraction\u0026#39; : 0.5, } evals_result={} num_round = 2000 lgbst = lgb.train(params=param, train_set=train_data, num_boost_round=num_round, valid_sets=[valid_data], evals_result=evals_result, early_stopping_rounds=1000, verbose_eval=10) lgbst.save_model(\u0026#39;model_lgb.txt\u0026#39;, num_iteration=lgbst.best_iteration) 결과 결과는 가채점 기준 36위 / 201팀, 최종 데이터셋 기준 채점 및 중복 및 부정 제출 등의 여부가 판결 뒤 산출된 최종 순위는 16위 / 187팀를 기록했다. 전체 참가팀만 놓고 보면 1091팀이지만, 실제로 제출한 팀은 20%에 그친 것을 확인할 수 있었다.\n   소감 무엇보다 아쉬운점은 초반의 의욕과는 달리, 데이터사이언스 대회의 best practice를 실습해보지 못 했고, 제출과 점수에 급급한 채로 마무리 했다는 점이다. 시간 부족과 의사소통 부재로 EDA 깊이 있게 하지 못 했고, Hyperparameter tuning과 grid search를 제대로 시행하지 못 햇다.\n마지막으로, 이번 원자력발전소 상태 판단 대회 참가는 데이터사이언스 관련 대회 경험을 쌓기 위해서였지만, 한편으로는 전혀 접하지 못 했던 원자력발전소 관련 데이터를 접하고 발전소 관련 도메인을 조금이나마 얻기 위함이기도 했다. 그러나 데이터셋의 컬럼은 모두 비식별 처리가 되어 있어 무엇을 의미하는지 데이터의 특성과 정보에 대한 접근이 불가했다는 점이 대회를 참가하는 와중에 흥미가 조금 깎이게 된 요인이 되지 않았나 하는 생각이 든다.\n어쨌거나 저쨌거나 완주를 했고, 상위 10% 내 라는 기대도 하지 않았던 성적으로 마무리를 했기 때문에 여기에 의의를 두며, 다음에 참가하는 데이터사이언스 대회는 이번에 아쉬웠던 점들이 꼭 보완될 수 있도록 다짐을 해본다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-07-05-review-dacon-nuclear-competition/","summary":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)","title":"[KR] 데이콘 원자력발전소 상태 판단 대회 후기"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다. (영상)\n원본의 Figure. The Wagon Wheel Effect에서 하단의 sampling rate 슬라이더를 조절하며 왼쪽의 시계방향으로 회전하는 물체와, 오른쪽의 sampling rate에 따른 스냅샷(혹은 샘플)를 관찰해보자\n   \u0026amp;nbsp\n직관적으로 봤을 때, 왼쪽과 마찬가지로 오른쪽 스냅샷도 시계방향으로 회전하는 듯한 모션을 취하기 위해서는 최소한 왼쪽에서 한번 회전하는 동안 2번 이상의 스냅샷을 찍어야 한다. 만약 sampling rate을 엄청 낮게 설정한다면 스냅샷은 기존의 회전 방향과는 반대로 반시계 방향으로 찍히게 된다.\n회전 방향이 결정되지 않고 정지해 있는 듯한 특이한 케이스도 있다. 만약 sampling rate을 한바퀴당 1번으로 설정한다면 스냅샷에는 움직임이 없을 것이고, sampling rate을 한바퀴당 2번으로 설정한다면 스냅샷은 그냥 앞뒤로만 움직이기 때문에 진행 방향을 유추할 수 없게 될 것이다.\nSine Wave Aliasing : Multiples of the sampling rate Sive wave에는 다음과 같은 법칙이 있다.\n sampling rate이 \\(SR\\) 헤르쯔와 정수 \\(K\\)가 주어졌을 때,\n\\(F\\) 라는 frequency를 지닌 sine wave가 있고, \\(F+(k * SR)\\) 의 진동수를 지닌 sine wave를 샘플링했다고 할 때, 둘은 서로 구별이 가능하지 않다.\n 예를 들어, Sampling rate이 6Hz라고 했을 때, 다음 두 그룹의 sive wave는 서로 구별이 가능하지 않다.\n 샘플링을 거친 1Hz의 sine wave 샘플링을 거친 다음 세 가지의 sine wave  \\( 1+(1 * 6) = 7Hz \\) \\( 1+ (2 * 6) = 13Hz \\) \\( 1+(3 * 6) = 19Hz \\)    (사실 여기까지는 조금 이해가 안 갔는데 \u0026hellip;)\n원본의 Figure. All Sampled Sine Waves Have Aliases을 보면 더욱 직관적이다. Figure 1.의 Time domain 그래프는 1Hz sine wave(파랑색)와, 그 외 sine wave(회색)을 비교하고 있다. 샘플링을 거치고 나면(Frequency domain 그래프), 샘플링이 표현하는 부분은 파랑색과 회색 다 일치하기 때문에, 샘플링을 한 뒤에는 각각 어떤 frequency 였는지 구분을 할 수 가 없게 된다.\n   \u0026amp;nbsp\n이 말인즉슨, 샘플링을 거친 sine wave는 무한대의 alias를 가진다는 것이다. 그저 기존의 frequency에 sampling rate의 배수만 더해주면 기존 frequency에 대한 새로운 alias가 형성된다. 따라서 이 법칙은, 어떠한 신호라도 샘플링을 거치면 다른 샘플링 신호와 구별이 가능하지 않는 상황이 올 수 있다는 것을 의미한다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-06-20-dsp-basic-s01-9/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Wagon Wheel Effect \u0026 Aliasing"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.\n 특정 신호 내 가장 높은 frequency를 알고 있다면, 샘플링 주기를 그 두배로 설정하면 된다는 뜻이다. 예를 들어 300Hz sine wave 를 샘플링하려 한다면, 우리는 두배 이상 즉, 600Hz 를 샘플링해야 한다. 반대로 샘플링 주파수가 두배보다 작을 경우, 간섭이 일어나며 앞에서 언급한 aliasing(참고)이 발생하게 된다.\n신호 위주가 아닌 sampling rate 위주로 법칙을 정하면 다음과 같다.\n 샘플링 주기 FS hertz에 대하여, FS/2 혹은 그 이상인 frequency를 가진 신호는 적절하게 샘플링할 수 없다.\n 특히 sampling rate의 절반 즉, FS/2 라는 값은 Nyquist Limit 또는 Nyquist Frequency라고도 불리며, 샘플링 법칙을 논할 때 매우 중요하게 다뤄지는 값이다.\n원글의 Figure 1.(링크)의 Sampling period 값을 조절하면서 직관적으로 이해해보자. Sampling rate이 16Hz일 경우, FS/2는 8이다. 모든 네가지 sine wave의 경우 8을 넘지 않으므로 샘플링이 적절하게 가능하다.\n   \u0026amp;nbsp\n하지만 sampling rate을 4Hz으로 설정하면 FS/2의 값은 2이다. 따라서 2Hz, 3Hz, 4Hz의 sine wave는 제대로 샘플링 되지 않는다.\n   \u0026amp;nbsp\n지난 글에서 인간은 20Hz ~ 20,000Hz 사이의 소리만 들을 수 있다고 말한 바 있다. 따라서 샘플링 법칙에 따라 인간이 들을 수 있는 범위 내에서 음악의 sampling rate를 정하고자 하면, 20,000의 두배 즉 40,000Hz 이상이 되어야 한다. 오디오와 음악의 주파수가 40,000Hz 근처(정확하게 말하자면 44,100Hz)인 것은 이 이유 때문이다.\n\u0026amp;nbsp\nNyquist Frequency를 초과하는 경우 샘플링 법칙은 Nyquist Frequency(샘플링주기의 절반)에 한해서는 어떤 신호든 정확히 샘플링할 수 있고, 반대로 샘플링하고자 하는 샘플이 Nyquist frequency보다 높은 frequency를 가지고 있다면, 이 샘플링이 제대로 이루어지지 않는다는 것을 알게 되었다. 그렇다면 Nyquist Frequency를 초과하는 신호는 샘플링이 이루어지지 않는 이유에 대해서 알아보자.\n원글의 Figure 1.(링크)에서 sampling rate은 24Hz이고 Nyquist frequcny는 24Hz의 절반인 12Hz이다. 플레이 했을 때, 신호가 Nyquist frequency를 지나면, 신호의 샘플들은 하나의 파랑색의 파형 뿐 아니라 새롭게 생성된, 점점 감소하는 frequency 형태의 회색의 파형까지도 sampling 결과를 통해 표현이 가능하게 된다.\n   \u0026amp;nbsp\n이렇게 신호의 frequency를 제한하지 않고 Nyquist limit을 넘도록 허용하면, 샘플링된 신호는 Nyquist frequency가 투영된 새로운 sine wave를 표현하게 된다. 결국 실제로는 존재하지도 않는 회색 신호를 샘플링에 포함시키게 되는 결과를 낳게 되는 것이다.\n다음 포스팅에서 alias에 대한 다른 예시를 더 깊게 다루어서 정리를 할 예정이다.\n Additional material https://youtu.be/5wyYgy6LPyQ\n","permalink":"https://wonyoungseo.github.io/posts/2020-06-07-dsp-basic-s01-8/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 나이퀴스트 샘플링 법칙 (Nyquist Sampling Theorum)"},{"content":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.5 ~ 5점 사이로 평가를 내릴 수 있다. 넷플릭스에서 보유한 사용자 평가 데이터에는 MAP가 사용될 수 있지만, 왓챠에서는 MAP가 적절하지 않을 것으로 생각된다.\n반면에 nDCG(Normalized Discounted Cumulative Gain)의 경우 복수의 컨텐츠가 relevance를 가지고 있다고 하더라도, 그 정도(점수)에 따라 어떠한 컨텐츠가 더 관련있고, 덜 관련 있는지를 평가할 수 있다. 대부분의 검색과 추천시스템의 경우 다수의 사용자들은 1, 2페이지 또는 상위의 리스트만 참조할 것이기 때문에 상위 리스트 사이에서 변별력을 갖춰야 하는 경우 nDCG를 통한 평가가 설득력을 얻는다.\n\u0026amp;nbsp\nCumulative Gain (CG) nDCG에서는 우선 CG이란 개념이 등장한다. CG는 Cumulative Gain 이라는 이름에서도 알 수 있듯, 전체 추천된 리스트에 대하여 gain의 총 합을 구한 것이다.\n$$CP_{p} = \\sum_{i=1}^{p} rel_{i}$$\n \\(p\\) : 추천된 아이템 \\(rel_{i}\\) : i번 째 아이템의 relevance 정도. Gain 이라고 한다.  예시) 추천시스템의 결과와 Relevance\n   Rank Relevance     1 3   2 3   3 3   4 4   5 2   6 2    $$CG = 17$$\n\u0026amp;nbsp\nDiscounted Cumulateive Gain (DCG) DCG 에서는 각 추천된 아이템의 relevance를 log함수로 나누어 값을 구한다. log 함수로 나누어주는 부분은, 랭킹의 위치에 따른 페널티를 주는 효과를 가진다. 순위의 값이 클 수록(즉, 순위가 낮을 수록) DCG의 값은 작아진다. 하지만, 높은 순위의 경우 간격이 크고, 낮은 순위의 경우 실제 체감하는 차이는 낮다.\n$$DCG_{p} = \\sum_{i=1}^p \\frac{rel_i}{log_2(i+1)} = rel_1 + \\sum_{i=2}^p \\frac{rel_i}{log_2i}$$\n예시) 각 추천된 컨텐츠 당 Discounted Gain\n   Rank Relevance Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\)   2 3 \\(3 / \\log_2(2+1) = 1.89\\)   3 3 \\(3 / \\log_2(3+1) = 1.5\\)   4 4 \\(4 / \\log_2(4+1) = 1.72\\)   5 2 \\(2 / \\log_2(5+1) = 0.77\\)   6 2 \\(2 / \\log_2(6+1) = 0.71\\)    $$DCG = 9.59$$\n\u0026amp;nbsp\nNormalized DCG (nDCG) DCG는 현재 추천시스템이 추천한 결과에 대한 상태를 보여주는데, DCG만 놓고 볼 경우 추천된 아이템의 갯수에 따라 DCG가 다를 수 있으므로, 이를 0~1사이의 값으로 정규화 해줄 필요성이 있다. 따라서 현재 추천된 리스트의 결과에 기반한 DCG를 현재 추천된 결과의 가장 이상적인 형태를 가정했을 때의 DCG(ideal DCG, iDCG)로 나누어서 정규화한다.\n사용자가 컨텐츠를 평가하지 않은 경우와, 관련성이 아예 없는 경우 manual적으로 값을 0으로 설정하거나, 적절하게 imputation을 취해주어야 한다는 취약점이 있다. 하지만, nDCG는 relevance가 등급이나 범위로 매겨지거나(graded relevance), 이분법적인 경우(binary relevance) 둘 다 평가가 가능하다. 또한 log함수를 통해 순서에 대한 가중치가 주어지므로 추천시스템에 적용하기 매우 적절한 평가지표라고 할 수 있다.\n$$IDCG_p = \\sum_{i=1}^{|REL_p|} \\frac{2^{rel_i} - 1}{log_2(i+1)}$$\n$$nDCG_p = \\frac{DCG_p}{IDCG_p}$$\n예시) Ideal Relevance와 Discounted Gain\n   Rank Relevance Discounted Gain Ideal Relevance Ideal Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\) 4 4   2 3 \\(3 / \\log_2(2+1) = 1.89\\) 3 1.89   3 3 \\(3 / \\log_2(3+1) = 1.5\\) 3 1.5   4 4 \\(4 / \\log_2(4+1) = 1.72\\) 3 1.16   6 2 \\(2 / \\log_2(6+1) = 0.71\\) 2 0.71   5 2 \\(2 / \\log_2(5+1) = 0.77\\) 2 0.77    $$ nDCG_p = \\frac{DCG_p}{IDCG_p} = \\frac{9.59}{10.03} = 0.95 $$\nReference  위키피디아 MRR vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them  ","permalink":"https://wonyoungseo.github.io/posts/2020-05-10-normalized-discounted-cumulative-gain/","summary":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.","title":"[KR] 추천시스템의 평가 지표 : nDCG"},{"content":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)\n추천 시스템을 통해 추천되는 아이템의 경우 추천의 정도가 동일하지 않다. 대부분의 추천 결과는 다음처럼 나올 수 있다고 생각해 볼 수 있다.\n 1순위 : 가장 관심을 가질만한 것.\n2순위 : 그 다음 차선책으로 관심을 가질만한 것.\n3순위 : 그 다음으로 사용자가 관심을 가질만한 것.\n4순위 : 또 그 다음 \u0026hellip; \u0026hellip;\n \u0026amp;nbsp\nMean Average Precision (MAP) 은 순서 또는 순위를 감안하는 부분을 반영하여 추천 시스템의 성능을 평가하는 지표로서, 과거 캐글의 Stander Product Recommendation, 카카오아레나의 브런치 사용자를 위한 글 추천 대회 등 추천 시스템 관련 컴퍼티션에서 채점 방식으로 적용되었다. 특히 분류 문제에서 흔히 언급되는 Precision과 Recall이 적용된 성능평가 방법으로, 아주 낯설지는 않다.\n\u0026amp;nbsp\nPrecision \u0026amp; Recall     Predict Positive Predict Negative     Actual Positive True Positive False Negative   Actual Negetave False Positive True Negative    MAP에 대한 개념는 Precision과 Recall에서부터 시작한다. 일반적으로 위와과 같이 confusion matrix가 있다고 할 때, Precision과 Recall은 다음과 같다. (더 자세한 설명은 링크를 참조하도록 하자)\n$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$$\n$$\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$$\n\u0026amp;nbsp\n추천시스템 관점에서의 Precision \u0026amp; Recall 추천시스템에서는 Precision과 Recall을 다음과 같이 해석할 수 있다. 추천시스템에서는 분자 부분을 relevant(관련있는) 라고 표현하기도 한다.\n     Precision 또는 \\(P\\):\n 추천한 아이템 중, 실제로 사용자의 관심사와 겹치는 아이템의 비율 \\(\\text{Precision} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{Total items from recommendation}}\\)    Recall 또는 \\(r\\):\n 실제로 사용자가 관심을 가진 아이템 중, 추천된 아이템이 겹치는 비율 \\(\\text{Recall} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{User\u0026rsquo;s interest}}\\)    \u0026amp;nbsp\nCutoff (@K) MAP에서는 Cutoff의 개념이 등장한다. Cutoff는 \u0026ldquo;잘라낸다\u0026quot;는 뜻으로, 쉽게 말하면 \u0026ldquo;상위 K개만 고려하고 그 아래로는 쳐내기\u0026rdquo; 라고 이해하면 된다. Cutoff를 가질 경우에는, @K 를 덧붙여서 표기한다.\n어떠한 사용자의 기록을 통해서 자동차 용품와 관련된 아이템을 추천한 결과가 다음과 같다고 하자.\n   순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 운전자 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    다음 예시의 추천시스템의 결과에 대하여 \\(k\\)개 Cutoff를 적용하여 Precision을 구한다면, 이를 Precision@K라고 한다. Precision@K는 Cutoff에 따라 달라질 수 있다.\n\u0026amp;nbsp\n Cutoff k=10인 경우 \\(\\rightarrow P(k=10) = 0.6\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 자동차 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    \u0026amp;nbsp\n Cutoff k=3인 경우 \\(\\rightarrow P(k=3) = 1\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답    \u0026amp;nbsp\n Cutoff k=5인 경우 \\(\\rightarrow P(k=5) = 0.8\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답    \u0026amp;nbsp\n Cutoff k=7인 경우 \\(\\rightarrow P(k=7) = 0.714\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답    \u0026amp;nbsp\nAverage Precision (AP@K) Cutoff가 \\(K\\)개인 Average Precision(AP@K)은 Precision@K의 평균을 구하는 과정이다.\n$$ AP@K = \\frac{1}{m} \\sum_{j=1}^K P(j) \\cdot rel(j) \\dots \\begin{cases} rel(j)=1 \u0026amp; \\text{if } j^{th} \\text{ item is relevant}, \\cr rel(j)=0 \u0026amp; \\text{if } j^{th} \\text{ item is not relevant}, \\cr \\end{cases} $$\n \\(K\\) : Cutoff 갯수 \\(m\\) : 추천 아이템 중 relevance가 있는 아이템의 갯수 (number of relevant document) \\(j\\) : 전체 추천 아이템 리스트 중, 해당 추천 아이템의 index \\(P(j)\\) : \\(j\\)번째 까지의 precision값 \\(rel(j)\\) : \\(j\\)번째의 relevance 여부  \u0026amp;nbsp\n위에서 예시로 들었던 자동차용품 추천결과를 통해, [\\(AP@5\\), \\(AP@7\\), \\(AP@9\\), \\(AP@10\\)] 을 계산해 보았다. 특히 \\(AP@7\\) 와 \\(AP@9\\) 의 결과에서 관찰할 수 있듯이, \\(\\frac{1}{m}\\)에서 \\(m\\)은 relevance가 있는 경우만을 포함하기 때문에, 뒤에서 오답이 추가되어도 AP의 값이 페널티를 받지는 않는다.\n$$AP@5 = \\frac{1}{4} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5}\\right) = 0.95$$\n$$AP@7 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} \\right) = 0.90$$\n$$AP@9 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 \\right) = 0.90$$\n$$AP@10 = \\frac{1}{6} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 + \\frac{6}{10}\\right) = 0.85$$\n\u0026amp;nbsp\n또한, 아래의 예시에서 Case A와 Case B를 비교해보면, 순위가 높은 추천 아이템이 정확할 수록 높은 AP값이 계산되므로, 추천의 순서 또는 순위가 평가 지표에 영향을 끼침을 알 수 있다.\n$$AP@5 = \\frac{1}{3} \\cdot \\left(\\frac{1}{1} + 0 + \\frac{2}{3} + 0 + \\frac{3}{5}\\right) = 0.75 \\dots \\text{(Case A)}$$\n$$AP@5 = \\frac{1}{3} \\cdot \\left(0 + \\frac{1}{2} + 0 + \\frac{2}{4} +\\frac{3}{5} \\right) = 0.53 \\dots \\text{(Case B)}$$\n\u0026amp;nbsp\nMean Average Precision (MAP@K) AP는 각각의 사용자(또는 쿼리)에 대하여 계산한 것이므로, 각 사용자에 따라 AP값이 산출된다. Mean Average Precision(MAP)은 AP값들의 Mean을 구한 것으로, 식은 다음과 같다.\n$$MAP@K = \\frac{1}{U} \\sum_{u=1}^{U} (AP@K)_u$$\n \\(U\\) : 총 사용자의 수  \u0026amp;nbsp \u0026amp;nbsp\n마무리하며 이번 MAP에 대해 알아보았다. 다음 포스트에서는 역시나 추천시스템의 평가지표로 자주 등장하는 DCG(Discounted Cumulative Gain)에 대해서 공부하고 정리 할 예정이다.\nAP, MAP의 파이썬 코드로 된 구현체는 링크를 통해 참조할 수 있다.\n\u0026amp;nbsp \u0026amp;nbsp\nReference   https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\n  http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n  http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html\n  ","permalink":"https://wonyoungseo.github.io/posts/2020-04-23-mean-average-precision/","summary":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)","title":"[KR] 추천시스템의 평가 지표 : MAP"},{"content":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움. 하지만 아무도 두려워하지 않는 듯해 보여서 더 두려운 고요한 두려움.\n\u0026ldquo;대체 언제까지 슬랙으로 엑셀 파일을 주고 받아야 하는가?\u0026ldquo;에 대한 해답을 찾던 중, 구글 스프레드시트(Google Spreadsheet)와 연동하는 것으로 몇가지 고민을 해결할 수 있게 되었다. gspread라는 라이브러리를 찾게 되었다. 별 거 아닌데, 왜 여태 사용해보지 않았을까. 이거다 싶었다.\n   바로 적용한 아주 간단한 예시를 들자면, 리서쳐들이 구글 스프레드시트에서 작업한 것들을 DB에 적재하는 과정을 자동화할 수 있었고, 데이터를 누가 작업했고, 업데이트가 되었는지에 대한 여부 또한 스프레드시트를 기반으로 작업하게 되니 해결 되었다. Python과 gspread 통해서 구글 스프레드시트와 연동하는 과정을 다룬 사내 튜토리얼을 다시 정리해보았다.\n\u0026amp;nbsp\n1. 튜토리얼 1.1. GCP에서 사용자 인증 설정 Python으로 스프레드시트를 연동하기 위해서는, GCP(Google Cloud Platform)에서 사용자 인증과 API 사용 인증이 준비되어야 한다.\n\u0026amp;nbsp\n1.1.1. 구글 클라우드 플랫폼에 로그인  회사에서는 팀원들과 드라이브를 공유하기에 Gsuite 계정으로 로그인했다.     \u0026amp;nbsp\n1.1.2. 새로운 프로젝트 생성  구글 드라이브, 구글 스프레드시트와 연동할 새로운 프로젝트를 새로 생성한다.              \u0026amp;nbsp\n1.1.3. 구글 드라이브 API 사용 설정  새로 생성한 프로젝트를 선택하고, API 개요 이동하여 구글 드라이브 API 사용을 설정한다.                     \u0026amp;nbsp\n1.1.4. 구글 드라이브 API에 대한 인증정보 생성.  구글 드라이브 API를 사용함에 있어서 필요한 사용자 인증정보를 추가한다. 새 서비스 계정을 만들면, 서비스 계정 및 Key가 JSON 형태의 파일이 받아지게 된다.  특히 추후 파이썬 코드 내에서 Key가 필요하게 되니, 적절한 디렉토리에 저장하도록 한다.                 \u0026amp;nbsp\n1.1.5. 구글 스프레드시트 API 사용 설정  구글 드라이브 API 사용 설정한 방법과 동일하게, 구글 스프레드시트 API 사용설정을 활성화 한다.        \u0026amp;nbsp \u0026amp;nbsp\n1.2. 파이썬 패키지 설치  파이썬에서는 gspread와 oauth2client가 필요하다.  gspread: 파이썬을 통해 구글 스프레드시트와 연동하고, 제어할 수 있게 하는 패키지. oauth2client: OAuth2을 통해 사용자 인증을 하기 위해 설치함.   터미널 또는 CMD에서 pip를 통해 설치하도록 한다.  pip install gspread pip install --upgrade oauth2client \u0026amp;nbsp \u0026amp;nbsp\n1.3. 구글 스프레드시트 파일 설정  파이썬을 통해서 접근하고 연동하고자 하는 해당 구글 시트에서 사용자 인증 정보를 설정하다. 앞서 내려 받은 JSON파일에서 client_email의 값을 복사한 뒤, 해당 스프레드시트 우측 상단 공유를 클릭한 뒤 입력하여 권한을 부여한다..         복잡하지는 않지만, 스프레드시트마다 이 작업을 해주어야 한다는 점이 약간 번거로운 부분이다. 공유 권한부여까지 완료되었다면, 이제 파이썬으로 스프레드시트의 데이터를 읽어와보자!  \u0026amp;nbsp \u0026amp;nbsp\n1.4. 파이썬에서 테스트 1.4.1. 인증과 연동  사용자 인증 파일(JSON)을 통해 연동을 한다.  from oauth2client.service_account import ServiceAccountCredentials scope = [\u0026#34;https://spreadsheets.google.com/feeds\u0026#34;, \u0026#34;https://www.googleapis.com/auth/spreadsheets\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive.file\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive\u0026#34;] creds = ServiceAccountCredentials.from_json_keyfile_name(\u0026#34;{your_JSON_filename}.json\u0026#34;, scope) \u0026amp;nbsp\n1.4.2. 스프레드 시트 선택  테스트를 위해서 샘플 스프레드시트를 생성한 상태이다.  빈 시트인 시트1, 그리고 상장기업종목코드와 기업명이 담긴 sample_data 시트가 있다.        gspread 패키지를 통해 인증 후, 접근하고자 하는 시트의 이름을 패스한다.  import gspread spreadsheet_name = \u0026#34;{your target spreadsheet name}\u0026#34; client = gspread.authorize(creds) spreadsheet = client.open(spreadsheet_name) \u0026amp;nbsp\n1.4.3. 시트 불러오기 for sheet in spreadsheet.worksheets(): print(sheet) \u0026lt;Worksheet '시트1' id:1966713574\u0026gt; \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.4. 시트 선택하기  이름 또는 인덱스를 통해 시트를 선택할 수 있다.  ## by name sheet = spreadsheet.worksheet(\u0026#34;sample_data\u0026#34;) ## OR by index sheet = spreadsheet.get_worksheet(1) print(sheet) \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.5. 시트 내 데이터 읽어오기  get_all_values() 함수는 시트 내 데이터를 모두 출력한다.  sheet.get_all_records()[:3] [{'CompanyStockCode': 'S030190', 'PA_CompanyID': 1, 'IA_CompanyID': 'ID00001', 'DelistStatus': '', 'CompanyKorName': 'NICE평가정보'}, {'CompanyStockCode': 'S038620', 'PA_CompanyID': 2, 'IA_CompanyID': 'ID00002', 'DelistStatus': '', 'CompanyKorName': '위즈코프'}, {'CompanyStockCode': 'S039020', 'PA_CompanyID': 3, 'IA_CompanyID': 'ID00003', 'DelistStatus': '', 'CompanyKorName': '이건홀딩스'}] \u0026amp;nbsp\n1.4.6. Pandas DataFrame 형태로 변환  간단한 함수를 작성하여 Pandas의 DataFrame형태로 데이터를 불러올 수 있다.  def gsheet2df(sheet): df = pd.DataFrame(columns=list(sheet.get_all_records()[0].keys())) for item in sheet.get_all_records(): df.loc[len(df)] = item return df df_sample_data = gsheet2df(sheet) df_sample_data.head()    \u0026amp;nbsp \u0026amp;nbsp\n2. 마무리하며 리서쳐와 데이터를 주고 받으며 일하던 와중에, gspread의 적용은 매우 빛과 소금 같은 시원한, 그리고 매우 신속했던 해결책이었다. 내 업무영역에 적용한 뒤, 바로 다른 분석가 동료들에게 튜토리얼을 통해 공유하는 세션을 가지기도 했던 만큼 적용되는 범위가 컸다.\n누구든 일을 하다 잠재적인 기술 부채를 맞닥뜨리면, 미래의 나를 위해서 어떻게든 털어내버리고 싶을 것이다. 하지만, 함께 해결해나갈 인력이 없는 환경에서 비개발자인 동료들도 불편해하지 않게끔 적절한 방법을 찾고 도입하는 것은 생각보다 쉽지 않았다. 답답할 때가 없지는 않지만, 이번 케이스처럼 또 하나하나씩 도입해나가면 더 매끄러워지지 않을까라는 생각을 해보며 또 다른 기술 부채를 맞닥뜨릴 마음의 준비를 해본다.\n\u0026amp;nbsp\n 3. Reference   gspread 공식 도큐먼트\n  Manage Google Spreadsheets with Python and Gspread\n  ","permalink":"https://wonyoungseo.github.io/posts/2020-04-12-python-spreadsheet-gspread/","summary":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움.","title":"[KR] Python으로 구글 스프레드시트 연동하기 (ft. gspread)"},{"content":"   Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.   Raw Data  테이블 형태의 데이터셋을 확인할 수 있다.   Map: Confirmed  1월 22일부터 현재까지 전세계의 확진자 현황을 지도로 시각화한다.       (데이터셋은 Johns Hopkins CSSE의 Github에 제공된 COVID-19 데이터를 사용하였다.)\n\u0026amp;nbsp\n 사전 준비 2: Heroku Heroku는 웹사이트나 어플리케이션을 빌드하고 배포할 수 있는 PaaS이다. 파이썬, 자바, 루비, PHP, Node.js, Go 등 여러 언어를 지원하고 있다. 소규모 어플리케이션이라면 무료로도 배포할 수 있다.\nHeroku 가입 Heroku를 통해 배포를 하기 위해서는 우선 계정이 있어야 한다. 아래 링크를 통해 Heroku 계정을 생성하자.\n https://www.heroku.com/  Heroku CLI 설치 설치 계정을 생성하였다면, 현재 개발 환경에 Heroku Command Line Interface를 설치한다. 아래의 링크에는 각자의 OS에 맞추어 Heroku CLI의 설치 가이드가 제공되어 있다.\n https://devcenter.heroku.com/articles/getting-started-with-python#set-up  CLI를 통한 로그인 설치가 완성되었다면, 쉘에서 heroku 커맨드를 사용할 수 있다. 다음 명령어로 Heroku에 로그인 해보자.\nheroku login    아래와 같은 결과가 출력되며, 브라우저를 열어 로그인 하도록 한다. Heroku에 대한 준비작업은 이로서 완료되었다.\n   \u0026amp;nbsp\n 배포를 위한 파일 생성하기 Heroku에 배포를 하기 위해서는 몇 가지 준비사항이 충족되어야 한다.\n app.py가 위치한 가상환경에서 Git Repository를 생성한다.  git init 배포를 위해 필요한 파일들을 생성한다.  파일 #1. requirements.txt pip freeze \u0026gt; requirements.txt 파일 #2. .gitignore venv *.pyc .DS_Store .env 파일 #3. setup.sh setup.sh에서는 Streamlit에 대한 config.toml 파일을 생성한다.\nmkdir -p ~/.streamlit/ echo \u0026#34;\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT\\n\\ \u0026#34; \u0026gt; ~/.streamlit/config.toml 파일 #4. Procfile Procfile에서는 Heroku로 하여금 웹어플리케이션을 시작시키기 위해 명령어를 순서대로 실행하도록 한다.\nweb: sh setup.sh \u0026amp;\u0026amp; streamlit run app.py (Procfile에 대한 자세한 사항은 다음 링크에서 참조할 수 있다.)\n\u0026amp;nbsp\n Heroku에 배포하기 배포를 위한 파일이 모두 준비되었다면, 이제 본격적으로 Heroku에 배포를 할 수 있게 되었다.\nHeroku App 생성 heroku create {your app name} 앞서 Heroku CLI를 생성했을 때와 같은 Heroku 로그인 과정을 거쳐, Heroku App이 생성 되었다.\n   생성된 App은 Heroku 사이트의 대시보드에서도 다음과 같이 확인이 가능하다.\n   Heroku repository 생성 Heroku는 배포를 위해 Git을 사용하므로, 방식도 같다.\ngit add . git commit -m \u0026#39;Init app boilerplate\u0026#39; git push heroku master       Heroku 의 프로세스 인스턴스를 1개로 제한하는 것을 권장한다. 프로세스가 많을 수록 유료가 될 수 있기 때문에, 무료인 상태에서 계속 사용하고 싶다면, 아래의 명령어를 실행시켜준다.\nheroku ps:scale web=1    Heroku 배포 확인 드디어 배포가 완료되었다! 이제 배포된 웹 어플리케이션을 https://your_app.herokuapp.com 에서 확인할 수 있다. 본 튜토리얼을 위해 배포된 웹 어플리케이션은 https://tuto-covid19-map.herokuapp.com/ 에서 확인할 수 있다.\n   \u0026amp;nbsp\n 마무리 하며 간단한 과정을 거쳐 웹 어플리케이션을 배포까지 해보았다. AWS와 같은 클라우드에 배포하는 과정은 익숙하지 않았고, 현재 Streamlit이 자체적으로 특정 배포 방식을 권장하고 있지는 않기에, Dash(Plotly에서 개발한 웹어플리케이션 프레임워크)에서 웹어플리케이션을 Heroku에 배포하는 가이드를 적용해 보았다. 큰 어려움 없이 배포를 완료하였기에, 많은 분들도 이 과정을 참고하시어 Streamlit을 활용한 다양한 프로젝트를 진행할 수 있으리라 생각된다.\n\u0026amp;nbsp\n 코드  Github  \u0026amp;nbsp\n참고자료  Dash 공식 페이지 Johns Hopkins CSSE Github Repository \u0026lt;em\u0026gt;Mapping the Spread of Coronavirus COVID-19 with python and Plotly\u0026lt;/em\u0026gt; by. Babak Fard  ","permalink":"https://wonyoungseo.github.io/posts/2020-03-29-deploy-streamlit-to-heroku/","summary":"Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.","title":"[KR] Streamlit 웹 어플리케이션 배포하기 (feat. Heroku)"},{"content":" Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;\n웹개발을 몰라도 된다.\n 웹개발에 대해 아는 것이 전혀 없는 나 같은 사람도 페이지를 띄울 수 있다. 주로 사내용으로 이용되기 때문에 UI/UX적인 측면에서 뛰어나지 않아도 되기 때문에 일정 수준의 미적인 요소들이 기본적으로 적용되있는 점이 매우 편리하다. 간결하고 명확한 API 덕분에 다른 웹프레임워크와 비교해서 상대적으로 진입장벽이 낮다. 일정한 수준의 결과를 내기 위해 투자하는 시간이 매우 절약된다.  전달력이 매우 좋다.\n 웹어플리케이션은 사용자에게도 진입장벽이 낮다. 특히 interactive한 성향 덕분에, 슬라이드 포맷의 레포트나 자료보다 전달력과 만족도가 높은 것을 볼 수 있었다.  \u0026amp;nbsp\n덕분에 Streamlit은 조직 내부적으로 탐색적 데이터 분석(EDA) 결과를 공유하거나, 간단한 ML 모델을 배포하고 테스트를 하는 용도에 부합하는 툴이라고 할 수 있다. Streamlit API에서 제공하는 기능들을 간단하게 훑어보자.\n Streamlit 간단 맛 보기 설치 pip를 통해 설치하기 $ pip install streamlit \u0026amp;nbsp\n실행하기 Streamlit은 8501 포트에 앱이 실행된다. 일단 지금 아무 것도 없는 상황에서는 우측 상단 버튼만 있는 페이지를 볼 수 있다.\n$ streamlit run {your app}.py You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://{your_network}:8501    Streamlit 불러오기 Streamlit은 st라는 alias로 불러온다.\n# import Streamlit Library import streamlit as st 소스에 변경이 생길 경우 경우 상단에 알림이 뜬다. Rerun을 해주도록 하자.\n   \u0026amp;nbsp\n텍스트 출력 Header와 Text  Title, Header \u0026amp; Subheader  Header와 Subheader를 다음과 같이 달 수 있다. 다만 Header의 경우 subheader 레벨까지만 가능하다. Title, Header, Subheader이 각각 Header, Subheader, Subsubheader인 것으로 인식하면 될 것 같다.    ## Title st.title(\u0026#39;Streamlit Tutorial\u0026#39;) ## Header/Subheader st.header(\u0026#39;This is header\u0026#39;) st.subheader(\u0026#39;This is subheader\u0026#39;) ## Text st.text(\u0026#34;Hello Streamlit! 이 글은 튜토리얼 입니다.\u0026#34;)    \u0026amp;nbsp\nMarkdown Streamlit도 Dash와 마찬가지로 Markdown을 지원한다.\n## Markdown syntax st.markdown(\u0026#34;# This is a Markdown title\u0026#34;) st.markdown(\u0026#34;## This is a Markdown header\u0026#34;) st.markdown(\u0026#34;### This is a Markdown subheader\u0026#34;) st.markdown(\u0026#34;- item 1\\n\u0026#34; \u0026#34; - item 1.1\\n\u0026#34; \u0026#34; - item 1.2\\n\u0026#34; \u0026#34;- item 2\\n\u0026#34; \u0026#34;- item 3\u0026#34;) st.markdown(\u0026#34;1. item 1\\n\u0026#34; \u0026#34; 1. item 1.1\\n\u0026#34; \u0026#34; 2. item 1.2\\n\u0026#34; \u0026#34;2. item 2\\n\u0026#34; \u0026#34;3. item 3\u0026#34;) 2020-03-13-intro-to-streamlit/4\n\u0026amp;nbsp\nLatex Latex의 경우 백슬래시(\\)를 빈번히 사용되기 때문에, 일반 string 대신 raw string을 붙여주는 편이 좋다.\n## Latex st.latex(r\u0026#34;Y = \\alpha + \\beta X_i\u0026#34;) ## Latex-inline st.markdown(r\u0026#34;회귀분석에서 잔차식은 다음과 같습니다 $e_i = y_i - \\hat{y}_i$\u0026#34;)    \u0026amp;nbsp\n메세지와 에러메세지, 예외처리 메세지 기본적으로 포맷된 메세지 박스 기능을 제공한다.\n## Error/message text st.success(\u0026#34;Successful\u0026#34;) st.info(\u0026#34;Information!\u0026#34;) st.warning(\u0026#34;This is a warning\u0026#34;) st.error(\u0026#34;This is an error!\u0026#34;) st.exception(\u0026#34;NameError(\u0026#39;Error name is not defined\u0026#39;)\u0026#34;)    \u0026amp;nbsp \u0026amp;nbsp\n데이터프레임과 테이블 출력. 데이터를 출력하는 방법에는 3가지 방법이 있다.\n  st.table:\n 단순히 입력 테이블 전체를 리턴한다.    st.dataframe:\n 적절히 10개의 행을 기준으로 스크롤을 통해 데이터를 관찰 할 수 있고 각 열마다 정렬도 가능하다. 각 테이블의 우측 상단의 확대 버튼을 통해 테이블을 더 크게 볼 수 있고,    st.write:\n st.dataframe과 똑같은 결과를 리턴한다.    ## Load data import pandas as pd from sklearn.datasets import load_iris iris = load_iris() iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df[\u0026#39;target\u0026#39;] = iris[\u0026#39;target\u0026#39;] iris_df[\u0026#39;target\u0026#39;] = iris_df[\u0026#39;target\u0026#39;].apply(lambda x: \u0026#39;setosa\u0026#39; if x == 0 else (\u0026#39;versicolor\u0026#39; if x == 1 else \u0026#39;virginica\u0026#39;)) ## Return table/dataframe # table st.table(iris_df.head()) # dataframe st.dataframe(iris_df) st.write(iris_df)    \u0026amp;nbsp \u0026amp;nbsp\n이미지, 오디오, 비디오 파일 출력. 이미지, 영상, 오디오 파일을 열어서 재생할 수 있다.\n st.image : 파이썬 이미지 라이브러리와 함께 쓸 수 있다. st.video :  파일의 포맷을 지정해야 하며, 디폴트로는 video/mp4가 설정되어 있다. start_time 변수를 통해 재생시작점을 조절할 수 있다.   st.audio :  파일 포맷은 audio/wav 가 디폴트로 설정되어 있다. 마찬가지로 start_time 변수를 통해 재생시작점을 조절할 수 있다.    ##Show image from PIL import Image img = Image.open(\u0026#34;files/example_cat.jpeg\u0026#34;) st.image(img, width=400, caption=\u0026#34;Image example: Cat\u0026#34;) ## Show videos vid_file = open(\u0026#34;files/example_vid_cat.mp4\u0026#34;, \u0026#34;rb\u0026#34;).read() st.video(vid_file, start_time=2) ## Play audio file. audio_file = open(\u0026#34;files/loop_w_bass.mp3\u0026#34;, \u0026#34;rb\u0026#34;).read() st.audio(audio_file, format=\u0026#39;audio/mp3\u0026#39;, start_time=10)    \u0026amp;nbsp \u0026amp;nbsp\n위젯 st.checkbox - 체크박스 ## Checkbox if st.checkbox(\u0026#34;Show/Hide\u0026#34;): st.write(\u0026#34;체크박스가 선택되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.radio - 라디오버튼 ## Radio button status = st.radio(\u0026#34;Select status.\u0026#34;, (\u0026#34;Active\u0026#34;, \u0026#34;Inactive\u0026#34;)) if status == \u0026#34;Active\u0026#34;: st.success(\u0026#34;활성화 되었습니다.\u0026#34;) else: st.warning(\u0026#34;비활성화 되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.selectbox - 드랍다운 선택 ## Select Box occupation = st.selectbox(\u0026#34;직군을 선택하세요.\u0026#34;, [\u0026#34;Backend Developer\u0026#34;, \u0026#34;Frontend Developer\u0026#34;, \u0026#34;ML Engineer\u0026#34;, \u0026#34;Data Engineer\u0026#34;, \u0026#34;Database Administrator\u0026#34;, \u0026#34;Data Scientist\u0026#34;, \u0026#34;Data Analyst\u0026#34;, \u0026#34;Security Engineer\u0026#34;]) st.write(\u0026#34;당신의 직군은 \u0026#34;, occupation, \u0026#34; 입니다.\u0026#34;)       \u0026amp;nbsp\nst.multiselect - 드랍다운 다중 선택 ## MultiSelect location = st.multiselect(\u0026#34;선호하는 유투브 채널을 선택하세요.\u0026#34;, (\u0026#34;운동\u0026#34;, \u0026#34;IT기기\u0026#34;, \u0026#34;브이로그\u0026#34;, \u0026#34;먹방\u0026#34;, \u0026#34;반려동물\u0026#34;, \u0026#34;맛집 리뷰\u0026#34;)) st.write(len(location), \u0026#34;가지를 선택했습니다.\u0026#34;)       \u0026amp;nbsp\nst.slider - 슬라이더 ## Slider level = st.slider(\u0026#34;레벨을 선택하세요.\u0026#34;, 1, 5)    \u0026amp;nbsp\nst.button - 버튼 ## Buttons if st.button(\u0026#34;About\u0026#34;): st.text(\u0026#34;Streamlit을 이용한 튜토리얼입니다.\u0026#34;)    \u0026amp;nbsp\n텍스트 입력 # Text Input first_name = st.text_input(\u0026#34;Enter Your First Name\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;first_name\u0026#39;): result = first_name.title() st.success(result) # Text Area message = st.text_area(\u0026#34;메세지를 입력하세요.\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;message\u0026#39;): result = message.title() st.success(result)    \u0026amp;nbsp\n날짜와 시간 입력 ## Date Input import datetime today = st.date_input(\u0026#34;날짜를 선택하세요.\u0026#34;, datetime.datetime.now()) the_time = st.time_input(\u0026#34;시간을 입력하세요.\u0026#34;, datetime.time())       \u0026amp;nbsp\n코드와 JSON 출력  with st.echo(): 이하의 코드는 코드블럭으로 출력된다.  ## Display Raw Code - one line st.subheader(\u0026#34;Display one-line code\u0026#34;) st.code(\u0026#34;import numpy as np\u0026#34;) # Display Raw Code - snippet st.subheader(\u0026#34;Display code snippet\u0026#34;) with st.echo(): # 여기서부터 아래의 코드를 출력합니다. import pandas as pd df = pd.DataFrame() ## Display JSON st.subheader(\u0026#34;Display JSON\u0026#34;) st.json({\u0026#39;name\u0026#39; : \u0026#39;민수\u0026#39;, \u0026#39;gender\u0026#39;:\u0026#39;male\u0026#39;, \u0026#39;Age\u0026#39;: 29})    \u0026amp;nbsp\n사이드바 st.sidebar에서도 대부분의 위젯을 지원하므로, 다양하게 사이드바를 구성할 수 있다. (단, st.echo, st.spinner, st.write제외)\n## Sidebars st.sidebar.header(\u0026#34;사이드바 메뉴\u0026#34;) st.sidebar.selectbox(\u0026#34;메뉴를 선택하세요.\u0026#34;, [\u0026#34;데이터\u0026#34;, \u0026#34;EDA\u0026#34;, \u0026#34;코드\u0026#34;])    \u0026amp;nbsp\n차트 그리기 Streamlit은 자체 내장된 기본적인 차트 외 matplotlib, plot.ly, altair, vega_ilte, bokeh, deck_gl, pydeck, graph_viz 등 다양한 시각화 패키지를 지원한다.\n(Streamlit은 EDA 용도로 많이 사용되는 만큼, 시각화 부분은 따로 다룰 계획이다.)\n## Plotting st.subheader(\u0026#34;Matplotlib으로 차트 그리기\u0026#34;) iris_df[iris_df[\u0026#39;target\u0026#39;]==\u0026#39;virginica\u0026#39;][\u0026#39;petal length (cm)\u0026#39;].hist() st.pyplot()    \u0026amp;nbsp \u0026amp;nbsp\n 마무리 Streamlit의 API를 훑어보면서, 전체적으로 많은 부분이 간결하고, 쉽다고 느껴졌다. Flask나 Django로 개발하는 개발자 입장에서 Streamlit 같은 프레임워크는 자유도가 제한된다고 느껴질 수도 있겠다. 하지만 등장한지 얼마 안 되는 만큼 커뮤니티 포럼에서는 활발한 토론과 기능 추가에 대한 요청이 이어지고 있는 중이다. Streamlit 개발자들이 적극적으로 피드백을 반영하는 모습을 보이고 있으므로, 앞으로의 발전이 더 기대된다.\n이번 소개 글에 이어, 개인적으로 Streamlit으로 개발하면서 얻은 팁이나, 클라우드 또는 Heroku에 배포하는 과정, 데이터사이언스 프로젝트를 위해 웹어플리케이션을 구성하는 팁 등을 시리즈로 작성할 예정이다. 나처럼 웹개발은 모르지만 데이터분석 결과를 그럴 듯하게 구성하고 싶은 분들께 도움이 되었으면 한다.\n\u0026amp;nbsp\n코드  Github  \u0026amp;nbsp\n참고자료  Streamlit 공식 페이지  ","permalink":"https://wonyoungseo.github.io/posts/2020-03-13-intro-to-streamlit/","summary":"Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;","title":"[KR] 파이썬 웹어플리케이션 맛보기 (feat. Streamlit)"},{"content":" 신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다. 회사에 시니어가 없어서 그런지 이 루트로 도움을 많이 받고 있음을 체감한다. 자연스럽게 ‘나도 할 수 있을까?’ 라는 생각이 들었다. 데이터를 다루는 일을 하고 있지만, 내가 하는 일을, 내가 아는 것들을 글로서 풀어낼 수 있을지 스스로 물어보면 돌아오는 답은 ‘자신이 없다’ 였다. 솔직히 아직은 자신이 많이 없다.\n   \u0026#34;핑계는 수백만개라도 댈 수 있죠\u0026#34; Source: The Office   \u0026amp;nbsp\n이런 와중에 글또라는 모임은 1년 전부터 여러 커뮤니티와 변성윤님을 통해 몇 차례 접하고 있었다. 글 쓰는 또라이가 세상을 바꾼다라니… 너무 멋있다고 생각했다 (역시 대단한 사람들!). 마침 글또 4기의 모집 글을 접하게 되었고, 글쓰는 것에 대해 여러 생각을 하는 중 타이밍이 맞았다. 다른 때 같았으면 모집글만 보고 “내가 저런 거 할 시간이 어딨어” 등등 갖가지 이유를 대며 안 할 수도 있긴 했다. 그래서 일단 질렀다. 글또 지원은 2020년에 강림하신 첫 지름신이었다.\n글또를 통해 이루고자 하는 것 글쓰는 꾸준한 습관 몇 년 전부터 글을 써보고자 다양한 플랫폼에서 시도했었다. 페이스북, 워드프레스 블로그, 인스타그램, 힙합엘이…. 여러 가지 이유가 있었겠지만, 가장 주요한 이유를 냉정하게 꼽아보자면 끈기가 부족했다. 글 쓰는 체력이 부족하고, 쓰다가 아니다 싶으면 접어버리는 안 좋은 습관도 있다는 것을 인지하게 됐다.\n반면에 글또는 다 함께 정한 규칙과 약간의 강제성이 있고, 글을 읽고 피드백을 해주는 분들이 있다. 나 또한 다른 분들의 글을 읽고 피드백을 해야 한다. 지금 나에게 필요한 것은 약간의 강제성을 부여받고 다른 멋진 분들을 통해 자극을 받는 것이니까, fit이 딱 맞는다. 예치금을 삭감당하지 않겠다. 기필코.\n\u0026amp;nbsp\nClear할 때까지 글또를 통해서 얻고자 또 다른 목표는, 글쓰기를 통해 이해도와 설명력을 끌어올리는 것이다. 설명력은 높은 이해를 기반으로 이루어지는 것으로 생각하는데, 여태까지 회사에서 일하고 개인적으로 공부를 하면서 늘 어중간하게 알고 있는 것 같은 찝찝함을 버리지 못했다. 반면에 공부를 잘하는 친구들은 (1) 교과서만 봤어요, (2)질문을 많이 했어요, (3)다른 친구들에게 (혹은 부모님을 앉혀놓고) 설명을 해주면서 스스로 공부가 많이 됐어요 라고 들 많이 하더라. 생각해보면 난 학창 시절에 저런 걸 하나도 안 했다. 하지만 글또를 통해서 나 자신이 더 공부하고 성장을 할 수 있을 것이라 기대한다. 무언가에 대해 글을 쓰고 남에게 보이려면, 어쭙잖게 쓰면 안 된다는 것은 본능적으로 알고 있으니까.\n\u0026amp;nbsp\n글또 4기에서 쓰고자 하는 글 글또에 참여하시는 다른 분들을 보고, 나는 어떤 글을 써볼지 고민해봤다. 무엇보다 내가 배우고 느낀 점을 기록하는 것이 현시점에서는 우선이라 신속한 정보 전달의 목적은 2순위로 두기로 했다. 일단 떠오르는 주제가 여러 가지 있지만 좀 더 구체적인 정리가 더 필요하다.\n\u0026amp;nbsp\n스스로 돌아보는 반기 별 회고 글\n 회고를 하지 않다 보니 작년 한 해 동안 대체 무엇을 한 건지…. 남는 게 많이 없었고 개인적으로 허무하다는 느낌을 많이 받았다. 이번에는 회고 글을 정리하면서 작년과 무엇이 다른지 한번 느껴보고 싶다.  \u0026amp;nbsp\n회사에서 삽질한 경험\n 시니어가 없고, 늘 개인적으로 고군분투하다 보니까 이러한 경험도 글로 남기면 좋겠다고 생각해본다. 나만 그런 것은 아닐 테니까. 그리고 증거로 남겨야겠다.  \u0026amp;nbsp\n데이터 사이언스 대회 참가 후기\n 한 번도 on-going 데이터 사이언스 대회에 참가해 본 적이 없는데, 이 또한 갖가지 핑계를 대며 하지 않았던 거라고 자평해본다. 데이터 사이언스 대회 참가를 통해 배운 점, 삽질한 점들을 돌아보고 정리해보고자 한다.  \u0026amp;nbsp\n개발 관련 책 리뷰\n 책은 자주 많이 사는데, 늘 훑어보기만 했던 게 좀 찝찝했다. 책에 대한 리뷰글을 틈틈이 쓰고자 한다. 출판사 분들 지켜봐 주세요  \u0026amp;nbsp\n글을 마치며 작년 말 느꼈던 성장에 대한 고민, 답답함을 기반으로 올해 1, 2월 사이 내린 결정들이 있는데, 글또 참여도 그중 하나였다. 벌린 일이 많아 걱정은 되지만, 앞으로 글또에서 만나게 될 많은 멋진 분들이 계시니까 아주 큰 걱정은 아니라고 생각된다. 개복치처럼 터지지 말고, 꾸준히 존버하자.\n","permalink":"https://wonyoungseo.github.io/posts/2020-02-23-init-geultto-4th/","summary":"신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다.","title":"[KR] 글또 4기 다짐글"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.) 원을 두 바퀴, 세 바퀴 돌면 720도, 1080도가 되는데 이런 식의 계산은 편리하지가 않다. 두바퀴는 2 * 2pi randians으로 계산하는 것이 더 편하다고 하는데 일단 지켜보자.\n아래의 Figure 1.에서처럼 45도의 각도는 45/360 * 2pi = 0.785 radians 로 표현할 수 있다.\n   \u0026amp;nbsp\n삼각함수 복습 삼각함수를 좀 복습해야겠다. 위의 그림의 삼각형을 보고 다음과 같이 부른다면\n Hypotenuse 빗변 (파랑, 원의 반지름) : r Adjacent 밑변 (빨강, x축 길이) : x Opposite 높이 (초록, y축 길이) : y  피타고라스의 정리를 통해 다음과 같이 말할 수 있다.\n$$x^2 + y^2 = r^2$$\n그렇다면, sine, cosine도 다음과 같이 구할 수 있다.\n$$\\sin = \\dfrac{y}{r}$$\n$$\\cos = \\dfrac{x}{r}$$\n단위원(unit circle)를 기준으로 본다면 r=1 이므로 다음과 같다.\n$$\\sin = \\dfrac{y}{1}$$\n$$\\cos = \\dfrac{x}{1}$$\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-25-dsp-basic-s01-7/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Trigonomatry"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.\n      \u0026amp;nbsp\n선이 회전하는 속도는 곧 frequency로 나타낼 수 있다. 앞서 frequency는 1초에 몇 사이클을 회전할 수 있는지를 통해 측정한다고 했는데, 원문의 Figure 1.을 통해서 확인할 수 있듯이 frequency가 높을 수록 회전하는 선이 속력이 빨라지고, 주기도 빨라지게 된다. 반대로 frequency가 낮으면 주기가 느려진다.\n      \u0026amp;nbsp\nThe Cosine Wave : The Counterpart to Sine 반대로 cosine wave(코사인파)는 원을 그리며 회전하는 선과 X축에서 수평의 길이와 관계 있다. 사실 sine wave와 cosine wave는 90도로 회전하면 서로 같은 모양을 가지고 있다.\n   \u0026amp;nbsp\nSine wave와 cosine wave는 밀접한 관계를 맺고 있으며, 둘 다 주기성을 띄는 주기함수(periodic signal)임을 이해하는 것이 매우 중요하다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-23-dsp-basic-s01-6/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sine Wave"},{"content":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,\ndf = pd.DataFrame({ \u0026#39;A\u0026#39;: [1,2,3], \u0026#39;B\u0026#39;: [\u0026#34;goodbye\u0026#34;, \u0026#34;cruel\u0026#34;, \u0026#34;world\u0026#34;], \u0026#39;C\u0026#39;: [False, True, False] }) df.info() 결과물 출력 비교\npandas 0.x.x\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): A 3 non-null int64 B 3 non-null object C 3 non-null bool dtypes: bool(1), int64(1), object(1) memory usage: 179.0+ bytes pandas 1.0.0\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 A 3 non-null int64 1 B 3 non-null object 2 C 3 non-null object dtypes: int64(1), object(2) memory usage: 200.0+ bytes \u0026amp;nbsp\n.to_markdown() : DataFrame을 Markdown 형식으로 DataFrame을 바로 Markdown 형식으로 출력할 수 있게 되었다. 문서화 작업을 할 때 항상 markdown table generator 같은 도구를 썼었는데, 매우 반갑고 편리한 기능!\ndf = pd.DataFrame({\u0026#34;A\u0026#34;: [1, 2, 3], \u0026#34;B\u0026#34;: [1, 2, 3]}, index=[\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) print(df.to_markdown()) | | A | B | |:---|----:|----:| | a | 1 | 1 | | a | 2 | 2 | | b | 3 | 3 | \u0026amp;nbsp\ningore_index : index reset 파라미터 추가 기존에는 정렬이나 중복값 제거 후 .reset_index(drop=True)를 추가적으로 해줘야 했으나, ignore_index 파라미터를 통해 index를 리셋할 수 있게 되었다. default는 False. 다음 기능들에서 찾아볼 수 있다.\n .sort_values() .sort_index() .drop_duplicates()  \u0026amp;nbsp\npd.NA : 새로운 missing value의 실험 기존에 Pandas에서 missing value를 처리할 때는 np.nan이나 None이라는 싱글턴이 사용되었다. 그러나 data type이 float일 때는 np.nan , object일 경우에는 np.nan이나 None, datetime일 경우에는 np.NaT가 사용된다. 데이터타입마다 Null 데이터의 표현이 각기 달랐기 때문에, pd.NA는 datatype이 각기 달라도 missing data를 통일되게 표현할 수 있기 위해 도입되었다. 현재는 pd.NA는 data type 중 integer, boolean, 그리고 새로 도입된 string에서 사용 가능하다. pd.NA 값은 \u0026lt;NA\u0026gt;로 리턴된다. 실험적으로 도입했다고 하니, 지켜보면 좋을 듯.\n\u0026amp;nbsp\nstring : 새로운 data type 도입 기존에는 object 이라는 data type으로 뭉뚱그려진 느낌이 있었으나, string 이라고 따로 지정할 수 있게 됨으로서 EDA나 wrangling 측면에서 더욱 편해질 것 같다. (pd.NA도 확인 가능함)\npd.Series([\u0026#39;abc\u0026#39;, None, \u0026#39;def\u0026#39;], dtype=pd.StringDtype()) 0 abc 1 \u0026lt;NA\u0026gt; 2 def Length: 3, dtype: string \u0026amp;nbsp\nbool : missing value 표현 가능 기존에 boolean은 True / False 만 표기가 가능했으나, Pandas 1.0.0 에서는 missing value도 가능하다. (pd.NA도 가능함)\npd.Series([True, False, None], dtype=pd.BooleanDtype()) 0 True 1 False 2 \u0026lt;NA\u0026gt; Length: 3, dtype: boolean \u0026amp;nbsp\n Reference https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-16-pandas-new-release/","summary":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,","title":"[KR] Pandas 1.0.0 : 바뀐 점을 ARABOJA"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다. 음색이라는 개념은 음의 높이나 강도와는 별개의 특성이다. 플룻과 바이올린으로 똑같은 음을 연주하더라도, 소리는 서로 확연이 다른 것과 연관이 있다. 음색은 주로 배음(overtone)과 고조파(harmonic)의 유무에 따라 결정 된다.\n대부분의 음악은 fundamental frequency와 다수의 fundamental frequency에 위치한 harmonics로 이루어져 있다. 예를 들어 A4의 fundamental frequency는 440Hz이고, harmonics는 880Hz, 1320Hz, 1760Hz \u0026hellip; 의 주기로 이루어져 있다. 악기들은 대개 연주가 될 때, fundamental frequency와 harmonics에 위치한 소리를 생성한다.\n신호처리에서 sound wave를 논할 때는 주로 3가지 기본 sound wave를 지칭한다.\n sine wave: overtone이 없는 순음(pure tone) squire wave: fundamental frequency와 fundamental frequency의 홀수 배음 harmonics saw wave: fundamental frequency와 fundamental frequency의 전체 harmonics  신호를 관찰해보면 fundamental frequency가 가장 소리가 큰 부분을 차지하고 있고, harmonics는 frequency가 증가할 수록 감소한다.\n원본의 실습에서는 각기 다른 sound wave의 소리를 직접 들어보고 이에 따른 스펙트럼(spectrum)을 확인할 수 있다. 스펙트럼이란 특정 신호 안에 담긴 frequency들을 시각화된 형태이다.\n   \u0026amp;nbsp\n실제로 실행을 하고 스펙트럼을 확인해보면, 앞에서 설명한 바와 같이 Sine Wave는 단 하나의 440Hz frequency만 있는 순음이다. 반대로 Noise Wave는 어떠한 형태의 frequency도 구별을 할 수가 없다.\nSquare Wave 와 Saw Wave를 보면 Sine Wave를 여러번 반복한 형태와 비슷하게 보인다. 또한 spectrum에서도 여러 frequency 가 공존하는 것을 볼 수 있다.\n뒤 따르는 내용에서는 어떻게 하면 여러 개의 sine wave가 모여 complex wave(복합파형)를 이루는 지를 알아보자.\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-15-dsp-basic-s01-5/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Timbre"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다. 이런 밀고 당김의 연쇄작용이 진동을 발생하고 공기 중으로 전파가 나아갈 수 있게 한다.\n원문의 Figure 1.을 직접 참고해보기를 바란다.\n   \u0026amp;nbsp\n왼쪽 상단의 회색 직사각형이 파동의 진원지인 물체이고 무수히 많은 점들이 공기 중의 입자라고 보자. 원문의 이미지에서 이 입자들의 움직임을 잘 관찰해보면 물체의 움직임에 따라 특정 공간 내에서만 좌우로 움직이고 입자 자체가 자리를 이동하지는 않는 것을 볼 수 있다. 움직이는 물체로 인해 입자들도 비슷한 움직을을 보이게 되는데, 이것을 공명한다고 한다.\n입자 예시 아래 보이는 곡선은 기압의 정도를 나타낸다. 곡선이 수평축의 위를 지나갈 때는 공기의 입자들이 서로 컴팩트하게 모여있어 기압이 높은 상태(compression: 압축, 고밀도)이고, 반대인 경우 기압이 낮은 상태(rarefaction: 저밀도, 희박한 상태)이다.\n진동의 속도를 frequency(주파수)라고 한다. Frequency는 1초에 몇 사이클을 지났는지를 측정한 것이고, Hertz(Hz : 헤르쯔)라는 단위를 쓴다. Compression과 rarefaction의 반복 사이클을 1초에 몇번 왔다갔다 했는지를 측정하면 frequency를 구할 수 있다. 아래 Figure 1a.를 보면, compression과 rarefaction을 지나간 사이클 1번 이라고 볼 수 있겠다.\n   \u0026amp;nbsp\nFigure 1.을 통해 이것저것 시도해 보다 보면, frequency가 증가할 수록 wavelength(파장)이 줄어드는 것을 볼 수 있다. 파형(wave)의 길이(length)는 각 파형의 꼭지점이나 골짜기 사이의 거리를 측정하면 되는데, 이 거리는 frequency와 반비례 한다. 또한, frequency가 아무리 변해도 파형이 이동하는 속도는 일정하다. 해수면을 기준으로 소리는 공기에서 340m/s, 물과 같은 물질에서는 1500m/s 정도로 이동한다.\n직접 들어보자 Figure 1.처럼 부드럽게 진동하는 파형의 경우, 우리의 청각은 이 파형을 순음(pure tone)으로 받아들인다. Frequency가 낮은 음파는 낮은 음역(bass), 높은 frequency는 높은 음역(treble)이라고 하는데, 청각이 뛰어난 사람의 경우에 20 ~20,000Hz의 음역대를 들을 수 있다. 나이가 들어가면서 청력이 저하되면 들을 수 있는 음역대도 당연히 줄어들게 된다. 원문의 Figure 2.를 통해 여러가지 frequency를 한번 시도해보자.\n(실행이 되지 않는다면 아래의 링크에서 따로 실험해보자. 20,000 이상의 소리를 들어보자. 나는 들리지 않는다.)\nOnline Tone Generator - generate pure tones of any frequency\n   \u0026amp;nbsp\nX축을 보면, frequency의 눈금이 linear하지 않고 logarithmic한 것을 알 수 있다.\n440Hz는 우리가 흔히 말하는 음 \u0026ldquo;라\u0026quot;인데 A4라고 표현한다. 880Hz도 마찬가지로 \u0026ldquo;라\u0026quot;인데 한 옥타브가 높은 A5라고 한다. 여기에서 자세한 설명은 없지만, 왜 간격이 linear하지 않고 logarithmic한지 직관적으로 느낌은 알 수 있을 듯 하다\n우리의 귀는 대부분 20~8,000Hz 사이의 음역대에 반응하고, 사람의 목소리는 300~3,000Hz 사이에 속한다. 88건반 피아노는 22~4,000Hz의 기본 주파수의 소리를 내는데, 그 이상의 소리를 내는 경우도 있으며, 이를 overtone이라 한다.\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-13-dsp-basic-s01-4/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sound Waves"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다. 그 외의 대답은 모두 거짓말이다.\n현재 우리가 가지고 있는 이산신호의 맥락에서 봤을 때, 65분 당시의 고도에 대한 기록은 어디에도 없으므로, 우리는 확신을 가지고 이야기 할 수가 없다.\naltitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] 위와 같이 10분 주기로 기록된 현재의 이산신호는 빨강, 파랑, 주황, 초록으로 기록된 아래의 다양한 비행고도의 변화를 모두 표현(represent)할 수 있다. 연속신호로 봤을 때는 고도의 변화가 제각기 다르지만, 샘플링 주기만 봤을 때는 모두 같은 신호로 볼 수 있다. 따라서 60분과 70분의 점을 이어서 31,000 이라고 답을 하는 것은 엄청난 착각을 불러일으킬 수 있다.\n   각기 다른 네 가지 연속신호를 나타내는 빨강, 파랑, 주황, 초록 곡선      네 가지 연속신호를 10분 샘플링 주기의 이산신호로 표현했을 때   \u0026amp;nbsp\nAlias 위의 빨강, 파랑, 주황, 초록 곡선은 10분 샘플링 주기의 이산신호로 표현 될 때 서로 구별이 불가능하며, 이를 각기 서로의 alias라고 부른다. 위의 네가지 곡선은 서로 다르지만, 10분 주기로 샘플링 되었을 때, 정확히 똑같아 보이기 때문이다.\n본 자료에서 두 점을 선으로 잇지 말라고 강조하고 있으나, 이해하기 쉽도록 시각화를 하기 위해서 앞으로도 지속적으로 점과 점 사이를 선으로 이어서 예를 들 것임. 글쓴이가 선을 잇는다고 해서 선을 잇는 행동은 하지 말 것 추천함.\nRemoving Uncertainty : Frequency and Context 샘플과 샘플 사이에 간격이 있다면, 불확실성(uncertainty)가 동반된다. 샘플 간격 사이에 벌어지는 급격한(rapid fluctuation) 변화에 대한 정보가 손실 될 수 있기 때문에, 간격이 크면 클 수록 우리는 샘플링이 된 현재의 이산신호가 실제 현상을 그대로 표현한다고 확신할 수가 없게 된다. 이러한 불확실성을 줄이는 방법은 반대로 간격을 줄이는 것 즉, 샘플링 주기를 줄이는 것이다.\n비행고도 예시에서, 샘플링 주기를 10분이 아닌 5분 단위로 설정한다고 해보자. 이렇게 되면 샘플링 주기가 적용된 파랑색 곡선의 이산신호를 봤을 때, 빨강, 주황, 초록 곡선은 더 이상 파랑 곡선의 alias라고 할 수 없다. (이제는 서로 구별할 수 있다!)\n   \u0026amp;nbsp\n샘플링 주기를 무작정 줄일 수도 있겠지만, 샘플링에는 감수해야할 부분도 있다. 샘플들이 저장되는 메모리가 확보되어야 한다는 것. 따라서 샘플링 주기를 줄일 때에는 꼭 주의를 기울어야 한다.\n샘플링 주기를 제대로 설정하는 방법은 말은 쉽지만 정보의 손실이 없을 정도로 설정하는 것이다. 만약 필요한 만큼보다 더 자잘하게 샘플링 주기를 설정한다면 오버샘플링(oversampling)이 되어 메모리나 연산 리소스를 낭비하게 되고, 너무 뜸하게 설정하면 언더샘플링(undersampling)으로 정보 손실이 발생할 수 있게 된다.\n샘플링에 대한 이론은 frequency(주파수) 개념을 이해하면 좀 더 쉬워질 수 있을 것 같다. 다음에는 주파수에 대한 개념을 접할 수 있는 sound wave(음파)를 한번 알아보자.\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-12-dsp-basic-s01-3/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sampling \u0026 Aliasing"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다. (\u0026lt;strong\u0026gt;원문 Figure 1. 참조\u0026lt;/strong\u0026gt;)\n   altitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] Figure 1.의 파랑색 점 하나하나가 샘플이며, 그래프말고도 다음과 같이 이산값의 리스트로도 표현하고, 인덱싱(indexing)을 할 수도 있다.\n altitude[4] → 35,000 altitude[8] → 27,000  Sampling Period (샘플링 주기) 샘플링 주기(sampling period)는 연속적인 신호 사이의 지속기간(duration)을 뜻한다. 위의 비행 고도 샘플링 예시에서 각 고도를 기록하는 주기를 10분으로 놨었는데, 이때 샘플링 주기가 10분이고, 아래와 같이 나타낼 수 있다.\n$$\\text{sampling period} = 10\\text{ minutes} / 1\\text{ sample}$$\n샘플링 주기를 알 수 있다면, index에 샘플링 주기를 곱함으로서 몇 번 째 샘플이 언제 기록 되었는지를 추적(?)할 수 있다.\n$$\\text{time of 3rd sample} = 2 \\cdot 10 \\text{ minutes} / 1 \\text{ sample} = 20 \\text{ minutes}$$\n이산신호를 해석하기 위해서는 context를 파악하는 것이 매우 중요하다. 샘플링 주기를 알아야지만, 이산신호의 값들이 의미를 가지고 make sense할 수 있는 것이고, sampling period를 알지 못 한다면, 이러한 값들이 의미를 잃게 된다.\n추가적으로 \u0026hellip;    원본의 Figure 1.에서 비행기의 실제 고도를 주의깊게 봤다면, 60-70분 사이 급격한 하강 후 고도를 회복하는 부분이 있었음을 관찰할 수 있는데, 우리의 샘플링 주기는 10분이었기 때문에 정작 샘플링 당시에는 기록되지 못 했다. 샘플링 주기가 적절하지 않았기 때문에 중요한 정보가 손실 된 것이다.\n따라서 특정 물리적인 현상을 관측하기 위한 이산신호를 기록하려면, 이산신호가 그 현상을 제대로 나타낼 수 있도록 샘플링 주기를 적절하게 선택해야 한다. 이렇듯 샘플링 주기의 결정은 신호처리 분야에서도 매우 중요하게 다루는 부분 중 하나이다.\n다음 번에는 신호주기를 매우 뜸하게 설정 했을 때 나타날 수 있는 결과에 대해 좀 더 알아보도록 하자.\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-11-dsp-basic-s01-2/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Discrete Signals"},{"content":" 본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다. 컴퓨터는 인간과 비교해서 겁나 빠르다는 장점이 있지만 또 반대로 컴퓨터는 겁나 단순해서 오직 이산값(discrete values)만 읽고 처리가 가능하다.\n하지만 현실은 그렇지 않다. 실생활에서 발생하는 대부분 연속적(continuous) 신호이다. 따라서 컴퓨터에서 분석하기 이전에 연속 신호를 이산적인, 디지털의, 딱딱 떨어지는 값으로 변환(translate)하는 과정을 거쳐야 한다.\n\u0026lt;strong\u0026gt;원문의 Figure2\u0026lt;/strong\u0026gt;를 보면 이산 신호를 통해서 연속 신호를 완벽하게 재현하는 것은 불가능 해보일 수 있다. 그리고 실제로 이는 근사화(approximation)에 그친다고 주장하는 사람도 있다.\n      하지만 DSP를 공부한다면, 이산 지점를 이용해서 연속 신호를 완벽하게 표현하는 것이 불가능하지 않다고 하니, 열심히 공부해보자!\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-10-dsp-basic-s01-1/","summary":"본 포스트는 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 신호란?"},{"content":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.name: my-local-es node.name: local ### For Response by External Request network.host: 0.0.0.0 ### For Head http.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/elasticsearch-6.6.1 $ nohup bin/elasticsearch \u0026amp; \u0026amp;nbsp\n정상적으로 실행 중인지 확인 $ ps ax | grep elasticsearch $ curl localhost:9200 http://localhost:9200 실행\n\u0026amp;nbsp\n 02. 키바나 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-linux-x86_64.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-darwin-x86_64.tar.gz \u0026amp;nbsp\n압축 풀기 $ tar -xzvf kibana-6.6.1-linux-x86_64.tar.gz $ rm kibana-6.4.0-linux-x86_64.tar.gz \u0026amp;nbsp\n환경 설정 $ cd elasticstack/kibana-6.6.1-linux-x86_64/config $ vi kibana.yml server.host : 0.0.0.0 elasticsearch.url : \u0026#34;http://localhost:9200\u0026#34; kibana.index : \u0026#34;.kibana\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/kibana-6.6.1-linux-x86_64 $ bin/kibana \u0026amp;nbsp\n실행확인 http://localhost:5601 실행\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-09-elasticsearch-kibana-local-env/","summary":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.","title":"[KR] 로컬 환경에 엘라스틱서치, 키바나 설치하기"},{"content":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인. 특히 로그를 운반하는 역할.   Beats 데이터 수집, 운송 Logstash와 비슷하나, 변환 기능이 제외되어 있음. 보다 가볍게 사용할 수 있음.    이외에 Elastic Cloud와 X-pack이 추가로 있으며, 기업을 대상으로 한 Enterprise 솔루션도 확대되고 있는 추세다.\n X-pack의 경우 유료이며, 보안을 강화하여 유저에게 권한까지 부여 가능하다. X-pack의 머신러닝 기능은 현재로서는 데이터의 이상징후를 탐지하는 수준이다.  Elastic, 어떻게 좋은가?   Near Realtime\n 데이터 색인 후 약 1초 후 Refresh 시점부터 거의 실시간으로 검색결과에 반영됨    Fast\n 기본적으로 모든 Field에 대해 Indexing(색인) 처리를 하기 때문에 검색 처리 시간이 짧다    Horizontal Scalability\n Elastic Cluster에 Elasticsearch Node를 1개씩 추가하며 수평적으로 확장하기에 용이하다    Distributed Operation\n 데이터를 조각(shard)로 세분화 하여 분산 저장하기 때문에 처리 속도가 향상된다.    Replica Shard\n 데이터 조각을 복제하여 다른 Node에도 저장하기 때문에, 특정 Node가 다운되거나 손실이 생겨도 데이터 유실 없이 운영할 수 있다.    Elastic Stack에서의 용어 비교    RDBMS Excel Elastic Elastic에서의 개념     Database Excel File Index 최상위 데이터 계층. Document의 덩어리   Table Sheet Type Document를 담고 있는 컨테이너 (*)   Row Row Document 데이터 검색을 위한 최소의 단위   Column Column Field JSON으로 이루어진 데이터의 property   Schema 없음 Mapping Index Document의 저장 규칙을 의미     RDBMS, Excel과는 달리 엘라스틱에서는 1 Index에 1개의 Type만 할당되어 사실상 의미가 사라진 상태이며, 7.0버전으로 업그레이드 시 Type이란 개념은 폐지 될 예정이다.  Elastic의 Work Flow      Elasticsearch\n Mapping 설정    Logstash\n 데이터 전처리 \u0026amp; 전송    Elasticsearch\n 데이터 저장    Kibana\n Index 등록 EDA 차트 선택 Aggregation 선택 데이터 시각화 대시보드 생성    ","permalink":"https://wonyoungseo.github.io/posts/2020-01-05-elastic-stack/","summary":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인.","title":"[KR] 엘라스틱스택(Elastic Stack) 소개"},{"content":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.\n","permalink":"https://wonyoungseo.github.io/posts/2020-01-02-first-post/","summary":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.","title":"[KR] 첫 포스팅"},{"content":"Storage Class  PV를 수동으로 생성하는 과정은 정적 프로비저닝 Static Provisioning 매번 PV를 수동으로 생성하지 않고, 스토리지가 필요한 경우 자동으로 프로비저닝 하길 원한다면 Storage Class 를 활용함  Storage Class 오브젝트는 Google Storage 등 PV 프로바이더 이용해 동적인 프로비저닝 (Dynamic Provisioning) 함\nStorage Class는 PVC와의 바인딩을 통해 연결함\n# ex) sc-definition.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: google-storage provisioner: kubernetes.io/gce-pd # ex) pvc-definition.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce storageClassName: google-storage  # storage class resources: requests: storage: 500Mi  각기 다른 thrid party provider가 선택지로 있음  Reference  Kubernetes Docs - Storage Classes  ","permalink":"https://wonyoungseo.github.io/posts/2024-03-02-k8s-storage-class/","summary":"Storage Class  PV를 수동으로 생성하는 과정은 정적 프로비저닝 Static Provisioning 매번 PV를 수동으로 생성하지 않고, 스토리지가 필요한 경우 자동으로 프로비저닝 하길 원한다면 Storage Class 를 활용함  Storage Class 오브젝트는 Google Storage 등 PV 프로바이더 이용해 동적인 프로비저닝 (Dynamic Provisioning) 함\nStorage Class는 PVC와의 바인딩을 통해 연결함\n# ex) sc-definition.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: google-storage provisioner: kubernetes.io/gce-pd # ex) pvc-definition.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce storageClassName: google-storage  # storage class resources: requests: storage: 500Mi  각기 다른 thrid party provider가 선택지로 있음  Reference  Kubernetes Docs - Storage Classes  ","title":"[KR] Kubernetes - Storage Class"}]