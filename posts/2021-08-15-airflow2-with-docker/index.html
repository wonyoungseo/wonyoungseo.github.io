<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[KR] Docker를 활용하여 Airflow 2.x 실행하기 | WY's Tech Blog</title><meta name=keywords content="airflow,docker"><meta name=description content="작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1."><meta name=author content="Wonyoung Seo"><link rel=canonical href=https://lucaseo.github.io/posts/2021-08-15-airflow2-with-docker/><meta name=google-site-verification content="XYZabc"><link href=/assets/css/stylesheet.min.d8cbf60331b9ced42909130bf88f8b97d2eb3de242444dcc9e2df410ceb098b9.css integrity="sha256-2Mv2AzG5ztQpCRML+I+Ll9LrPeJCRE3Mni30EM6wmLk=" rel="preload stylesheet" as=style><link rel=icon href=https://lucaseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://lucaseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://lucaseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://lucaseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://lucaseo.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><meta property="og:title" content="[KR] Docker를 활용하여 Airflow 2.x 실행하기"><meta property="og:description" content="작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1."><meta property="og:type" content="article"><meta property="og:url" content="https://lucaseo.github.io/posts/2021-08-15-airflow2-with-docker/"><meta property="og:image" content="https://lucaseo.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:published_time" content="2021-08-15T19:43:01+09:00"><meta property="article:modified_time" content="2021-08-15T19:43:01+09:00"><meta property="og:site_name" content="WY's Tech Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://lucaseo.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[KR] Docker를 활용하여 Airflow 2.x 실행하기"><meta name=twitter:description content="작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[KR] Docker를 활용하여 Airflow 2.x 실행하기","name":"[KR] Docker를 활용하여 Airflow 2.x 실행하기","description":"작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했 …","keywords":["airflow","docker"],"articleBody":"작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 puckel의 Airflow Docker Image가 많이 사용되었는데요, 이 이미지는 Airflow 1.x에 한정되어 있기 때문에, 이번 글에서는 Airflow의 공식 Docker 이미지를 활용해보도록 하겠습니다.\n0. Docker 설치 Docker가 설치되어 있지 않은 경우 아래의 링크를 통해 Docker를 설치하도록 합니다.\n Docker 설치 (맥, 윈도우, 리눅스)  맥, 윈도우의 경우 Docker 데스크탑을 설치할 시, Docker Compose가 함께 설치 됩니다. 단, 리눅스의 경우에는 별도의 설치가 필요합니다.\n Docker Compose 설치 (리눅스)  1. 아무것도 모르지만 일단 실행해보기 Airflow 공식 문서(Running Airflow in Docker)에서는 기본적인 Docker 및 Docker Compose를 활용한 Airflow 예제를 제공하고 있습니다. 이 파일을 토대로 일단 한번 실행 과정을 따라가보겠습니다.\n1.1. Airflow 2 Docker Compose 파일 공식문서에서의 Docker Compose 파일 기본적으로 CeleryExecutor를 사용하는 환경설정이 정의되어 있지만, 이번 예제에서는 단일 머신에서 구동을 할 것이기 때문에 LocalExecutor를 사용하도록 docker-compose.yml 파일을 아래와 같이 약간만 변경하도록 하겠습니다. (기본적으로 Celery와 Redis와 관련된 설정은 제외하였습니다.)\n./docker-compose.yml\nversion: '3' x-airflow-common: \u0026airflow-common image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}  # Docker 이미지는 Airflow 2.1.2 를 사용 environment: \u0026airflow-common-env AIRFLOW__CORE__EXECUTOR: LocalExecutor  # LocalExecutor를 사용 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # PostgreSQL을 데이터베이스로 사용 AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' # 실행시 DAG가 자동으로 동작되지 않음 AIRFLOW__CORE__LOAD_EXAMPLES: 'false' # 기본 예제 DAG는 불러오지 않음 AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} volumes:\t # DAG, 로그, 플러그인 파일이 저장될 경로의 폴더를 볼륨으로 마운트함 - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins - ./data_files:/opt/airflow/data_files user: \"${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\" depends_on: postgres: condition: service_healthy services: postgres: image: postgres:13 environment: POSTGRES_USER: airflow POSTGRES_PASSWORD: airflow POSTGRES_DB: airflow volumes: - postgres-db-volume:/var/lib/postgresql/data healthcheck: test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"] interval: 10s retries: 5 restart: always airflow-webserver: : *airflow-common command: webserver ports: - 8080:8080\t# 실행되는 컨테이너와 localhost의 포트를 8080으로 맞춰, 실행 중인 Webserver에 접근할 수 있도록 함 restart: always airflow-scheduler: : *airflow-common command: scheduler restart: always airflow-init: : *airflow-common command: version environment: : *airflow-common-env _AIRFLOW_DB_UPGRADE: 'true' _AIRFLOW_WWW_USER_CREATE: 'true' # 기본 유저 계정을 생성함 _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}  # 기본 유저 Username _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}  # 기본 유저 Password volumes: postgres-db-volume: 1.2. Docker Compose 파일 실행하여 Airflow 컨테이너 띄우기   우선 DAG, 로그, 플러그인, 데이터 파일을 저장할 경로를 생성합니다.\n mkdir ./dags ./logs ./plugins ./data_files    Docker 컨테이너에서 구동되는 파일과 호스트의 파일이 동일한 user / group permission을 가질 수 있도록 .env파일을 생성합니다.\n echo -e \"AIRFLOW_UID=$(id -u)\\nAIRFLOW_GID=0\"  .env    Airflow를 실행하기에 앞서, 데이터베이스를 구축하고 유저를 생성하는 명령어를 실행합니다.\n docker-compose up airflow-init 다음과 같은 메세지와 함께 완료되었다면 데이터베이스와 유저 생성이 성공적으로 실행된 것입니다.       3번이 정상적으로 실행되었다면, 다음 명령어를 통해 Airflow를 실행합니다. (실행하는 시간이 걸릴 수 있습니다.)\n docker-compose up    localhost:8080 , 즉 Airflow Webserver UI로 접속해보겠습니다.\n docker-compose.yml 에서 정의한 Username과 Password를 입력해서 로그인합니다.            Security - List Users 경로에서 추가적으로 유저 계정을 생성할 수 있습니다.           1.3. DAG 작성하기 예제로 사용할 수 있는 간단한 DAG를 작성하고 로컬 경로(./dags/)에 저장해서, 현재 구동 중인 Airflow 컨테이너에서 실행해보겠습니다. 해당 DAG 파일은 Launch Library API에서 우주 발사 관련 데이터를 다운 받고, 이 파일을 변환하여 text파일로 저장하는 프로세스를 수행합니다.\n( Launch Library API는 SpaceDev 사에서 제공하는 API로, 전세계의 우주 로켓 등 모든 우주 발사체의 발사 일정을 데이터로 제공하는 무료 API입니다. )\n./dags/dag_rocket_launch_schedule.py\nimport datetime import requests import airflow from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator def save_launch_schedule_txt(): with open(\"/tmp/launches.json\") as f: launches = json.load(f)['results'] launches_df = pd.DataFrame.from_dict(launches, orient='records') cur_datetime = str(datetime.datetime.now()) launches_df.to_csv('/opt/airflow/data_files/{}_launch_schedule.txt'.format(cur_datetime), encoding='utf-8', sep='\\t') default_args = { \"start_date\" : airflow.utils.dates.days_ago(1), \"owner\" : \"airflow_admin\" } with DAG( dag_id=\"get_upcoming_rocker_launch_schedule\", schedule_interval=@daily, default_args = default_args ) as dag: download_launches = BashOperator( task_id = \"download_launches\", bash_command = \"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\" ) save_launch_schedule = PythonOperator( task_id = \"update_launch_schedule\", python_callable = save_launch_schedule_txt ) download_launches  save_launch_schedule 작성한 DAG 가 실행되었고, 결과 파일이 성공적으로 저장된 것을 마운트한 로컬 머신의 디렉토리에서 확인할 수 있습니다.\n                 2. 커스텀 이미지 만들기 가장 기본적인 예제를 순서대로 따라가며 Airflow 2.x를 Docker를 활용해서 실행해보았습니다. 그렇다면 이대로 예제를 수정해가며 DAG를 작성해서 실전에 적용시켜도 될까요? 실제 프로젝트에 적용하기에는 한계가 존재합니다. Airflow 공식 Docker 이미지는 말 그대로 reference image로, Airflow가 Docker 컨테이너에서 구동하기 위한 최소한의 요소만 갖추어져 있습니다. 즉, 대부분의 경우에는 실제 프로젝트에서 사용하는 커스텀 패키지나 의존적인 부분이 설치되어 있지 않습니다. 따라서 사용자는 Airflow 이미지를 활용하여 커스텀 이미지를 빌드하는 것이 적절한 사용 예라고 Airflow에서 안내하고 있습니다. (Airflow 공식 문서 - Building Image)\n2.1. 커스텀 Airflow Docker 이미지 만들기 커스텀 Docker 이미지를 만드는 것은 그리 어렵지 않습니다.\n  사용하고자 하는 패키지가 정의된 requirements.txt 파일 작성합니다.\n./requirements.txt(예시)\nCython==0.29.23 numpy==1.19.2 pymssql==2.1.5 pandas==1.1.2 pytz==2020.1   Docker 파일 작성을 작성합니다.\n./Dockerfile\nFROMapache/airflow:2.1.2 # 앞서 다룬 docker-compose.yml 파일에서 사용한 동일한 Airflow 이미지를 사용합니다. USERroot RUN apt-get update \\  \u0026\u0026 apt-get install -y --no-install-recommends \\  build-essential libopenmpi-dev \\  \u0026\u0026 apt-get autoremove -yqq --purge \\  \u0026\u0026 apt-get clean \\  \u0026\u0026 rm -rf /var/lib/apt/lists/* USERairflowCOPY requirements.txt requirements.txt # 현재 경로에 작성해둔 requirements.txt 파일으로 복사합니다.RUN pip3 install -r requirements.txt --no-cache-dir # 패키지를 설치합니다. 캐쉬는 저장할 필요가 없음을 파라미터로 넘깁니다.  커스텀 Docker 이미지를 빌드합니다.\n  --tag 로 이미지의 이름과 버전을 전달합니다.\n docker build . --tag \"my_custom_image:0.0.1\"        Docker 이미지가 빌드된 것을 확인할 수 있습니다.\n          2.2. 커스텀 Airflow Docker 이미지를 통해 컨테이너 띄우기  커스텀 Docker 이미지를 활용하여 Airflow를 실행할 수 있도록 docker-compose.yml를 수정합니다.\n./docker-compose.yml\nversion: '3' x-airflow-common: \u0026airflow-common image: ${AIRFLOW_IMAGE_NAME:-my_custom_image:0.0.1} # Airflow 이미지를 커스텀 Docker 이미지로 수정합니다. environment: \u0026airflow-common-env AIRFLOW__CORE__EXECUTOR: LocalExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' ...   앞서 docker-compose.yml 파일을 실행하여 컨테이너를 띄웠던 과정을 다시 실행합니다.\ndocker-compose up airflow-init docker-compose up   3. 마무리하며 Airflow 공식 문서를 참고하여, Docker를 활용하여 2.x 버전을 실행시키는 과정을 정리해봤습니다. 확실히 Docker를 쓰면 앞으로의 관리나 지식 전달 등의 과정도 무척 간소화될 것 같아 적극적으로 적용해야겠다는 생각이 드네요.\nReference  Airflow 공식 문서 - Running Airflow in Docker Airflow 공식 문서 - Building Image Datapipelines with Apache Airflow (Bas Harenslak, Julian de Ruiter)  ","wordCount":"928","inLanguage":"en","datePublished":"2021-08-15T19:43:01+09:00","dateModified":"2021-08-15T19:43:01+09:00","author":{"@type":"Person","name":"Wonyoung Seo"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://lucaseo.github.io/posts/2021-08-15-airflow2-with-docker/"},"publisher":{"@type":"Organization","name":"WY's Tech Blog","logo":{"@type":"ImageObject","url":"https://lucaseo.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://lucaseo.github.io/ accesskey=h title="Tech Blog (Alt + H)">Tech Blog</a>
<span class=logo-switches><a id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://lucaseo.github.io/archives/ title=posts><span>posts</span></a></li><li><a href=https://lucaseo.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://lucaseo.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://lucaseo.github.io/about/ title=about><span>about</span></a></li><li><a href=https://lucaseo.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[KR] Docker를 활용하여 Airflow 2.x 실행하기</h1><div class=post-meta>August 15, 2021&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Wonyoung Seo</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#0-docker-%ec%84%a4%ec%b9%98 aria-label="0. Docker 설치">0. Docker 설치</a></li><li><a href=#1-%ec%95%84%eb%ac%b4%ea%b2%83%eb%8f%84-%eb%aa%a8%eb%a5%b4%ec%a7%80%eb%a7%8c-%ec%9d%bc%eb%8b%a8-%ec%8b%a4%ed%96%89%ed%95%b4%eb%b3%b4%ea%b8%b0 aria-label="1. 아무것도 모르지만 일단 실행해보기">1. 아무것도 모르지만 일단 실행해보기</a><ul><li><a href=#11-airflow-2-docker-compose-%ed%8c%8c%ec%9d%bc aria-label="1.1. Airflow 2 Docker Compose 파일">1.1. Airflow 2 Docker Compose 파일</a></li><li><a href=#12-docker-compose-%ed%8c%8c%ec%9d%bc-%ec%8b%a4%ed%96%89%ed%95%98%ec%97%ac-airflow-%ec%bb%a8%ed%85%8c%ec%9d%b4%eb%84%88-%eb%9d%84%ec%9a%b0%ea%b8%b0 aria-label="1.2. Docker Compose 파일 실행하여 Airflow 컨테이너 띄우기">1.2. Docker Compose 파일 실행하여 Airflow 컨테이너 띄우기</a></li><li><a href=#13-dag-%ec%9e%91%ec%84%b1%ed%95%98%ea%b8%b0 aria-label="1.3. DAG 작성하기">1.3. DAG 작성하기</a></li></ul></li><li><a href=#2-%ec%bb%a4%ec%8a%a4%ed%85%80-%ec%9d%b4%eb%af%b8%ec%a7%80-%eb%a7%8c%eb%93%a4%ea%b8%b0 aria-label="2. 커스텀 이미지 만들기">2. 커스텀 이미지 만들기</a><ul><li><a href=#21-%ec%bb%a4%ec%8a%a4%ed%85%80-airflow-docker-%ec%9d%b4%eb%af%b8%ec%a7%80-%eb%a7%8c%eb%93%a4%ea%b8%b0 aria-label="2.1. 커스텀 Airflow Docker 이미지 만들기">2.1. 커스텀 Airflow Docker 이미지 만들기</a></li><li><a href=#22-%ec%bb%a4%ec%8a%a4%ed%85%80-airflow-docker-%ec%9d%b4%eb%af%b8%ec%a7%80%eb%a5%bc-%ed%86%b5%ed%95%b4-%ec%bb%a8%ed%85%8c%ec%9d%b4%eb%84%88-%eb%9d%84%ec%9a%b0%ea%b8%b0 aria-label="2.2. 커스텀 Airflow Docker 이미지를 통해 컨테이너 띄우기">2.2. 커스텀 Airflow Docker 이미지를 통해 컨테이너 띄우기</a></li></ul></li><li><a href=#3-%eb%a7%88%eb%ac%b4%eb%a6%ac%ed%95%98%eb%a9%b0 aria-label="3. 마무리하며">3. 마무리하며</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><p>작년 Airflow를 처음으로 실전에 적용하는 과정에서, 다양한 환경설정과 그에 따라 달라지는 데이터베이스 등 신경써야 할 부분이 많았고, 이러한 점 때문에 지식 전달은 물론 다른 동료들이 직접 사용하는데 진입장벽이 좀 생긴다는 점을 느낄 수 있었습니다. 마침 새로운 프로젝트부터는 Airflow 2.x 을 도입하기로 했고, 적용해야 되는 김에 Docker를 활용했습니다. 그리하여 이번 글에서는 Docker를 활용한 Airflow 2.x을 설치하는 과정와 방법을 정리합니다. 기존에 많이 알려진 Airflow와 Docker와 관련된 예제에는 <a href=https://hub.docker.com/r/puckel/docker-airflow target=_blank>puckel의 Airflow Docker Image</a>가 많이 사용되었는데요, 이 이미지는 Airflow 1.x에 한정되어 있기 때문에, 이번 글에서는 Airflow의 공식 Docker 이미지를 활용해보도록 하겠습니다.</p><h2 id=0-docker-설치>0. Docker 설치<a hidden class=anchor aria-hidden=true href=#0-docker-설치>#</a></h2><p>Docker가 설치되어 있지 않은 경우 아래의 링크를 통해 Docker를 설치하도록 합니다.</p><ul><li><a href=https://docs.docker.com/desktop/ target=_blank>Docker 설치 (맥, 윈도우, 리눅스)</a></li></ul><p>맥, 윈도우의 경우 Docker 데스크탑을 설치할 시, Docker Compose가 함께 설치 됩니다. 단, 리눅스의 경우에는 별도의 설치가 필요합니다.</p><ul><li><a href=https://docs.docker.com/compose/install/ target=_blank>Docker Compose 설치 (리눅스)</a></li></ul><h2 id=1-아무것도-모르지만-일단-실행해보기>1. 아무것도 모르지만 일단 실행해보기<a hidden class=anchor aria-hidden=true href=#1-아무것도-모르지만-일단-실행해보기>#</a></h2><p>Airflow 공식 문서(<a href=https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html target=_blank>Running Airflow in Docker</a>)에서는 기본적인 Docker 및 Docker Compose를 활용한 Airflow 예제를 제공하고 있습니다. 이 파일을 토대로 일단 한번 실행 과정을 따라가보겠습니다.</p><h3 id=11-airflow-2-docker-compose-파일>1.1. Airflow 2 Docker Compose 파일<a hidden class=anchor aria-hidden=true href=#11-airflow-2-docker-compose-파일>#</a></h3><p>공식문서에서의 <a href=https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml target=_blank>Docker Compose 파일</a> 기본적으로 CeleryExecutor를 사용하는 환경설정이 정의되어 있지만, 이번 예제에서는 단일 머신에서 구동을 할 것이기 때문에 LocalExecutor를 사용하도록 <code>docker-compose.yml</code> 파일을 아래와 같이 약간만 변경하도록 하겠습니다. (기본적으로 Celery와 Redis와 관련된 설정은 제외하였습니다.)</p><p><code>./docker-compose.yml</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>version</span>: <span style=color:#e6db74>&#39;3&#39;</span>
<span style=color:#f92672>x-airflow-common</span>:
  <span style=color:#75715e>&amp;airflow-common</span>
  <span style=color:#f92672>image</span>: <span style=color:#ae81ff>${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}      </span> <span style=color:#75715e># Docker 이미지는 Airflow 2.1.2 를 사용</span>
  <span style=color:#f92672>environment</span>:
    <span style=color:#75715e>&amp;airflow-common-env</span>
    <span style=color:#f92672>AIRFLOW__CORE__EXECUTOR</span>: <span style=color:#ae81ff>LocalExecutor                </span> <span style=color:#75715e># LocalExecutor를 사용</span>
    <span style=color:#f92672>AIRFLOW__CORE__SQL_ALCHEMY_CONN</span>: <span style=color:#ae81ff>postgresql+psycopg2://airflow:airflow@postgres/airflow</span> <span style=color:#75715e># PostgreSQL을 데이터베이스로 사용</span>
    <span style=color:#f92672>AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION</span>: <span style=color:#e6db74>&#39;true&#39;</span>     <span style=color:#75715e># 실행시 DAG가 자동으로 동작되지 않음</span>
    <span style=color:#f92672>AIRFLOW__CORE__LOAD_EXAMPLES</span>: <span style=color:#e6db74>&#39;false&#39;</span>                  <span style=color:#75715e># 기본 예제 DAG는 불러오지 않음</span>
    <span style=color:#f92672>AIRFLOW__API__AUTH_BACKEND</span>: <span style=color:#e6db74>&#39;airflow.api.auth.backend.basic_auth&#39;</span>
    <span style=color:#f92672>_PIP_ADDITIONAL_REQUIREMENTS</span>: <span style=color:#ae81ff>${_PIP_ADDITIONAL_REQUIREMENTS:-}</span>
  <span style=color:#ae81ff>volumes:			                                      </span> <span style=color:#75715e># DAG, 로그, 플러그인 파일이 저장될 경로의 폴더를 볼륨으로 마운트함</span>
    - <span style=color:#ae81ff>./dags:/opt/airflow/dags</span>
    - <span style=color:#ae81ff>./logs:/opt/airflow/logs</span>
    - <span style=color:#ae81ff>./plugins:/opt/airflow/plugins</span>
    - <span style=color:#ae81ff>./data_files:/opt/airflow/data_files</span>
  <span style=color:#f92672>user</span>: <span style=color:#e6db74>&#34;${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}&#34;</span>
  <span style=color:#f92672>depends_on</span>:
    <span style=color:#f92672>postgres</span>:
      <span style=color:#f92672>condition</span>: <span style=color:#ae81ff>service_healthy</span>

<span style=color:#f92672>services</span>:
  <span style=color:#f92672>postgres</span>:
    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>postgres:13</span>
    <span style=color:#f92672>environment</span>:
      <span style=color:#f92672>POSTGRES_USER</span>: <span style=color:#ae81ff>airflow</span>
      <span style=color:#f92672>POSTGRES_PASSWORD</span>: <span style=color:#ae81ff>airflow</span>
      <span style=color:#f92672>POSTGRES_DB</span>: <span style=color:#ae81ff>airflow</span>
    <span style=color:#f92672>volumes</span>:
      - <span style=color:#ae81ff>postgres-db-volume:/var/lib/postgresql/data</span>
    <span style=color:#f92672>healthcheck</span>:
      <span style=color:#f92672>test</span>: [<span style=color:#e6db74>&#34;CMD&#34;</span>, <span style=color:#e6db74>&#34;pg_isready&#34;</span>, <span style=color:#e6db74>&#34;-U&#34;</span>, <span style=color:#e6db74>&#34;airflow&#34;</span>]
      <span style=color:#f92672>interval</span>: <span style=color:#ae81ff>10s</span>
      <span style=color:#f92672>retries</span>: <span style=color:#ae81ff>5</span>
    <span style=color:#f92672>restart</span>: <span style=color:#ae81ff>always</span>

  <span style=color:#f92672>airflow-webserver</span>:
    <span style=color:#f92672>&lt;&lt;</span>: <span style=color:#75715e>*airflow-common</span>
    <span style=color:#f92672>command</span>: <span style=color:#ae81ff>webserver</span>
    <span style=color:#f92672>ports</span>:
      - <span style=color:#ae81ff>8080</span>:<span style=color:#ae81ff>8080</span>						 <span style=color:#75715e># 실행되는 컨테이너와 localhost의 포트를 8080으로 맞춰, 실행 중인 Webserver에 접근할 수 있도록 함</span>
    <span style=color:#f92672>restart</span>: <span style=color:#ae81ff>always</span>

  <span style=color:#f92672>airflow-scheduler</span>:
    <span style=color:#f92672>&lt;&lt;</span>: <span style=color:#75715e>*airflow-common</span>
    <span style=color:#f92672>command</span>: <span style=color:#ae81ff>scheduler</span>
    <span style=color:#f92672>restart</span>: <span style=color:#ae81ff>always</span>

  <span style=color:#f92672>airflow-init</span>:
    <span style=color:#f92672>&lt;&lt;</span>: <span style=color:#75715e>*airflow-common</span>
    <span style=color:#f92672>command</span>: <span style=color:#ae81ff>version</span>
    <span style=color:#f92672>environment</span>:
      <span style=color:#f92672>&lt;&lt;</span>: <span style=color:#75715e>*airflow-common-env</span>
      <span style=color:#f92672>_AIRFLOW_DB_UPGRADE</span>: <span style=color:#e6db74>&#39;true&#39;</span>
      <span style=color:#f92672>_AIRFLOW_WWW_USER_CREATE</span>: <span style=color:#e6db74>&#39;true&#39;</span> 		                              <span style=color:#75715e># 기본 유저 계정을 생성함</span>
      <span style=color:#f92672>_AIRFLOW_WWW_USER_USERNAME</span>: <span style=color:#ae81ff>${_AIRFLOW_WWW_USER_USERNAME:-airflow} </span> <span style=color:#75715e># 기본 유저 Username</span>
      <span style=color:#f92672>_AIRFLOW_WWW_USER_PASSWORD</span>: <span style=color:#ae81ff>${_AIRFLOW_WWW_USER_PASSWORD:-airflow} </span> <span style=color:#75715e># 기본 유저 Password</span>

<span style=color:#f92672>volumes</span>:
  <span style=color:#f92672>postgres-db-volume</span>:
</code></pre></div><h3 id=12-docker-compose-파일-실행하여-airflow-컨테이너-띄우기>1.2. Docker Compose 파일 실행하여 Airflow 컨테이너 띄우기<a hidden class=anchor aria-hidden=true href=#12-docker-compose-파일-실행하여-airflow-컨테이너-띄우기>#</a></h3><ol><li><p>우선 DAG, 로그, 플러그인, 데이터 파일을 저장할 경로를 생성합니다.</p><ul><li><code>mkdir ./dags ./logs ./plugins ./data_files</code></li></ul></li><li><p>Docker 컨테이너에서 구동되는 파일과 호스트의 파일이 동일한 user / group permission을 가질 수 있도록 <code>.env</code>파일을 생성합니다.</p><ul><li><code>echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" > .env</code></li></ul></li><li><p>Airflow를 실행하기에 앞서, 데이터베이스를 구축하고 유저를 생성하는 명령어를 실행합니다.</p><ul><li><code>docker-compose up airflow-init</code></li><li>다음과 같은 메세지와 함께 완료되었다면 데이터베이스와 유저 생성이 성공적으로 실행된 것입니다.</li></ul><figure><center><img src=/2021-08-15-airflow2-with-docker/image1.png></center></figure></li><li><p>3번이 정상적으로 실행되었다면, 다음 명령어를 통해 Airflow를 실행합니다. (실행하는 시간이 걸릴 수 있습니다.)</p><ul><li><code>docker-compose up</code></li></ul></li><li><p>localhost:8080 , 즉 Airflow Webserver UI로 접속해보겠습니다.</p><ul><li><code>docker-compose.yml</code> 에서 정의한 Username과 Password를 입력해서 로그인합니다.<ul><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image2.png></center></figure></li><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image2-1.png></center></figure></li></ul></li><li>Security -> List Users 경로에서 추가적으로 유저 계정을 생성할 수 있습니다.<ul><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image3.png></center></figure></li></ul></li></ul></li></ol><h3 id=13-dag-작성하기>1.3. DAG 작성하기<a hidden class=anchor aria-hidden=true href=#13-dag-작성하기>#</a></h3><p>예제로 사용할 수 있는 간단한 DAG를 작성하고 로컬 경로(<code>./dags/</code>)에 저장해서, 현재 구동 중인 Airflow 컨테이너에서 실행해보겠습니다. 해당 DAG 파일은 Launch Library API에서 우주 발사 관련 데이터를 다운 받고, 이 파일을 변환하여 text파일로 저장하는 프로세스를 수행합니다.</p><p>( <em><a href=https://ll.thespacedevs.com/ target=_blank>Launch Library API</a>는 SpaceDev 사에서 제공하는 API로, 전세계의 우주 로켓 등 모든 우주 발사체의 발사 일정을 데이터로 제공하는 무료 API입니다.</em> )</p><p><code>./dags/dag_rocket_launch_schedule.py</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> datetime

<span style=color:#f92672>import</span> requests
<span style=color:#f92672>import</span> airflow
<span style=color:#f92672>from</span> airflow <span style=color:#f92672>import</span> DAG
<span style=color:#f92672>from</span> airflow.operators.bash <span style=color:#f92672>import</span> BashOperator
<span style=color:#f92672>from</span> airflow.operators.python <span style=color:#f92672>import</span> PythonOperator


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>save_launch_schedule_txt</span>():
	<span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;/tmp/launches.json&#34;</span>) <span style=color:#66d9ef>as</span> f:
		launches <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>load(f)[<span style=color:#e6db74>&#39;results&#39;</span>]

	launches_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame<span style=color:#f92672>.</span>from_dict(launches, orient<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;records&#39;</span>)

	cur_datetime <span style=color:#f92672>=</span> str(datetime<span style=color:#f92672>.</span>datetime<span style=color:#f92672>.</span>now())
	launches_df<span style=color:#f92672>.</span>to_csv(<span style=color:#e6db74>&#39;/opt/airflow/data_files/{}_launch_schedule.txt&#39;</span><span style=color:#f92672>.</span>format(cur_datetime), encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;utf-8&#39;</span>, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#39;</span>)

default_args <span style=color:#f92672>=</span> {
	<span style=color:#e6db74>&#34;start_date&#34;</span> : airflow<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>dates<span style=color:#f92672>.</span>days_ago(<span style=color:#ae81ff>1</span>),
	<span style=color:#e6db74>&#34;owner&#34;</span> : <span style=color:#e6db74>&#34;airflow_admin&#34;</span>
}

<span style=color:#66d9ef>with</span> DAG(
	dag_id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;get_upcoming_rocker_launch_schedule&#34;</span>,
	schedule_interval<span style=color:#f92672>=</span><span style=color:#a6e22e>@daily</span>,
	default_args <span style=color:#f92672>=</span> default_args
	) <span style=color:#66d9ef>as</span> dag:

	download_launches <span style=color:#f92672>=</span> BashOperator(
		task_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;download_launches&#34;</span>,
		bash_command <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;curl -o /tmp/launches.json -L &#39;https://ll.thespacedevs.com/2.0.0/launch/upcoming&#39;&#34;</span>
		)

	save_launch_schedule <span style=color:#f92672>=</span> PythonOperator(
		task_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;update_launch_schedule&#34;</span>,
		python_callable <span style=color:#f92672>=</span> save_launch_schedule_txt
		)

download_launches <span style=color:#f92672>&gt;&gt;</span> save_launch_schedule
</code></pre></div><p>작성한 DAG 가 실행되었고, 결과 파일이 성공적으로 저장된 것을 마운트한 로컬 머신의 디렉토리에서 확인할 수 있습니다.</p><ul><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image4.png></center></figure></li><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image5.png></center></figure></li><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image6.png></center></figure></li></ul><h2 id=2-커스텀-이미지-만들기>2. 커스텀 이미지 만들기<a hidden class=anchor aria-hidden=true href=#2-커스텀-이미지-만들기>#</a></h2><p>가장 기본적인 예제를 순서대로 따라가며 Airflow 2.x를 Docker를 활용해서 실행해보았습니다. 그렇다면 이대로 예제를 수정해가며 DAG를 작성해서 실전에 적용시켜도 될까요? 실제 프로젝트에 적용하기에는 한계가 존재합니다. Airflow 공식 Docker 이미지는 말 그대로 <strong>reference image</strong>로, Airflow가 Docker 컨테이너에서 구동하기 위한 최소한의 요소만 갖추어져 있습니다. 즉, 대부분의 경우에는 실제 프로젝트에서 사용하는 커스텀 패키지나 의존적인 부분이 설치되어 있지 않습니다. 따라서 사용자는 Airflow 이미지를 활용하여 커스텀 이미지를 빌드하는 것이 적절한 사용 예라고 Airflow에서 안내하고 있습니다. (<a href=https://airflow.apache.org/docs/docker-stack/build.html target=_blank>Airflow 공식 문서 - Building Image</a>)</p><h3 id=21-커스텀-airflow-docker-이미지-만들기>2.1. 커스텀 Airflow Docker 이미지 만들기<a hidden class=anchor aria-hidden=true href=#21-커스텀-airflow-docker-이미지-만들기>#</a></h3><p>커스텀 Docker 이미지를 만드는 것은 그리 어렵지 않습니다.</p><ol><li><p>사용하고자 하는 패키지가 정의된 requirements.txt 파일 작성합니다.</p><p><code>./requirements.txt</code>(예시)</p><pre><code>Cython==0.29.23
numpy==1.19.2
pymssql==2.1.5
pandas==1.1.2
pytz==2020.1
</code></pre></li><li><p>Docker 파일 작성을 작성합니다.</p><p><code>./Dockerfile</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-docker data-lang=docker><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> apache/airflow:2.1.2    # 앞서 다룬 docker-compose.yml 파일에서 사용한 동일한 Airflow 이미지를 사용합니다. </span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>USER</span><span style=color:#e6db74> root</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>   <span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  <span style=color:#f92672>&amp;&amp;</span> apt-get install -y --no-install-recommends <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>         build-essential libopenmpi-dev <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  <span style=color:#f92672>&amp;&amp;</span> apt-get autoremove -yqq --purge <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  <span style=color:#f92672>&amp;&amp;</span> apt-get clean <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  <span style=color:#f92672>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>   <span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>USER</span><span style=color:#e6db74> airflow</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> requirements.txt requirements.txt  <span style=color:#75715e># 현재 경로에 작성해둔 requirements.txt 파일으로 복사합니다.</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> pip3 install -r requirements.txt --no-cache-dir  <span style=color:#75715e># 패키지를 설치합니다. 캐쉬는 저장할 필요가 없음을 파라미터로 넘깁니다.</span><span style=color:#960050;background-color:#1e0010>
</span></code></pre></div></li><li><p>커스텀 Docker 이미지를 빌드합니다.</p><ul><li><p><code>--tag</code> 로 이미지의 이름과 버전을 전달합니다.</p><ul><li><code>docker build . --tag "my_custom_image:0.0.1"</code></li><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image7.png></center></figure></li></ul></li><li><p>Docker 이미지가 빌드된 것을 확인할 수 있습니다.</p><ul><li><figure><center><img src=/2021-08-15-airflow2-with-docker/image8.png></center></figure></li></ul></li></ul></li></ol><h3 id=22-커스텀-airflow-docker-이미지를-통해-컨테이너-띄우기>2.2. 커스텀 Airflow Docker 이미지를 통해 컨테이너 띄우기<a hidden class=anchor aria-hidden=true href=#22-커스텀-airflow-docker-이미지를-통해-컨테이너-띄우기>#</a></h3><ol start=4><li><p>커스텀 Docker 이미지를 활용하여 Airflow를 실행할 수 있도록 <code>docker-compose.yml</code>를 수정합니다.</p><p><code>./docker-compose.yml</code></p><pre><code>version: '3'
x-airflow-common:
  &amp;airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-my_custom_image:0.0.1}   # Airflow 이미지를 커스텀 Docker 이미지로 수정합니다.
  environment:
    &amp;airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
...
</code></pre></li><li><p>앞서 docker-compose.yml 파일을 실행하여 컨테이너를 띄웠던 과정을 다시 실행합니다.</p><pre><code>docker-compose up airflow-init
docker-compose up
</code></pre></li></ol><h2 id=3-마무리하며>3. 마무리하며<a hidden class=anchor aria-hidden=true href=#3-마무리하며>#</a></h2><p>Airflow 공식 문서를 참고하여, Docker를 활용하여 2.x 버전을 실행시키는 과정을 정리해봤습니다. 확실히 Docker를 쓰면 앞으로의 관리나 지식 전달 등의 과정도 무척 간소화될 것 같아 적극적으로 적용해야겠다는 생각이 드네요.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html target=_blank>Airflow 공식 문서 - Running Airflow in Docker</a></li><li><a href=https://airflow.apache.org/docs/docker-stack/build.html target=_blank>Airflow 공식 문서 - Building Image</a></li><li>Datapipelines with Apache Airflow (Bas Harenslak, Julian de Ruiter)</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://lucaseo.github.io/tags/airflow/>airflow</a></li><li><a href=https://lucaseo.github.io/tags/docker/>docker</a></li></ul></footer></article></main><footer class=footer><span>© Wonyoung Seo 2021</span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>