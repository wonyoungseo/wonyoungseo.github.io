<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[KR] LangChain 과 RAG 찍먹 후기 | Wonyoung's Tech Blog</title><meta name=keywords content="langchain,rag,llm,openai,vectordb"><meta name=description content="최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라가보았다.
참고 튜토리얼 - LangChain: Q&amp;amp;A with RAG
&nbsp
테니스GPT 챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 이번에도 테니스 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다. 대부분의 구현은 튜토리얼을 따라갔지만, 아주 약간은 다르게 설정했다.
&nbsp
전처리 Loader  테니스와 관련된 문서를 그 자리에서 간략하게 수집했다."><meta name=author content="Wonyoung Seo"><link rel=canonical href=https://wonyoungseo.github.io/posts/2024-03-19-langchain-rag-qa-toy-tennis-app/><meta name=google-site-verification content="XYZabc"><link href=/assets/css/stylesheet.min.d8cbf60331b9ced42909130bf88f8b97d2eb3de242444dcc9e2df410ceb098b9.css integrity="sha256-2Mv2AzG5ztQpCRML+I+Ll9LrPeJCRE3Mni30EM6wmLk=" rel="preload stylesheet" as=style><link rel=icon href=https://wonyoungseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://wonyoungseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://wonyoungseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://wonyoungseo.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://wonyoungseo.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><meta property="og:title" content="[KR] LangChain 과 RAG 찍먹 후기"><meta property="og:description" content="최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라가보았다.
참고 튜토리얼 - LangChain: Q&amp;amp;A with RAG
&nbsp
테니스GPT 챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 이번에도 테니스 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다. 대부분의 구현은 튜토리얼을 따라갔지만, 아주 약간은 다르게 설정했다.
&nbsp
전처리 Loader  테니스와 관련된 문서를 그 자리에서 간략하게 수집했다."><meta property="og:type" content="article"><meta property="og:url" content="https://wonyoungseo.github.io/posts/2024-03-19-langchain-rag-qa-toy-tennis-app/"><meta property="og:image" content="https://wonyoungseo.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:published_time" content="2024-03-18T23:57:30+09:00"><meta property="article:modified_time" content="2024-03-18T23:57:30+09:00"><meta property="og:site_name" content="WY's Tech Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wonyoungseo.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[KR] LangChain 과 RAG 찍먹 후기"><meta name=twitter:description content="최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라가보았다.
참고 튜토리얼 - LangChain: Q&amp;amp;A with RAG
&nbsp
테니스GPT 챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 이번에도 테니스 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다. 대부분의 구현은 튜토리얼을 따라갔지만, 아주 약간은 다르게 설정했다.
&nbsp
전처리 Loader  테니스와 관련된 문서를 그 자리에서 간략하게 수집했다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[KR] LangChain 과 RAG 찍먹 후기","name":"[KR] LangChain 과 RAG 찍먹 후기","description":"최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라가보았다.\n참고 튜토리얼 - LangChain: Q\u0026amp;amp;amp;A …","keywords":["langchain","rag","llm","openai","vectordb"],"articleBody":"최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라가보았다.\n참고 튜토리얼 - LangChain: Q\u0026amp;A with RAG\n\u0026nbsp\n테니스GPT 챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 이번에도 테니스 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다. 대부분의 구현은 튜토리얼을 따라갔지만, 아주 약간은 다르게 설정했다.\n\u0026nbsp\n전처리 Loader  테니스와 관련된 문서를 그 자리에서 간략하게 수집했다. 위키파일도 있으며, 동호인 웹사이트에 존재하는 문서도 있기에 모두 txt 파일로 저장했다.  Text Splitter Text Splitter의 경우 그대로 RecursiveCharacterTxtSplitter 를 사용했다. Chunk가 충분히 작아질 때까지 단락, 문단, 문장, 단어 순으로 split을 반복 시도하는, 가장 범용으로 사용되는 splitter이다.\nEmbedding Embedding의 경우 가장 가성비가 좋은 text-embedding-3-small을 사용했다. 디폴트가 가성비가 더 안 좋은 text-embedding-ada-002-v2 인 것을 나중에 알았다. (pricing)\nVector Database Vector DB 는 FAISS를 사용해보았다. 튜토리얼의 흐름에서는 애플리케이션이 실행될 때마다 문서 인덱싱을 새로 하게 되는 것 같아서, 사전에 생성된 인덱스를 로컬에 저장하고 애플리케이션 실행 시 로드하도록 했다.\n\u0026nbsp\nRetrieval \u0026 Generation Prompt 프롬프트 템플릿으로는 튜토리얼에서도 사용되었고, 가장 보편적인 형태를 보이는 rlm/rag-prompt 를 사용했다. (이 기회에 LangSmith도 경험이 되었다.)\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question}` Context: {context}` Answer: LLM Model LLM모델은 OpenAI의 gpt-3.5-turbo, gpt-4-turbo-preview 두가지 모두 사용해보았다. 아무래도 gpt4 가 한국어 생성 측면에서 전자에 비해 더 매끄럽고 자연스럽다. 과금은 좀 더 비싸지만.\nApplication 애플리케이션 프레임워크로는 Streamlit을 사용해서 정말 최소x3의 형태로 만들었다.\nChatGPT-3.5와의 비교 동일한 프롬프트를 적용하고, ChatGPT에서는 context에 해당하는 retrieval output을 제외하고 동일한 질문을 했을 때 출력되는 답변을 비교 해볼 수 있다. (** 참고로 RAG에 쓰인 텍스트 데이터의 시점은 ChatGPT와 거의 동일한 2021년이다.)\n 테니스 단체 정보        2024년 마지막이 될 것 같은 전설 같은 선수, 라파엘 나달에 관한 정보.        동호인 대회 나가서 가장 스트레스 받는 점        테린이 최대 관심사 중 하나        \u0026nbsp\nAI 또는 LLM에 대한 배경지식 없는 테니스 동호인이 실제로 사용한다고 가정했을 때, ChatGPT는 다소 두루뭉술한 답을 하는 반면, RAG에 기반해서 구축한 Q\u0026A 애플리케이션이 같은 프롬프트를 기반으로 질문에 대해서 훨씬 직접적이고, 간결한 답을 구사한다고 판단할 수 있다. 마지막 질문에 대해서 RAG 기반 애플리케이션에서는 할루시네이션도 없었다. (코트 예약은 … 빠르게 … 잘 … 정직하게 … )\n애초에 동작하는 방식과 목적, 적용 방안이 다르기에 직접적인 비교는 무리가 있지만, 제너럴한 답변을 하는 ChatGPT와 VectorDB에 인덱싱 된 문서로부터 답변을 추출한 RAG 사이에 이 정도의 차이점은 있다는 것을 확인할 수 있었다.\nRAG 찍먹 후기  랭체인을 활용한 RAG는 정말이 너무 간편했다. 간편함을 레버리지 삼아. 퀄리티를 높일 수 있는 고민거리에 시간과 에너지를 더 사용할 수 있을 것 같다. 가장 간편한 텍스트의 형태의 데이터만 사용했지만, 실무에서는 다양한 형태의 문서, 특히 이미지와 표가 함께 섞여 있는 PDF 같은 데이터를 “제대로”, “잘” 파싱하는 것이 실무에서는 매우 중요할 것 같다. Text Splitter - 실무를 한다면 어떤 것을 사용할 지, 이 부분이 가장 고민 될 것 같고, 실험을 많이 해봐야 할 것 같다. 가장 기본적으로 사용해볼 수 있는 CharacterTextSplitter, RecursiveCharacterTextSplitter 만 비교해서 사용해봤으나, HuggingFace의 토크나이저까지도 고려대상이 될 수 있을 것 같다. 가장 기본적으로 OpenAI Embedding을 사용했으나, 나중에는 과금의 요소를 무시하지 못 할 것 같다. (현재의 가격도 PoC나 토이프로젝트 레벨에서는 그리 부담되는 정도는 아니긴 하다.) 더 찾아보니 CashBackedEmbedding 을 통해서 동일한 임베딩의 경우에는 캐싱이 가능해 속도가 빠르고, 중복 과금을 피할 수 있다고 하니 이 부분은 기본으로 장착 하는 것이 좋아보인다. 결과적으로 약간의 과금을 감수하고서라도, 간편함과 가벼움에 이점이 있어서 OpenAI의 임베딩과 모델을 사용했다. Huggingface 모델도 사용해보려 했으나, 현재 자원에서 모델 로드 자체가 제한적이었다. (Colab에서도 모델의 사이즈 때문에 로드가 되지 않더라.) 인프라가 뒷받침이 된다면 HF에서 여러가지 모델을 실험, 비교해볼 수 있을 것 같다.  Reference  LangChain - Q\u0026amp;A with RAG OpenAI API Documentation  Code  Github  ","wordCount":"593","inLanguage":"en","datePublished":"2024-03-18T23:57:30+09:00","dateModified":"2024-03-18T23:57:30+09:00","author":{"@type":"Person","name":"Wonyoung Seo"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wonyoungseo.github.io/posts/2024-03-19-langchain-rag-qa-toy-tennis-app/"},"publisher":{"@type":"Organization","name":"Wonyoung's Tech Blog","logo":{"@type":"ImageObject","url":"https://wonyoungseo.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://wonyoungseo.github.io/ accesskey=h title="Tech Blog (Alt + H)">Tech Blog</a>
<span class=logo-switches><a id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://wonyoungseo.github.io/archives/ title=posts><span>posts</span></a></li><li><a href=https://wonyoungseo.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://wonyoungseo.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://wonyoungseo.github.io/about/ title=about><span>about</span></a></li><li><a href=https://wonyoungseo.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[KR] LangChain 과 RAG 찍먹 후기</h1><div class=post-meta>March 18, 2024&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Wonyoung Seo</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#%ed%85%8c%eb%8b%88%ec%8a%a4gpt aria-label=테니스GPT>테니스GPT</a><ul><li><a href=#%ec%a0%84%ec%b2%98%eb%a6%ac aria-label=전처리>전처리</a><ul><li><a href=#loader aria-label=Loader>Loader</a></li><li><a href=#text-splitter aria-label="Text Splitter">Text Splitter</a></li><li><a href=#embedding aria-label=Embedding>Embedding</a></li><li><a href=#vector-database aria-label="Vector Database">Vector Database</a></li></ul></li><li><a href=#retrieval--generation aria-label="Retrieval &amp;amp; Generation">Retrieval & Generation</a><ul><li><a href=#prompt aria-label=Prompt>Prompt</a></li><li><a href=#llm-model aria-label="LLM Model">LLM Model</a></li><li><a href=#application aria-label=Application>Application</a></li></ul></li></ul></li><li><a href=#chatgpt-35%ec%99%80%ec%9d%98-%eb%b9%84%ea%b5%90 aria-label="ChatGPT-3.5와의 비교">ChatGPT-3.5와의 비교</a></li><li><a href=#rag-%ec%b0%8d%eb%a8%b9-%ed%9b%84%ea%b8%b0 aria-label="RAG 찍먹 후기">RAG 찍먹 후기</a></li><li><a href=#reference aria-label=Reference>Reference</a></li><li><a href=#code aria-label=Code>Code</a></li></ul></div></details></div><div class=post-content><p>최근에 VectorDB 라는 키워드를 접하고 훑어보다, Vector Database → RAG (Retrieval Augmented Generation) → LangChain 의 흐름으로 연관이 있음을 알게 되었고, RAG 튜토리얼을 간단하게 따라가보았다.</p><p><a href=https://python.langchain.com/docs/use_cases/question_answering/ target=_blank>참고 튜토리얼 - LangChain: Q&amp;amp;A with RAG</a></p><p>&nbsp</p><h2 id=테니스gpt>테니스GPT<a hidden class=anchor aria-hidden=true href=#테니스gpt>#</a></h2><p>챗GPT가 처음 등장했을 때, 테니스 관련해서 이것저것 물어보며 가지고 놀았던 기억이 있어, 이번에도 테니스 관련된 아주 간단한 QA애플리케이션을 만들어보면 어떨까 했다. 대부분의 구현은 튜토리얼을 따라갔지만, 아주 약간은 다르게 설정했다.</p><p>&nbsp</p><h3 id=전처리>전처리<a hidden class=anchor aria-hidden=true href=#전처리>#</a></h3><h4 id=loader>Loader<a hidden class=anchor aria-hidden=true href=#loader>#</a></h4><ul><li>테니스와 관련된 문서를 그 자리에서 간략하게 수집했다. 위키파일도 있으며, 동호인 웹사이트에 존재하는 문서도 있기에 모두 txt 파일로 저장했다.</li></ul><h4 id=text-splitter>Text Splitter<a hidden class=anchor aria-hidden=true href=#text-splitter>#</a></h4><p>Text Splitter의 경우 그대로 <code>RecursiveCharacterTxtSplitter</code> 를 사용했다. Chunk가 충분히 작아질 때까지 단락, 문단, 문장, 단어 순으로 split을 반복 시도하는, 가장 범용으로 사용되는 splitter이다.</p><h4 id=embedding>Embedding<a hidden class=anchor aria-hidden=true href=#embedding>#</a></h4><p>Embedding의 경우 가장 가성비가 좋은 <code>text-embedding-3-small</code>을 사용했다. 디폴트가 가성비가 더 안 좋은 <code>text-embedding-ada-002-v2</code> 인 것을 나중에 알았다. (<a href=https://platform.openai.com/docs/guides/embeddings/embedding-models target=_blank>pricing</a>)</p><h4 id=vector-database>Vector Database<a hidden class=anchor aria-hidden=true href=#vector-database>#</a></h4><p>Vector DB 는 FAISS를 사용해보았다. 튜토리얼의 흐름에서는 애플리케이션이 실행될 때마다 문서 인덱싱을 새로 하게 되는 것 같아서, 사전에 생성된 인덱스를 로컬에 저장하고 애플리케이션 실행 시 로드하도록 했다.</p><p>&nbsp</p><h3 id=retrieval--generation>Retrieval & Generation<a hidden class=anchor aria-hidden=true href=#retrieval--generation>#</a></h3><h4 id=prompt>Prompt<a hidden class=anchor aria-hidden=true href=#prompt>#</a></h4><p>프롬프트 템플릿으로는 튜토리얼에서도 사용되었고, 가장 보편적인 형태를 보이는 <code>rlm/rag-prompt</code> 를 사용했다. (이 기회에 LangSmith도 경험이 되었다.)</p><pre><code>You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Use three sentences maximum and keep the answer concise.
Question: {question}`
Context: {context}`
Answer:
</code></pre><h4 id=llm-model>LLM Model<a hidden class=anchor aria-hidden=true href=#llm-model>#</a></h4><p>LLM모델은 OpenAI의 <code>gpt-3.5-turbo</code>, <code>gpt-4-turbo-preview</code> 두가지 모두 사용해보았다. 아무래도 gpt4 가 한국어 생성 측면에서 전자에 비해 더 매끄럽고 자연스럽다. 과금은 좀 더 비싸지만.</p><h4 id=application>Application<a hidden class=anchor aria-hidden=true href=#application>#</a></h4><p>애플리케이션 프레임워크로는 Streamlit을 사용해서 정말 최소x3의 형태로 만들었다.</p><h2 id=chatgpt-35와의-비교>ChatGPT-3.5와의 비교<a hidden class=anchor aria-hidden=true href=#chatgpt-35와의-비교>#</a></h2><p>동일한 프롬프트를 적용하고, ChatGPT에서는 context에 해당하는 retrieval output을 제외하고 동일한 질문을 했을 때 출력되는 답변을 비교 해볼 수 있다. (** 참고로 RAG에 쓰인 텍스트 데이터의 시점은 ChatGPT와 거의 동일한 2021년이다.)</p><ol><li>테니스 단체 정보</li></ol><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/chatgpt-1.png></center></figure><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/tennisgpt-1.png></center></figure><ol start=2><li>2024년 마지막이 될 것 같은 전설 같은 선수, 라파엘 나달에 관한 정보.</li></ol><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/chatgpt-2.png></center></figure><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/tennisgpt-2.png></center></figure><ol start=3><li>동호인 대회 나가서 가장 스트레스 받는 점</li></ol><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/chatgpt-3.png></center></figure><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/tennisgpt-3.png></center></figure><ol start=4><li>테린이 최대 관심사 중 하나</li></ol><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/chatgpt-4.png></center></figure><figure><center><img src=/2024-03-19-langchain-rag-qa-toy-tennis-app/tennisgpt-4.png></center></figure><p>&nbsp</p><p>AI 또는 LLM에 대한 배경지식 없는 테니스 동호인이 실제로 사용한다고 가정했을 때, ChatGPT는 다소 두루뭉술한 답을 하는 반면, RAG에 기반해서 구축한 Q&A 애플리케이션이 같은 프롬프트를 기반으로 질문에 대해서 훨씬 직접적이고, 간결한 답을 구사한다고 판단할 수 있다. 마지막 질문에 대해서 RAG 기반 애플리케이션에서는 할루시네이션도 없었다. (코트 예약은 &mldr; 빠르게 &mldr; 잘 &mldr; 정직하게 &mldr; )</p><p>애초에 동작하는 방식과 목적, 적용 방안이 다르기에 직접적인 비교는 무리가 있지만, 제너럴한 답변을 하는 ChatGPT와 VectorDB에 인덱싱 된 문서로부터 답변을 추출한 RAG 사이에 이 정도의 차이점은 있다는 것을 확인할 수 있었다.</p><h2 id=rag-찍먹-후기>RAG 찍먹 후기<a hidden class=anchor aria-hidden=true href=#rag-찍먹-후기>#</a></h2><ol><li>랭체인을 활용한 RAG는 정말이 너무 간편했다. 간편함을 레버리지 삼아. 퀄리티를 높일 수 있는 고민거리에 시간과 에너지를 더 사용할 수 있을 것 같다.</li><li>가장 간편한 텍스트의 형태의 데이터만 사용했지만, 실무에서는 다양한 형태의 문서, 특히 이미지와 표가 함께 섞여 있는 PDF 같은 데이터를 “제대로”, “잘” 파싱하는 것이 실무에서는 매우 중요할 것 같다.</li><li>Text Splitter - 실무를 한다면 어떤 것을 사용할 지, 이 부분이 가장 고민 될 것 같고, 실험을 많이 해봐야 할 것 같다. 가장 기본적으로 사용해볼 수 있는 <code>CharacterTextSplitter</code>, <code>RecursiveCharacterTextSplitter</code> 만 비교해서 사용해봤으나, HuggingFace의 토크나이저까지도 고려대상이 될 수 있을 것 같다.</li><li>가장 기본적으로 OpenAI Embedding을 사용했으나, 나중에는 과금의 요소를 무시하지 못 할 것 같다. (현재의 가격도 PoC나 토이프로젝트 레벨에서는 그리 부담되는 정도는 아니긴 하다.) 더 찾아보니 CashBackedEmbedding 을 통해서 동일한 임베딩의 경우에는 캐싱이 가능해 속도가 빠르고, 중복 과금을 피할 수 있다고 하니 이 부분은 기본으로 장착 하는 것이 좋아보인다.</li><li>결과적으로 약간의 과금을 감수하고서라도, 간편함과 가벼움에 이점이 있어서 OpenAI의 임베딩과 모델을 사용했다. Huggingface 모델도 사용해보려 했으나, 현재 자원에서 모델 로드 자체가 제한적이었다. (Colab에서도 모델의 사이즈 때문에 로드가 되지 않더라.) 인프라가 뒷받침이 된다면 HF에서 여러가지 모델을 실험, 비교해볼 수 있을 것 같다.</li></ol><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=https://python.langchain.com/docs/use_cases/question_answering/ target=_blank>LangChain - Q&amp;amp;A with RAG</a></li><li><a href=https://platform.openai.com/docs/overview target=_blank>OpenAI API Documentation</a></li></ul><h2 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h2><ul><li><a href=https://github.com/wonyoungseo/chatgpt-streamlit-app target=_blank>Github</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://wonyoungseo.github.io/tags/langchain/>langchain</a></li><li><a href=https://wonyoungseo.github.io/tags/rag/>rag</a></li><li><a href=https://wonyoungseo.github.io/tags/llm/>llm</a></li><li><a href=https://wonyoungseo.github.io/tags/openai/>openai</a></li><li><a href=https://wonyoungseo.github.io/tags/vectordb/>vectordb</a></li></ul></footer></article></main><footer class=footer><span>© Wonyoung Seo 2023</span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>